<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>关于kafka中的反序列化</title>
      <link href="/2019/05/12/%E5%85%B3%E4%BA%8Ekafka%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"/>
      <url>/2019/05/12/%E5%85%B3%E4%BA%8Ekafka%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><strong>Kafka生产者</strong>需要序列化程序将==对象转换为字节数组==，然后发送到Kafka。 同样，Kafka消费者需要使用<strong>反序列化器</strong>将从Kafka收到的==字节数组转换为Java对象==。 在前面的示例中，我们假设每个消息的键和值都是字符串，我们在消费者配置中使用了默认的StringDeserializer。</p><p>在第3章关于Kafka生产者的过程中，我们了解了如何自定义序列化类型以及如何使用Avro和AvroSerializer根据模式定义生成Avro对象，然后在向Kafka生成消息时对其进行序列化。 我们现在将介绍如何为自己的对象创建自定义反序列化器以及如何使用Avro及其反序列化器。</p><p>很明显，用于向Kafka生成事件的序列化程序必须与消耗事件时将使用的反序列化程序匹配。 假如我们使用IntSerializer进行序列化，然后使用StringDeserializer进行反序列化，这很有可能出现意想不到的结果。 这意味着作为开发人员，你需要跟踪用于写入每个主题的序列化程序，并确保每个主题仅包含你使用的反序列化程序可以解析的数据。 这是使用Avro和Schema Repository进行序列化和反序列化的好处之一 —— AvroSerializer可以确保写入特定主题的所有数据都与主题的模式兼容，这意味着它可以通过匹配的反序列化器和模式进行反序列化 。 生产者或消费者方面的兼容性错误将通过适当的错误消息轻松捕获，这意味着我们不需要尝试调试字节数组以查找序列化错误。</p><p>我们将首先快速展示如何编写自定义反序列化器，即使这是不太常用的方法，然后我们将继续讨论如何使用Avro反序列化消息键和值的示例。</p><h3 id="Custom-deserializers"><a href="#Custom-deserializers" class="headerlink" title="Custom deserializers"></a>Custom deserializers</h3><p>让我们采用第3章中序列化的相同自定义对象，并为其编写反序列化器：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Customer</span> </span>&#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">int</span> customerID;</span><br><span class="line">            <span class="keyword">private</span> String customerName;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="title">Customer</span><span class="params">(<span class="keyword">int</span> ID, String name)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">this</span>.customerID = ID;</span><br><span class="line">                    <span class="keyword">this</span>.customerName = name;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getID</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> customerID;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> customerName;</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure></p><p>自定义反序列化器将如下所示：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.errors.SerializationException;</span><br><span class="line">    <span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line">    <span class="keyword">import</span> java.util.Map;</span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerDeserializer</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">Deserializer</span>&lt;<span class="title">Customer</span>&gt; </span>&#123; <span class="comment">//[1]</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// nothing to configure</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> Customer <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> id;</span><br><span class="line">        <span class="keyword">int</span> nameSize;</span><br><span class="line">        String name;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">          <span class="keyword">if</span> (data.length &lt; <span class="number">8</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Size of data received by</span></span><br><span class="line"><span class="string">              IntegerDeserializer is shorter than expected"</span>);</span><br><span class="line">          ByteBuffer buffer = ByteBuffer.wrap(data);</span><br><span class="line">          id = buffer.getInt();</span><br><span class="line">          String nameSize = buffer.getInt();</span><br><span class="line">          <span class="keyword">byte</span>[] nameBytes = <span class="keyword">new</span> Array[Byte](nameSize);</span><br><span class="line">          buffer.get(nameBytes);</span><br><span class="line">          name = <span class="keyword">new</span> String(nameBytes, <span class="string">'UTF-8'</span>);</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">new</span> Customer(id, name); <span class="comment">//[2]</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Error when serializing Customer</span></span><br><span class="line"><span class="string">           to byte[] "</span> + e);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">              <span class="comment">// nothing to close</span></span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure></p><blockquote><p>[1]: 消费者还需要Customer类的实现，并且类和序列化器都需要匹配生产和消费应用程序。 在一个拥有许多消费者和生产者共享数据访问权限的大型组织中，这可能变得具有挑战性。[2]我们只是在这里颠倒串行器的逻辑 - 我们从字节数组中获取客户ID和名称，并使用它们构造我们需要的对象。</p></blockquote><p>使用此序列化程序的使用者代码与此示例类似：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.CustomerDeserializer"</span>);</span><br><span class="line">    KafkaConsumer&lt;String, Customer&gt; consumer =</span><br><span class="line">      <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">    consumer.subscribe(<span class="string">"customerCountries"</span>)</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, Customer&gt; records =</span><br><span class="line">          consumer.poll(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, Customer&gt; record : records)</span><br><span class="line">        &#123;</span><br><span class="line">        System.out.println(<span class="string">"current customer Id: "</span> +</span><br><span class="line">        record.value().getId() + <span class="string">" and</span></span><br><span class="line"><span class="string">           current customer name: "</span> + record.value().getName());</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure></p><p>同样，重要的是要注意不建议实现自定义序列化器和反序列化器。 它紧密地耦合了生产者和消费者，并且易碎且容易出错。 ==更好的解决方案是使用标准消息格式，如JSON，Thrift，Protobuf或Avro==。 我们现在将看到如何将Avro反序列化器与Kafka消费者一起使用。 </p><h3 id="使用Avro反序列化与Kafka消费者"><a href="#使用Avro反序列化与Kafka消费者" class="headerlink" title="使用Avro反序列化与Kafka消费者"></a>使用Avro反序列化与Kafka消费者</h3><p>假设我们正在使用第3章中显示的Avro中Customer类的实现。为了从Kafka中使用这些对象，你希望实现类似于此的消费应用程序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>,</span><br><span class="line">       <span class="string">"io.confluent.kafka.serializers.KafkaAvroDeserializer"</span>); <span class="comment">//[1]</span></span><br><span class="line">    props.put(<span class="string">"schema.registry.url"</span>, schemaUrl); <span class="comment">//[2]</span></span><br><span class="line">    String topic = <span class="string">"customerContacts"</span></span><br><span class="line">    KafkaConsumer consumer = <span class="keyword">new</span></span><br><span class="line">       KafkaConsumer(createConsumerConfig(brokers, groupId, url));</span><br><span class="line">    consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line">    System.out.println(<span class="string">"Reading topic:"</span> + topic);</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, Customer&gt; records =</span><br><span class="line">          consumer.poll(<span class="number">1000</span>); <span class="comment">//[3]</span></span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, Customer&gt; record: records) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Current customer name is: "</span> +</span><br><span class="line">               record.value().getName()); <span class="comment">//[4]</span></span><br><span class="line">&#125;</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>[1] 我们使用KafkaAvroDeserializer来反序列化Avro消息。[2] schema.registry.url是一个新参数。 这只是指向我们存储模式的位置。 这样，消费者可以使用生产者注册的模式来反序列化消息。[3] 我们将生成的类Customer指定为记录值的类型。[4] record.value()是一个Customer实例，我们可以相应地使用它。</p></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html" target="_blank" rel="noopener">Chapter 4. Kafka Consumers: Reading Data from Kafka#Deserializers</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> Deserializer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Spark Dataset API中的Typed transformations和Untyped transformations</title>
      <link href="/2019/03/17/%E5%85%B3%E4%BA%8ESpark%20Dataset%20API%E4%B8%AD%E7%9A%84Typed%20transformations%E5%92%8CUntyped%20transformations/"/>
      <url>/2019/03/17/%E5%85%B3%E4%BA%8ESpark%20Dataset%20API%E4%B8%AD%E7%9A%84Typed%20transformations%E5%92%8CUntyped%20transformations/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习Spark源代码的过程中遇到了Typed transformations和Untyped transformations两个概念，整理了以下相关的笔记。对于这两个概念，不知道怎么翻译好，个人理解为强类型转换和弱类型转换，也不知道对不对，欢迎各位大神指正。</p><h2 id="关于Dataset"><a href="#关于Dataset" class="headerlink" title="关于Dataset"></a>关于Dataset</h2><p>Dataset是特定领域对象(domain-specific object)的强类型集合，它可以使用函数或关系运算进行并行转换。 每个Dataset还有一个名为DataFrame的弱类型视图，相当于<code>Dataset[Row]</code>。对于Spark(Scala)，DataFrames只是类型为Row的Dataset。 “Row”类型是Spark中用于计算的，优化过的，in-memory的一种内部表达。</p><p>Dataset上可用的操作分为 <strong>转换(transformation)</strong> 和 <strong>执行(action)</strong> 两种。</p><ul><li>Transformation操作可以产生新的Dataset，如map，filter，select和aggregate（groupBy）等。</li><li>Action操作触发计算和返回结果。 如count，show或写入文件系统等。</li></ul><h2 id="关于Dataset-API"><a href="#关于Dataset-API" class="headerlink" title="关于Dataset API"></a>关于Dataset API</h2><h3 id="Typed-and-Un-typed-APIs"><a href="#Typed-and-Un-typed-APIs" class="headerlink" title="Typed and Un-typed APIs"></a>Typed and Un-typed APIs</h3><p>实质上，在Saprk的结构化API中，可以分成两类，“无类型(untyped)”的DataFrame API和“类型化(typed)”的Dataset API。 确切的说Dataframe并不是”无类型”的, 它们有类型，只是类型检查没有那么严格，只检查这些类型是否在 ==运行时(run-time)== 与schema中指定的类型对齐。 而Dataset在 ==编译时(compile-time)== 就会检查类型是否符合规范。 </p><p>Dataset API仅适用于 ==基于JVM的语言(Scala和Java)==。我们可以使用Scala 中的case class或Java bean来进行类型指定。</p><p>关于不同语言中的可用API可参考下表。</p><table><thead><tr><th>Language</th><th>Main Abstraction</th></tr></thead><tbody><tr><td>Scala</td><td>Dataset[T] &amp; DataFrame (alias for Dataset[Row])</td></tr><tr><td>Java</td><td>Dataset[T]</td></tr><tr><td>Python<em></em></td><td>DataFrame</td></tr><tr><td>R</td><td>DataFrame</td></tr></tbody></table><blockquote><p>由于Python和R没有<code>compile-time type-safety</code>，因此只有 Untyped API，即DataFrames。</p></blockquote><h2 id="关于Transformations"><a href="#关于Transformations" class="headerlink" title="关于Transformations"></a>关于Transformations</h2><p>转换(transformation)可以被分为:</p><ul><li><strong>强类型转换(Typed transformations)</strong></li><li><strong>弱类型转换(Untyped transformations)</strong><h3 id="Typed-transformations-vs-Untyped-transformations"><a href="#Typed-transformations-vs-Untyped-transformations" class="headerlink" title="Typed transformations vs Untyped transformations"></a>Typed transformations vs Untyped transformations</h3>简单来说，如果转换是弱类型的，它将返回一个Dataframe(==确切的说弱类型转换的返回类型还有 <strong><em>Column</em></strong>,  <strong><em>RelationalGroupedDataset</em></strong>, <strong><em>DataFrameNaFunctions</em></strong>  和 <strong><em>DataFrameStatFunctions</em></strong>  等==)，而强类型转换返回的是一个Dataset。 在源代码中，我们可以看到弱类型转换API的返回类型是Dataframe而不是Dataset，且带有<code>@group untypedrel</code>的注释。 因此，我们可以通过检查该方法的签名来确定它是否是弱类型的(untyped)。<blockquote><p>强类型转换API带有<code>@group typedrel</code>的注释</p></blockquote></li></ul><p>例如Dataset.scala类中的<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L864-L876" target="_blank" rel="noopener">join方法</a>就属于弱类型转换(untyped transformations).<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Join with another `DataFrame`.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Behaves as an INNER JOIN and requires a subsequent join predicate.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> right Right side of the join operation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@group</span> untypedrel</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> 2.0.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">def <span class="title">join</span><span class="params">(right: Dataset[_])</span>: DataFrame </span>= withPlan &#123;</span><br><span class="line">  Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通常，任何更改Dataset列类型或添加新列的的转换是弱类型。 当我们需要修改Dataset的schema时，我们就需要退回到Dataframe进行操作。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/ch04.html" target="_blank" rel="noopener">Structured API Overview</a><a href="http://apache-spark-user-list.1001560.n3.nabble.com/Difference-between-Typed-and-untyped-transformation-in-dataset-API-td34650.html" target="_blank" rel="noopener">Difference-between-Typed-and-untyped-transformation-in-dataset-API</a><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">RDDs vs DataFrames and Datasets</a><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-dataset-operators.html" target="_blank" rel="noopener">spark-sql-dataset-operators</a><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">org.apache.spark.sql.Dataset</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> dataset </tag>
            
            <tag> transformations api </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ETL工具Talend最佳实践</title>
      <link href="/2019/03/01/ETL%E5%B7%A5%E5%85%B7Talend%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
      <url>/2019/03/01/ETL%E5%B7%A5%E5%85%B7Talend%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>和Talend这款软件打交道有一段时间了，主要用它来做一些ETL相关的作业开发，以下总结了一些自己配置与开发过程中的最佳实践。</p><h4 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h4><ol><li><p>可以通过修改Talend Studio 的 <strong>.ini</strong> 配置文件来给其分配更多的内存，例如，以下是我在64位8GB内存的电脑配置的参数</p><table><tr><td align="left">-vmargs <br>-Xms2014m <br>-Xmx4096m <br>-XX:MaxPermSize=512m <br>-Dfile.encoding=UTF-8 <br></td></tr></table></li><li><p>在开发过程中一定要注意对Null值得处理</p></li><li>可以创建Repository Metadata用于数据库连接</li><li>可以使用 ==t\<db>== 的数据连接组件定义数据库连接，并重复使用。</db></li><li>记得使用 ==t\<db>== 组件来关闭数据库连接</db></li><li>避免在Talend的组件中在使用硬编码值(hard coding)，使用Talend context 变量代替</li><li>尽可能使用变量代替硬编码</li><li>对于频繁的变换，可以通过创建routines或者functions来减少工作量</li><li>每次关机前记得保存并关闭Talend Studio！！！</li><li>尽可能早的使用tFilterColumns组件过滤去不需要的字段/列</li><li>尽可能早的使用tFilterRows组件过滤去不需要的数据</li><li>使用Select列表达式从数据库获取数据，尽量避免获取不需要的字段</li><li>当作业出现OOM错误时，调整JVM的参数，例如修改Xms和Xmx来分配更多的内存</li><li>通过使用并行化选项来提高作业性能，减少整体的运行时间，如并行化从数据读写数据等</li><li>给Main job起一个有意义的名字</li><li>在定义Sub job时，务必第一时间记录子作业的标题、描述和目的。</li><li>在设计作业尽可能将复杂的作业切割成一个个小作业</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> ETL </tag>
            
            <tag> Talend </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark.sql.shuffle.partitions 和 spark.default.parallelism 的区别</title>
      <link href="/2019/02/27/spark.sql.shuffle.partitions%20%E5%92%8C%20spark.default.parallelism%20%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/02/27/spark.sql.shuffle.partitions%20%E5%92%8C%20spark.default.parallelism%20%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p>在关于spark任务并行度的设置中，有两个参数我们会经常遇到，spark.sql.shuffle.partitions 和 spark.default.parallelism, 那么这两个参数到底有什么区别的？</p><p>首先，让我们来看下它们的定义</p><table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody><tr><td>spark.sql.shuffle.partitions</td><td>200</td><td align="left" width="200">Configures the number of partitions to use when shuffling data for <b>joins</b> or <b>aggregations</b>.</td></tr><tr><td>spark.default.parallelism</td><td align="left">For distributed shuffle operations like <strong>reduceByKey</strong> and <strong>join</strong>, the largest number of partitions in a parent RDD. <br><br>For operations like parallelize with no parent RDDs, it depends on the cluster manager: <br> <b>- Local mode:</b> number of cores on the local machine <br> <b>- Mesos fine grained mode</b>: 8 <br><b>- Others:</b> total number of cores on all executor nodes or 2, whichever is larger</td><td align="left">Default number of partitions in RDDs returned by transformations like <b>join</b>, <b>reduceByKey</b>, and parallelize when not set by user.</td></tr></tbody></table><p>看起来它们的定义似乎也很相似，但在实际测试中，</p><ul><li>spark.default.parallelism只有在处理RDD时才会起作用，对Spark SQL的无效。</li><li>spark.sql.shuffle.partitions则是对sparks SQL专用的设置</li></ul><p>我们可以在提交作业的通过 <code>--conf</code> 来修改这两个设置的值，方法如下：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --conf spark.sql.shuffle.partitions=20 --conf spark.default.parallelism=20</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Parallelism </tag>
            
            <tag> Tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么建议在Spark中使用Scala定义UDF</title>
      <link href="/2019/02/18/why-use-scala-udf/"/>
      <url>/2019/02/18/why-use-scala-udf/</url>
      
        <content type="html"><![CDATA[<p>虽然在Pyspark中，驱动程序是一个python进程，但是它创建的SparkSession对象以及其他DataFrames或者RDDs等都是利用Python封装过的 ==JVM对象== 。简单地说，虽然控制程序是Python，但它实际上是python代码告诉集群上的分布式Scala程序该做什么。 数据存储在JVM的内存中，并由Scala代码进行转换。</p><p>将这些对象从JVM内存中取出并将它们转换为Python可以读取的形式（称为序列化和反序列化）的过程开销是很大的。 一般情况下，将计算结果收集回Python驱动程序通常针对低容量样本，并且不经常进行，因此这种开销相对不被注意。 但是，如果程序在集群中的对整个数据集的Python和JVM对象之间来回转换时，性能将会受到显著影响。</p><p><img src="./sparkudf.png" alt="Credit:  https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9"></p><p>在上图中，Python程序的指令（1）被转换为Spark执行计划，并通过SparkSession JVM对象（2）传递给集群中不同机器上的两个执行程序（3）。 执行程序通常会从外部源（如HDFS）加载数据，在内存中执行某些转换，然后将数据写回外部存储。 数据将在程序的生命周期内保留在JVM（3）中。</p><p>而使用Python UDF时，数据必须经过几个额外的步骤。 首先，数据必须从Java（4）序列化，这样运行UDF所在的Python进程才可以将其读入（5）。 然后，Python运算完的结果经过一些列序列化和反序列化然后返回到JVM。</p><p>那么我们该如何优化呢？我们可以直接使用Scala来编写Spark UDF。Scala UDF可以直接在执行程序的JVM中运行，因此数据将跳过两轮序列化和反序列化，处理的效率将会比使用Python UDF高的多。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>启动Python进程的开销不小，但是真正的开销在于将数据序列化到Python中。推荐在Spark中定义UDF时首选Scala或Java，即使UDFs是用Scala/Java编写的，不用担心，我们依然可以在python(pyspark)中使用它们，简单实例如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### pyspark --jars [path/to/jar/x.jar]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pre Spark 2.1,</span></span><br><span class="line">spark._jvm.com.test.spark.udf.MyUpper.registerUDF(spark._jsparkSession)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.1+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">sqlContext.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.3+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">spark.udf.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your UDF</span></span><br><span class="line">spark.sql(<span class="string">"""SELECT my_upper('abeD123okoj')"""</span>).show()</span><br></pre></td></tr></table></figure></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9" target="_blank" rel="noopener">Using Scala UDFs in PySpark</a></p><p><a href="http://shop.oreilly.com/product/0636920034957.do" target="_blank" rel="noopener">[BOOK] Spark - The Definitive Guide</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Scala </tag>
            
            <tag> UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac共享主机网络给虚拟机</title>
      <link href="/2019/02/17/Mac%E5%85%B1%E4%BA%AB%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%99%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
      <url>/2019/02/17/Mac%E5%85%B1%E4%BA%AB%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%99%E8%99%9A%E6%8B%9F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>因工作需要需要且身边没有windows系统的笔记本，无奈只好在mac上利用虚拟机安装一个win7系统作为临时过渡。我使用的虚拟机软件是Parallels Desktop（以下简称PD）<img src="./2019021611181380.png" width="30%" alt>PD提供三种不同网络模式供用户选择:</p><ul><li>共享网络（推荐）</li><li>桥接网络</li><li>Host-Only网络<img src="./20190216110149123.png" width="60%" alt></li></ul><p>各种网络模式的区别请移步<a href="https://kb.parallels.com/en/4948" target="_blank" rel="noopener">官方文档</a></p><p>一开始我并没有任何设置，直接使用默认的<strong>共享网络</strong>模式，使用过程中出现了有时候连得上有时候连不上的情况。后来经过一番搜索发现即使用共享网络模式，也需要一些简单的设置。具体步骤如下：</p><ol><li><p>在PD的偏好设置中进行网络设置，添加(+)端口转发规则如下：<img src="./20190216112753690.png" width="60%" alt></p></li><li><p>在虚拟机的网络设置中使用<strong>共享网络</strong>模式<img src="./20190216113314497.png" width="60%" alt></p></li><li><p>重启虚拟机即可</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac上解决访问github慢问题</title>
      <link href="/2019/02/16/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E9%97%AE%E9%A2%98/"/>
      <url>/2019/02/16/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<ol><li>访问 <a href="http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo" target="_blank" rel="noopener">http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo</a> 获取github的IP地址</li><li>在/etc/hosts中加入查询到的IP和域名 （需要root 权限）</li><li>在终端在输以下指令刷新DNS（需要root 权限）  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo killall -HUP mDNSResponder;say DNS cache has been flushed</span><br></pre></td></tr></table></figure></li></ol><p> 或者也可以参见<a href="https://blog.csdn.net/yolohohohoho/article/details/87647036" target="_blank" rel="noopener">懒人版代码</a></p><p>其他Mac相关问题：<a href="https://blog.csdn.net/yolohohohoho/article/details/87892412" target="_blank" rel="noopener">brew update慢的解决方法</a><a href="https://blog.csdn.net/yolohohohoho/article/details/87893368" target="_blank" rel="noopener">conda install慢的解决方法</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Mac </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
