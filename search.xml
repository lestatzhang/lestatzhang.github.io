<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>记录一个看着比较顺眼的blockquote的css装饰</title>
      <link href="/2019/05/%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%AA%E7%9C%8B%E7%9D%80%E6%AF%94%E8%BE%83%E9%A1%BA%E7%9C%BC%E7%9A%84blockquote%E7%9A%84css%E8%A3%85%E9%A5%B0.html"/>
      <url>/2019/05/%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%AA%E7%9C%8B%E7%9D%80%E6%AF%94%E8%BE%83%E9%A1%BA%E7%9C%BC%E7%9A%84blockquote%E7%9A%84css%E8%A3%85%E9%A5%B0.html</url>
      
        <content type="html"><![CDATA[<p>代码如下</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">blockquote</span> &#123;</span><br><span class="line">    <span class="attribute">position</span>: relative;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">10px</span> <span class="number">15px</span> <span class="number">10px</span> <span class="number">60px</span>;</span><br><span class="line">    <span class="attribute">box-sizing</span>: border-box;</span><br><span class="line">    <span class="attribute">font-style</span>: italic;</span><br><span class="line">    <span class="attribute">background</span>: <span class="number">#f5f5f5</span>;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#777777</span>;</span><br><span class="line">    <span class="attribute">border-left</span>: <span class="number">4px</span> solid <span class="number">#9dd4ff</span>;</span><br><span class="line">    <span class="attribute">box-shadow</span>: <span class="number">0</span> <span class="number">2px</span> <span class="number">4px</span> <span class="built_in">rgba</span>(0, 0, 0, 0.14);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">blockquote</span><span class="selector-pseudo">:before</span>&#123;</span><br><span class="line">    <span class="attribute">display</span>: inline-block;</span><br><span class="line">    <span class="attribute">position</span>: absolute;</span><br><span class="line">    <span class="attribute">top</span>: <span class="number">15px</span>;</span><br><span class="line">    <span class="attribute">left</span>: <span class="number">15px</span>;</span><br><span class="line">    <span class="attribute">content</span>: <span class="string">"\f10d"</span>;</span><br><span class="line">    <span class="attribute">font-family</span>: FontAwesome;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#9dd4ff</span>;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">30px</span>;</span><br><span class="line">    <span class="attribute">line-height</span>: <span class="number">1</span>;</span><br><span class="line">    <span class="attribute">font-weight</span>: <span class="number">900</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">blockquote</span> <span class="selector-tag">p</span> &#123;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0</span>;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">7px</span> <span class="number">0</span>;</span><br><span class="line">    <span class="attribute">line-height</span>: <span class="number">1.7</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">blockquote</span> <span class="selector-tag">cite</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: block;</span><br><span class="line">    <span class="attribute">text-align</span>: right;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#888888</span>;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">0.9em</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体样式如下：<img src="/2019/05/记录一个看着比较顺眼的blockquote的css装饰/./20190531233330978.png" alt></p><p>参考资料：<a href="https://saruwakakun.com/html-css/reference/blockquote" target="_blank" rel="noopener">CSSで作る！魅力的な引用デザインのサンプル30（blockquote）</a></p>]]></content>
      
      
      <categories>
          
          <category> css </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blockquote </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac上如何使用SSH连接到GitHub</title>
      <link href="/2019/05/Mac%E4%B8%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8SSH%E8%BF%9E%E6%8E%A5%E5%88%B0GitHub.html"/>
      <url>/2019/05/Mac%E4%B8%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8SSH%E8%BF%9E%E6%8E%A5%E5%88%B0GitHub.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h2 id="关于SSH"><a href="#关于SSH" class="headerlink" title="关于SSH"></a>关于SSH</h2><p>使用SSH协议，你可以连接和验证远程服务器与服务。 使用SSH密钥，你可以在每次访问时<strong>不提供</strong>用户名或密码就可以连接到GitHub，即免密登陆。</p><p>当你设置SSH时，你需要生成SSH密钥并将其添加到ssh-agent，然后将密钥添加到您的GitHub帐户。 将SSH密钥添加到ssh-agent可确保你的SSH密钥通过使用passphrase而具有额外的安全层。</p><p>如果你超过一年未使用SSH密钥，那么根据安全预防措施，GitHub将自动删除你的非活动SSH密钥。</p><h2 id="检查现有的SSH密钥"><a href="#检查现有的SSH密钥" class="headerlink" title="检查现有的SSH密钥"></a>检查现有的SSH密钥</h2><p>在生成SSH密钥之前，你可以检查是否已经有SSH密钥存在。</p><blockquote><p>注意：在OpenSSH 7.0中不推荐使用DSA密钥。 如果你的操作系统使用OpenSSH，则在设置SSH时需要使用备用类型的密钥，例如RSA密钥。 例如，如果你的操作系统是MacOS Sierra，则可以使用RSA密钥设置SSH。</p></blockquote><ol><li><p>打开终端 </p></li><li><p>输入ls -al ~/.ssh以查看是否存在现有SSH密钥：</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ls -al ~/.ssh</span><br><span class="line"><span class="meta">#</span> Lists the files in your .ssh directory, if they exist</span><br></pre></td></tr></table></figure></li><li><p>检查目录列表以查看你是否已拥有公共SSH密钥。</p></li></ol><p>默认情况下，公钥的文件名是以下之一：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">id_dsa.pub</span><br><span class="line">id_ecdsa.pub</span><br><span class="line">id_ed25519.pub</span><br><span class="line">id_rsa.pub</span><br></pre></td></tr></table></figure><ul><li>如你没有现有的公钥和私钥对，或者不希望使用任何可用的连接到GitHub，则生成新的SSH密钥。</li><li>如果你看到要用于连接到GitHub的现有公钥和私钥对（例如id_rsa.pub和id_rsa），则可以将SSH密钥添加到ssh-agent。</li></ul><blockquote><p>提示：如果收到~/.ssh不存在的错误，请不要担心！ 我们可以通过指令生成新的SSH密钥。</p></blockquote><h2 id="生成新的SSH密钥并将其添加到ssh-agent"><a href="#生成新的SSH密钥并将其添加到ssh-agent" class="headerlink" title="生成新的SSH密钥并将其添加到ssh-agent"></a>生成新的SSH密钥并将其添加到ssh-agent</h2><p>检查现有SSH密钥后，可以生成用于身份验证的新SSH密钥，然后将其添加到ssh-agent。</p><p>如果你还没有SSH密钥，则必须生成新的SSH密钥。如果你不确定是否已有SSH密钥，请检查现有密钥。</p><p>如果你不想在每次使用SSH密钥时重新输入密码，则可以将密钥添加到ssh-agent，ssh-agent管理你的SSH密钥并记住你的密码。</p><h3 id="生成新的SSH密钥"><a href="#生成新的SSH密钥" class="headerlink" title="生成新的SSH密钥"></a>生成新的SSH密钥</h3><ol><li><p>打开终端 </p></li><li><p>在终端粘贴下面的文本，记得替换你自己的GitHub电子邮件地址。</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ssh-keygen -t rsa -b 4096 -C“your_email@example.com”</span><br></pre></td></tr></table></figure><p> 这将使用你提供的电子邮件作为标签创建一个新的ssh密钥。</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> Generating public/private rsa key pair.</span><br></pre></td></tr></table></figure></li><li><p>当系统提你 “Enter a file in which to save the key”时，按Enter键。这将使用默认的文件位置。</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>Enter a file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]</span><br></pre></td></tr></table></figure></li><li><p>在提示符下，键入安全密码。</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> Enter passphrase (empty for no passphrase): [Type a passphrase]</span><br><span class="line"><span class="meta">&gt;</span> Enter same passphrase again: [Type passphrase again]</span><br></pre></td></tr></table></figure></li></ol><h3 id="将SSH密钥添加到ssh-agent"><a href="#将SSH密钥添加到ssh-agent" class="headerlink" title="将SSH密钥添加到ssh-agent"></a>将SSH密钥添加到ssh-agent</h3><p>在将新的SSH密钥添加到ssh-agent以管理密钥之前，你应该检查现有的SSH密钥并生成新的SSH密钥。将SSH密钥添加到代理时，请使用默认的macOS ssh-add命令，而不是macports，homebrew或其他外部源安装的应用程序。</p><ol><li><p>在后台启动ssh-agent。</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> eval "$(ssh-agent -s)"</span><br><span class="line"><span class="meta">&gt;</span> Agent pid 59566</span><br></pre></td></tr></table></figure></li><li><p>如果你使用的是macOS Sierra 10.12.2或更高版本，则需要修改 <strong>~/.ssh/config</strong>文件以自动将密钥加载到ssh-agent中并在密钥链中存储密码。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Host *</span><br><span class="line">  AddKeysToAgent yes</span><br><span class="line">  UseKeychain yes</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa</span><br></pre></td></tr></table></figure></li><li><p>将SSH私钥添加到ssh-agent并将密码存储在密钥链中。如果使用其他名称创建密钥，或者要添加具有不同名称的现有密钥，请将命令中的id_rsa替换为私钥文件的名称。</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ssh-add -K ~/.ssh/id_rsa</span><br></pre></td></tr></table></figure></li></ol><blockquote><p>注意：-K选项是Apple的ssh-add标准版本，当您向ssh-agent添加ssh密钥时，它会将密码链存储在你的钥匙串中。如果你没有安装Apple的标准版本，则可能会收到错误消息 Error: ssh-add: illegal option -- K.。</p></blockquote><ol start="4"><li>将SSH密钥添加到GitHub帐户（看下一步）</li></ol><h3 id="将新SSH密钥添加到GitHub帐户"><a href="#将新SSH密钥添加到GitHub帐户" class="headerlink" title="将新SSH密钥添加到GitHub帐户"></a>将新SSH密钥添加到GitHub帐户</h3><p>要将GitHub帐户配置为使用新的(或现有的)SSH密钥，你还需要将其添加到GitHub帐户。具体步骤请参考<a href="https://help.github.com/en/articles/adding-a-new-ssh-key-to-your-github-account" target="_blank" rel="noopener">官网文档</a></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>在设置SSH密钥并将其添加到GitHub帐户后，你可以测试你的连接是否成功。在测试SSH连接之前，你应该：</p><ul><li>检查现有SSH密钥</li><li>生成新的SSH密钥</li><li>为你的GitHub帐户添加了新的SSH密钥</li></ul><p>测试连接时，你需要使用密码验证此操作，密码是你之前创建的SSH密钥密码。有关使用SSH密钥密码的更多信息，请参阅<a href="https://help.github.com/en/articles/working-with-ssh-key-passphrases" target="_blank" rel="noopener">“使用SSH密钥密码”</a>。</p><ol><li><p>打开终端</p></li><li><p>输入以下内容：</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ssh -T git@github.com</span><br><span class="line"><span class="meta">#</span> Attempts to ssh to GitHub</span><br></pre></td></tr></table></figure><p> 你可能会看到如下警告：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; The authenticity of host &apos;github.com (IP ADDRESS)&apos; can&apos;t be established.</span><br><span class="line">  &gt; RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.</span><br><span class="line">  &gt; Are you sure you want to continue connecting (yes/no)?</span><br></pre></td></tr></table></figure><p> 或者：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; The authenticity of host &apos;github.com (IP ADDRESS)&apos; can&apos;t be established.</span><br><span class="line">  &gt; RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.</span><br><span class="line">  &gt; Are you sure you want to continue connecting (yes/no)?</span><br></pre></td></tr></table></figure></li><li><p>验证你看到的消息中的fingerprint是否与步骤2中的某条消息匹配，然后键入yes：</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> Hi username! You've successfully authenticated, but GitHub does not</span><br><span class="line"><span class="meta">&gt;</span> provide shell access.</span><br></pre></td></tr></table></figure></li><li><p>验证生成的消息是否包含你的用户名。如果收到“permission denied”消息，请参阅<a href="https://help.github.com/en/articles/error-permission-denied-publickey" target="_blank" rel="noopener">&quot;Error: Permission denied (publickey)&quot;</a>。</p></li></ol><h2 id="使用SSH密钥密码"><a href="#使用SSH密钥密码" class="headerlink" title="使用SSH密钥密码"></a>使用SSH密钥密码</h2><p>你可以保护SSH密钥并配置身份验证代理，这样你就不必在每次使用SSH密钥时重新输入密码。</p><p>使用SSH密钥，如果有人获得对你计算机的访问权限，他们也可以访问使用该密钥的每个系统。 要添加额外的安全层，可以向SSH密钥添加密码。 你可以使用ssh-agent安全地保存密码，这样你就不必重新输入密码。</p><h3 id="添加或更改passphrase"><a href="#添加或更改passphrase" class="headerlink" title="添加或更改passphrase"></a>添加或更改passphrase</h3><p>你可以通过键入以下命令来更改现有私钥的密码，而无需重新生成密钥对：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ssh-keygen -p</span><br><span class="line"><span class="meta">#</span> Start the SSH key creation process</span><br><span class="line"><span class="meta">&gt;</span> Enter file in which the key is (/Users/you/.ssh/id_rsa): [Hit enter]</span><br><span class="line"><span class="meta">&gt;</span> Key has comment '/Users/you/.ssh/id_rsa'</span><br><span class="line"><span class="meta">&gt;</span> Enter new passphrase (empty for no passphrase): [Type new passphrase]</span><br><span class="line"><span class="meta">&gt;</span> Enter same passphrase again: [One more time for luck]</span><br><span class="line"><span class="meta">&gt;</span> Your identification has been saved with the new passphrase.</span><br></pre></td></tr></table></figure><p>如果你的密钥已有passphrase，系统将提示你输入密码短语，然后才能更改为新密码短语。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://help.github.com/en/articles/connecting-to-github-with-ssh" target="_blank" rel="noopener">Connecting to GitHub with SSH</a></p>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记录下本周末搭建个人博客的过程Mac+Hexo+GitHubPages</title>
      <link href="/2019/05/%E8%AE%B0%E5%BD%95%E4%B8%8B%E6%9C%AC%E5%91%A8%E6%9C%AB%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%9A%84%E8%BF%87%E7%A8%8BMac+Hexo+GitHubPages.html"/>
      <url>/2019/05/%E8%AE%B0%E5%BD%95%E4%B8%8B%E6%9C%AC%E5%91%A8%E6%9C%AB%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%9A%84%E8%BF%87%E7%A8%8BMac+Hexo+GitHubPages.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前本来有一个个人博客，但是因为太懒没有维护，就来投奔CSDN了。这几天突然一时兴起，让好好弄一下自己的个人博客，因为CSDN的广告实在是....一言难尽...搜索了一般，选取一个比较简单的实现方式：即Hexo+GitHub Pages以下记录如果搭建个人博客网站 <a href="https://lestatzhang.com/">lestatzhang.com</a>的过程</p><h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><ol><li>安装Git</li><li>安装Node.js</li><li>安装Hexo</li><li>博客初始化</li><li>将本地博客与GitHub关联</li><li>切换Hexo主题：Next</li><li>Goddady购买个人域名</li><li>绑定个人域名</li><li>其他TO-DO</li></ol><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><p>先查看是否已经安装Git <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lestat@Lestats-MBP:~$ git --version</span><br><span class="line">git version 2.14.3 (Apple Git-98)</span><br></pre></td></tr></table></figure></p><p> 如果Mac没有安装git可以通过Homebrew安装 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install git</span><br></pre></td></tr></table></figure></p><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><p> 如果Mac没有安装Node.js可以通过Homebrew安装 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install node</span><br></pre></td></tr></table></figure></p><p>中间有可能因为一些依赖库需要更新你的Xcode的Command Line Tools</p><p>我安装的版本如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lestat@Lestats-MBP:~$ node -v</span><br><span class="line">v12.3.1</span><br><span class="line">lestat@Lestats-MBP:~$ npm -v</span><br><span class="line">6.9.0</span><br></pre></td></tr></table></figure><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>Node.js和Git都安装成功后开始安装Hexo</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install -g hexo-cli</span><br></pre></td></tr></table></figure><h3 id="博客初始化"><a href="#博客初始化" class="headerlink" title="博客初始化"></a>博客初始化</h3><p>创建你本地的博客文件夹，比如我的就是 lestatzhang， 然后进入该文件夹，利用hexo进行初始化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd lestatzhang;</span><br><span class="line">hexo init;</span><br></pre></td></tr></table></figure><p>执行下述命令安装npm。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install;</span><br></pre></td></tr></table></figure><p>执行hexo命令生成本地网页文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure><p>执行hexo命令开启本地服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>然后我们就通过 <a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a> 查看本地博客。</p><h3 id="将本地博客与GitHub关联"><a href="#将本地博客与GitHub关联" class="headerlink" title="将本地博客与GitHub关联"></a>将本地博客与GitHub关联</h3><p>编辑站点配置文件<code>_config.yml</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi _config.yml</span><br></pre></td></tr></table></figure><p>打开后到文档最后部分，配置deploy设置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: https://github.com/lestatzhang/lestatzhang.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>然后为hexo配置git部署服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>运行hexo命令，将在lestatzhang下生成静态文件并上传到git服务器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>若未关联GitHub，执行hexo d时会提示输入GitHub账号用户名和密码，即:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">username for 'https://github.com':</span><br><span class="line">password for 'https://github.com':</span><br></pre></td></tr></table></figure><p><code>hexo d</code>执行成功后便可通过 <a href="https://lestatzhang.github.io" target="_blank" rel="noopener">https://lestatzhang.github.io</a> 访问博客，看到的内容和本地页面一致。</p><p>如果需要开启ssh，我们可以在Github中配置ssh keys。具体步骤可以参考<a href="https://help.github.com/en/articles/connecting-to-github-with-ssh" target="_blank" rel="noopener">Connecting to GitHub with SSH</a></p><h3 id="切换Hexo主题：Next"><a href="#切换Hexo主题：Next" class="headerlink" title="切换Hexo主题：Next"></a>切换Hexo主题：Next</h3><p>Hexo允许我们为自己的站点配置自己喜欢的主题, 在这里我选择一个个人比较喜欢的主题: <strong>hexo-theme-next</strong>。 安装过程如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd lestatzhang;</span><br><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><p>编辑站点配置文件_config.yml，将theme的值从landscape更改为next将blog目录下_config.yml里的theme的名称landscape更改为next。</p><p>然后重新生成站点文件,并查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g  </span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><h3 id="Godaddy购买个人域名"><a href="#Godaddy购买个人域名" class="headerlink" title="Godaddy购买个人域名"></a>Godaddy购买个人域名</h3><p>在Godday上买了一个自己的域名 <a href="https://lestatzhang.com/">lestatzhang.com</a></p><h3 id="绑定个人域名"><a href="#绑定个人域名" class="headerlink" title="绑定个人域名"></a>绑定个人域名</h3><p>Godaddy的配置可以参考如下图片<img src="/2019/05/记录下本周末搭建个人博客的过程Mac+Hexo+GitHubPages/./godaddy.png" alt></p><p>然后在next主题中source文件夹中创建CNAME文件，然后将个人域名 <a href="https://lestatzhang.com/">lestatzhang.com</a>添加进CNAME之后重新部署网站。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd themes/next/source/</span><br><span class="line">echo "lestatzhang.com" &gt; CNAME</span><br><span class="line">cd ../../../'</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><h3 id="TO-DO"><a href="#TO-DO" class="headerlink" title="TO-DO"></a>TO-DO</h3><p>具体博客搭建的步骤就这些了，后面主要是如何对网站页面/主题进行优化的过程。 TO-DO</p>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Spark中RDD对象无法调用toDF的解决方法</title>
      <link href="/2019/05/%E5%85%B3%E4%BA%8ESpark%E4%B8%ADRDD%E5%AF%B9%E8%B1%A1%E6%97%A0%E6%B3%95%E8%B0%83%E7%94%A8toDF%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.html"/>
      <url>/2019/05/%E5%85%B3%E4%BA%8ESpark%E4%B8%ADRDD%E5%AF%B9%E8%B1%A1%E6%97%A0%E6%B3%95%E8%B0%83%E7%94%A8toDF%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.html</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>今天在调试一个Scala程序时，IDEA报了<code>can not resolve symbol toDF</code>的错误, 查看了一下代码， 该行的逻辑是将RDD转成DataFrame，看起来似乎很正常， 但就是没有办法调用<code>toDF</code>方法，查看了一下上下文，原来是在代码重构的时候，不小心的将<code>import sqlContext.implicits._</code>给移除了。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>如果使用的是spark 2.0之前的版本，RDD转换之前， 加入以下代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlContext = <span class="keyword">new</span> org.apache.spark.sql.SQLContext(sc)</span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br></pre></td></tr></table></figure><p>如果你使用的时spark 2.0+，则可以使用以下方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession</span><br><span class="line">val spark = SparkSession.builder.master(<span class="string">"local[4]"</span>).getOrCreate</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>顺便总结下Spark中将RDD转换成DataFrame的两种方法, 代码如下：</p><ul><li><p>方法一： 使用<code>createDataFrame</code>方法</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//StructType and convert RDD to DataFrame  </span></span><br><span class="line"></span><br><span class="line">   val schema = StructType(  </span><br><span class="line">     Seq(  </span><br><span class="line">       StructField(<span class="string">"name"</span>,StringType,<span class="keyword">true</span>)            </span><br><span class="line">       ,StructField(<span class="string">"age"</span>,IntegerType,<span class="keyword">true</span>)  </span><br><span class="line">     )  </span><br><span class="line">   ) </span><br><span class="line">    </span><br><span class="line">   val rowRDD = sparkSession.sparkContext  </span><br><span class="line">     .textFile(<span class="string">"/tmp/people.txt"</span>,<span class="number">2</span>) </span><br><span class="line">     .map( x =&gt; x.split(<span class="string">","</span>)).map( x =&gt; Row(x(<span class="number">0</span>),x(<span class="number">1</span>).trim().toInt))    </span><br><span class="line">   sparkSession.createDataFrame(rowRDD,schema)  </span><br><span class="line"> &#125;  </span><br><span class="line">   </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>方法二： 使用<code>toDF</code>方法</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//use case class Person  </span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">Person</span><span class="params">(name:String,age:Int)</span>  </span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">//导入隐饰操作，否则RDD无法调用toDF方法  </span></span></span><br><span class="line"><span class="function"><span class="keyword">import</span> sparkSession.implicits._  </span></span><br><span class="line"><span class="function">val peopleRDD </span>= sparkSession.sparkContext  </span><br><span class="line">  .textFile(<span class="string">"/tmp/people.txt"</span>,<span class="number">2</span>)  </span><br><span class="line">  .map( x =&gt; x.split(<span class="string">","</span>)).map( x =&gt; Person(x(<span class="number">0</span>),x(<span class="number">1</span>).trim().toInt)).toDF()</span><br></pre></td></tr></table></figure></li></ul><blockquote><p><font color="red"><b>注意</b><font color="darkblue">请不要将<code>case Class</code>定义在main 方法中与toDF一起使用</font></font></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> DataFrame </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中将RDD转换成DataFrame的两种方法</title>
      <link href="/2019/05/Spark%E4%B8%AD%E5%B0%86RDD%E8%BD%AC%E6%8D%A2%E6%88%90DataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html"/>
      <url>/2019/05/Spark%E4%B8%AD%E5%B0%86RDD%E8%BD%AC%E6%8D%A2%E6%88%90DataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html</url>
      
        <content type="html"><![CDATA[<p>总结下Spark中将RDD转换成DataFrame的两种方法, 代码如下：</p><ul><li><p>方法一： 使用<code>createDataFrame</code>方法</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//StructType and convert RDD to DataFrame  </span></span><br><span class="line"></span><br><span class="line">   val schema = StructType(  </span><br><span class="line">     Seq(  </span><br><span class="line">       StructField(<span class="string">"name"</span>,StringType,<span class="keyword">true</span>)            </span><br><span class="line">       ,StructField(<span class="string">"age"</span>,IntegerType,<span class="keyword">true</span>)  </span><br><span class="line">     )  </span><br><span class="line">   ) </span><br><span class="line">    </span><br><span class="line">   val rowRDD = sparkSession.sparkContext  </span><br><span class="line">     .textFile(<span class="string">"/tmp/people.txt"</span>,<span class="number">2</span>) </span><br><span class="line">     .map( x =&gt; x.split(<span class="string">","</span>)).map( x =&gt; Row(x(<span class="number">0</span>),x(<span class="number">1</span>).trim().toInt))    </span><br><span class="line">   sparkSession.createDataFrame(rowRDD,schema)  </span><br><span class="line"> &#125;  </span><br><span class="line">   </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>方法二： 使用<code>toDF</code>方法</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//use case class Person  </span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">Person</span><span class="params">(name:String,age:Int)</span>  </span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">//导入隐饰操作，否则RDD无法调用toDF方法  </span></span></span><br><span class="line"><span class="function"><span class="keyword">import</span> sparkSession.implicits._  </span></span><br><span class="line"><span class="function">val peopleRDD </span>= sparkSession.sparkContext  </span><br><span class="line">  .textFile(<span class="string">"/tmp/people.txt"</span>,<span class="number">2</span>)  </span><br><span class="line">  .map( x =&gt; x.split(<span class="string">","</span>)).map( x =&gt; Person(x(<span class="number">0</span>),x(<span class="number">1</span>).trim().toInt)).toDF()</span><br></pre></td></tr></table></figure></li></ul><blockquote><p><font color="red"><b>注意</b><font color="darkblue">请不要将<code>case Class</code>定义在main 方法中与toDF一起使用，或与使用toDF定义在同一函数中</font></font></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> RDD </tag>
            
            <tag> DataFrame </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于kafka中的反序列化</title>
      <link href="/2019/05/%E5%85%B3%E4%BA%8Ekafka%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96.html"/>
      <url>/2019/05/%E5%85%B3%E4%BA%8Ekafka%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96.html</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><strong>Kafka生产者</strong>需要序列化程序将==对象转换为字节数组==，然后发送到Kafka。 同样，Kafka消费者需要使用<strong>反序列化器</strong>将从Kafka收到的==字节数组转换为Java对象==。 在前面的示例中，我们假设每个消息的键和值都是字符串，我们在消费者配置中使用了默认的StringDeserializer。</p><p>在第3章关于Kafka生产者的过程中，我们了解了如何自定义序列化类型以及如何使用Avro和AvroSerializer根据模式定义生成Avro对象，然后在向Kafka生成消息时对其进行序列化。 我们现在将介绍如何为自己的对象创建自定义反序列化器以及如何使用Avro及其反序列化器。</p><p>很明显，用于向Kafka生成事件的序列化程序必须与消耗事件时将使用的反序列化程序匹配。 假如我们使用IntSerializer进行序列化，然后使用StringDeserializer进行反序列化，这很有可能出现意想不到的结果。 这意味着作为开发人员，你需要跟踪用于写入每个主题的序列化程序，并确保每个主题仅包含你使用的反序列化程序可以解析的数据。 这是使用Avro和Schema Repository进行序列化和反序列化的好处之一 —— AvroSerializer可以确保写入特定主题的所有数据都与主题的模式兼容，这意味着它可以通过匹配的反序列化器和模式进行反序列化 。 生产者或消费者方面的兼容性错误将通过适当的错误消息轻松捕获，这意味着我们不需要尝试调试字节数组以查找序列化错误。</p><p>我们将首先快速展示如何编写自定义反序列化器，即使这是不太常用的方法，然后我们将继续讨论如何使用Avro反序列化消息键和值的示例。</p><h3 id="Custom-deserializers"><a href="#Custom-deserializers" class="headerlink" title="Custom deserializers"></a>Custom deserializers</h3><p>让我们采用第3章中序列化的相同自定义对象，并为其编写反序列化器：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Customer</span> </span>&#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">int</span> customerID;</span><br><span class="line">            <span class="keyword">private</span> String customerName;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="title">Customer</span><span class="params">(<span class="keyword">int</span> ID, String name)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">this</span>.customerID = ID;</span><br><span class="line">                    <span class="keyword">this</span>.customerName = name;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getID</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> customerID;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> customerName;</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure><p>自定义反序列化器将如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.errors.SerializationException;</span><br><span class="line">    <span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line">    <span class="keyword">import</span> java.util.Map;</span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerDeserializer</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">Deserializer</span>&lt;<span class="title">Customer</span>&gt; </span>&#123; <span class="comment">//[1]</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// nothing to configure</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> Customer <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> id;</span><br><span class="line">        <span class="keyword">int</span> nameSize;</span><br><span class="line">        String name;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">          <span class="keyword">if</span> (data.length &lt; <span class="number">8</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Size of data received by</span></span><br><span class="line"><span class="string">              IntegerDeserializer is shorter than expected"</span>);</span><br><span class="line">          ByteBuffer buffer = ByteBuffer.wrap(data);</span><br><span class="line">          id = buffer.getInt();</span><br><span class="line">          String nameSize = buffer.getInt();</span><br><span class="line">          <span class="keyword">byte</span>[] nameBytes = <span class="keyword">new</span> Array[Byte](nameSize);</span><br><span class="line">          buffer.get(nameBytes);</span><br><span class="line">          name = <span class="keyword">new</span> String(nameBytes, <span class="string">'UTF-8'</span>);</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">new</span> Customer(id, name); <span class="comment">//[2]</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Error when serializing Customer</span></span><br><span class="line"><span class="string">           to byte[] "</span> + e);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">              <span class="comment">// nothing to close</span></span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure><blockquote><p>[1]: 消费者还需要Customer类的实现，并且类和序列化器都需要匹配生产和消费应用程序。 在一个拥有许多消费者和生产者共享数据访问权限的大型组织中，这可能变得具有挑战性。[2]我们只是在这里颠倒串行器的逻辑 - 我们从字节数组中获取客户ID和名称，并使用它们构造我们需要的对象。</p></blockquote><p>使用此序列化程序的使用者代码与此示例类似：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.CustomerDeserializer"</span>);</span><br><span class="line">    KafkaConsumer&lt;String, Customer&gt; consumer =</span><br><span class="line">      <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">    consumer.subscribe(<span class="string">"customerCountries"</span>)</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, Customer&gt; records =</span><br><span class="line">          consumer.poll(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, Customer&gt; record : records)</span><br><span class="line">        &#123;</span><br><span class="line">        System.out.println(<span class="string">"current customer Id: "</span> +</span><br><span class="line">        record.value().getId() + <span class="string">" and</span></span><br><span class="line"><span class="string">           current customer name: "</span> + record.value().getName());</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure><p>同样，重要的是要注意不建议实现自定义序列化器和反序列化器。 它紧密地耦合了生产者和消费者，并且易碎且容易出错。 ==更好的解决方案是使用标准消息格式，如JSON，Thrift，Protobuf或Avro==。 我们现在将看到如何将Avro反序列化器与Kafka消费者一起使用。 </p><h3 id="使用Avro反序列化与Kafka消费者"><a href="#使用Avro反序列化与Kafka消费者" class="headerlink" title="使用Avro反序列化与Kafka消费者"></a>使用Avro反序列化与Kafka消费者</h3><p>假设我们正在使用第3章中显示的Avro中Customer类的实现。为了从Kafka中使用这些对象，你希望实现类似于此的消费应用程序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>,</span><br><span class="line">       <span class="string">"io.confluent.kafka.serializers.KafkaAvroDeserializer"</span>); <span class="comment">//[1]</span></span><br><span class="line">    props.put(<span class="string">"schema.registry.url"</span>, schemaUrl); <span class="comment">//[2]</span></span><br><span class="line">    String topic = <span class="string">"customerContacts"</span></span><br><span class="line">    KafkaConsumer consumer = <span class="keyword">new</span></span><br><span class="line">       KafkaConsumer(createConsumerConfig(brokers, groupId, url));</span><br><span class="line">    consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line">    System.out.println(<span class="string">"Reading topic:"</span> + topic);</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, Customer&gt; records =</span><br><span class="line">          consumer.poll(<span class="number">1000</span>); <span class="comment">//[3]</span></span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, Customer&gt; record: records) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Current customer name is: "</span> +</span><br><span class="line">               record.value().getName()); <span class="comment">//[4]</span></span><br><span class="line">&#125;</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>[1] 我们使用KafkaAvroDeserializer来反序列化Avro消息。[2] schema.registry.url是一个新参数。 这只是指向我们存储模式的位置。 这样，消费者可以使用生产者注册的模式来反序列化消息。[3] 我们将生成的类Customer指定为记录值的类型。[4] record.value()是一个Customer实例，我们可以相应地使用它。</p></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html" target="_blank" rel="noopener">Chapter 4. Kafka Consumers: Reading Data from Kafka#Deserializers</a></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> Deserializer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于spark yarn模式下的常用属性</title>
      <link href="/2019/05/%E5%85%B3%E4%BA%8Espark%20yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E5%B8%B8%E7%94%A8%E5%B1%9E%E6%80%A7.html"/>
      <url>/2019/05/%E5%85%B3%E4%BA%8Espark%20yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E5%B8%B8%E7%94%A8%E5%B1%9E%E6%80%A7.html</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>整理了spark官网提供的一些常用的spark属性。</p><h3 id="Spark属性"><a href="#Spark属性" class="headerlink" title="Spark属性"></a>Spark属性</h3><table><thead><tr><th>属性名</th><th>默认值</th><th>描述</th></tr></thead><tbody align="left"><tr><td>spark.yarn.am.memory</td><td>512m</td><td>在Client模式下用于YARN Application Master的内存量，格式与JVM内存字符串相同（例如512m，2g）。<br>在Cluster模式下，请改用spark.driver.memory。使用小写后缀，例如 k，m，g，t和p分别为kibi-，mebi-，gibi-，tebi-和pebibytes。</td></tr><tr><td>spark.yarn.am.cores</td><td>1</td><td>在Client模式下用于YARN Application Master的核心数。 在群集模式下，请改用spark.driver.cores。</td></tr><tr><td>spark.yarn.am.waitTime</td><td>100s</td><td>仅在Cluster模式下使用。 YARN Application Master等待SparkContext初始化的时间。</td></tr><tr><td>spark.yarn.submit.file.replication</td><td>The default HDFS replication (usually 3)</td><td>为应用程序上载到HDFS的文件的HDFS复制级别。 这些包括Spark jar，app jar和任何分布式缓存文件/存档</td></tr><tr><td><b>spark.yarn.stagingDir</b></td><td>当前用户在文件系统中的主目录</td><td>提交应用程序时使用的暂存目录.</td></tr><tr><td>spark.yarn.preserve.staging.files</td><td>false</td><td>设置为true以在作业结束时保留暂存文件（Spark jar，app jar，分布式缓存文件），而不是删除它们。</td></tr><tr><td>spark.yarn.scheduler.heartbeat.interval-ms</td><td>3000</td><td>Spark应用程序主服务器心跳到YARN ResourceManager的时间间隔（毫秒）。 对于到期间隔，该值的上限为YARN配置值的一半，即yarn.am.liveness-monitor.expiry-interval-ms。</td></tr><tr><td>spark.yarn.scheduler.initial-allocation.interval</td><td>200ms</td><td>当存在待处理的容器分配请求时，Spark应用程序主机急切地检测到YARN ResourceManager的初始间隔。 它应该不大于spark.yarn.scheduler.heartbeat.interval-ms。 如果挂起的容器仍然存在，则分配间隔将在连续的急切心跳上加倍，直到达到spark.yarn.scheduler.heartbeat.interval-ms。</td></tr><tr><td><b>spark.yarn.max.executor.failures</b></td><td>numExecutors * 2, 最小为 3</td><td>应用程序失败之前的最大执行程序失败次数。</td></tr><tr><td>spark.yarn.dist.archives</td><td>(none)</td><td>以逗号分隔的archives列表，将其提取到每个执行程序的工作目录中</td></tr><tr><td>spark.yarn.dist.files</td><td>(none)</td><td>以逗号分隔的文件列表，放在每个执行程序的工作目录中。</td></tr><tr><td>spark.yarn.dist.jars</td><td>(none)</td><td>以逗号分隔的jar列表，放在每个执行程序的工作目录中。</td></tr><tr><td>spark.yarn.dist.forceDownloadSchemes</td><td>(none)</td><td>以逗号分隔的Scheme列表，在将资源添加到YARN的分布式缓存之前将资源下载到本地磁盘。 用于YARN服务不支持Spark支持的scheme的情况，例如http，https和ftp，或者需要位于本地YARN客户端类路径中的jar。 表示通配符’*'以下载所有方案的资源。</td></tr><tr><td><b>spark.executor.instances</b></td><td>2</td><td>静态分配的执行程序数。 使用spark.dynamicAllocation.enabled，初始执行程序集至少含有该数量的executor数</td></tr><tr><td><b>spark.yarn.am.memoryOverhead</b></td><td>AM memory * 0.10, 最少为384</td><td>与spark.driver.memoryOverhead相同，但对于Client模式下的YARN Application Master。</td></tr><tr><td><b>spark.yarn.queue</b></td><td>default</td><td>提交应用程序的YARN队列的名称。</td></tr><tr><td><b>spark.yarn.jars</b></td><td>(none)</td><td>包含要分发到YARN容器的Spark代码的库列表。 默认情况下，YARN上的Spark将使用本地安装的Spark jar，但Spark jar也可以位于HDFS上的世界可读位置。 这允许YARN将其缓存在节点上，这样每次应用程序运行时都不需要分发它。 例如，要指向HDFS上的jar，请将此配置设置为hdfs:///some/path。 允许使用全局变量。</td></tr><tr><td>spark.yarn.archive</td><td>(none)</td><td>包含所需Spark Spark的存档，以便分发到YARN缓存。 如果设置，则此配置将替换spark.yarn.jars，并且该存档将用于所有应用程序的容器中。 存档应在其根目录中包含jar文件。 与之前的选项一样，存档也可以托管在HDFS上以加速文件分发。</td></tr><tr><td>spark.yarn.appMasterEnv.[EnvironmentVariableName]</td><td>(none)</td><td>将EnvironmentVariableName指定的环境变量添加到在YARN上启动的Application Master进程。 用户可以指定其中的多个并设置多个环境变量。 在集群模式下，它控制Spark驱动程序的环境，在客户端模式下，它仅控制执行程序启动程序的环境。</td></tr><tr><td>spark.yarn.containerLauncherMaxThreads</td><td>25</td><td>YARN Application Master中用于启动执行程序容器的最大线程数。</td></tr><tr><td><b>spark.yarn.am.extraJavaOptions</b></td><td>(none)</td><td>在客户端模式下传递给YARN Application Master的一串额外JVM选项。 在群集模式下，请改用spark.driver.extraJavaOptions。 请注意，使用此选项设置最大堆大小（-Xmx）设置是非法的。 可以使用spark.yarn.am.memory设置最大堆大小设置</td></tr><tr><td><b>spark.yarn.am.extraLibraryPath</b></td><td>(none)</td><td>设置在客户端模式下启动YARN Application Master时要使用的额外库路径。</td></tr><tr><td><b>spark.yarn.maxAppAttempts</b></td><td>yarn.resourcemanager.am.max-attempts in YARN</td><td>提交申请的最大尝试次数。 它应该不大于YARN配置中的全局最大尝试次数</td></tr><tr><td>spark.yarn.am.attemptFailuresValidityInterval</td><td>(none)</td><td>定义AM故障跟踪的有效性间隔。 如果AM已运行至少定义的时间间隔，则AM故障计数将被重置。 如果未配置，则不启用此功能.</td></tr><tr><td>spark.yarn.executor.failuresValidityInterval</td><td>(none)</td><td>定义执行程序故障跟踪的有效性间隔。 将忽略早于有效期间隔的执行程序故障。</td></tr><tr><td>spark.yarn.submit.waitAppCompletion</td><td>true</td><td>在YARN群集模式下，控制客户端在应用程序完成之前是否等待退出。 如果设置为true，则客户端进程将保持活动状态，报告应用程序的状态。 否则，客户端进程将在提交后退出。</td></tr><tr><td>spark.yarn.am.nodeLabelExpression</td><td>(none)</td><td>将调度限制节点集AM的YARN节点标签表达式。 只有大于或等于2.6的YARN版本才支持节点标签表达式，因此在针对早期版本运行时，将忽略此属性。</td></tr><tr><td>spark.yarn.executor.nodeLabelExpression</td><td>(none)</td><td>将调度限制节点执行程序集的YARN节点标签表达式。 只有大于或等于2.6的YARN版本才支持节点标签表达式，因此在针对早期版本运行时，将忽略此属性。</td></tr><tr><td>spark.yarn.tags</td><td>(none)</td><td>以逗号分隔的字符串列表，作为YARN ApplicationReports中出现的YARN应用程序标记传递，可在查询YARN应用程序时用于过滤。</td></tr><tr><td>spark.yarn.blacklist.executor.launch.blacklisting.enabled</td><td>false</td><td>标记，以启用具有YARN资源分配问题的节点的黑名单。 可以通过spark.blacklist.application.maxFailedExecutorsPerNode配置列入黑名单的错误限制。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何限制Spark作业失败的重试次数</title>
      <link href="/2019/05/%E5%A6%82%E4%BD%95%E9%99%90%E5%88%B6Spark%E4%BD%9C%E4%B8%9A%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%87%8D%E8%AF%95%E6%AC%A1%E6%95%B0.html"/>
      <url>/2019/05/%E5%A6%82%E4%BD%95%E9%99%90%E5%88%B6Spark%E4%BD%9C%E4%B8%9A%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%87%8D%E8%AF%95%E6%AC%A1%E6%95%B0.html</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近有个spark程序因为资源不足以及其他原因会在第一次提交时候失败，然后又会不断提交，导致过多的系统资源被无效占用。因此想限制Spark作业失败的重试次数，如果第一次失败，就让作业直接失败，那么该具体该如何实现呢？</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>首先查看了spark的属性配置，发现我们使用<code>spark.yarn.maxAppAttempts</code>属性在提交程序时限制其重试次数，如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --conf spark.yarn.maxAppAttempts=<span class="number">1</span></span><br></pre></td></tr></table></figure><p>该属性在Spark源代码中的使用如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.spark.deploy.yarn.config.scala</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] val MAX_APP_ATTEMPTS = ConfigBuilder(<span class="string">"spark.yarn.maxAppAttempts"</span>)</span><br><span class="line">    .doc(<span class="string">"Maximum number of AM attempts before failing the app."</span>)</span><br><span class="line">    .intConf</span><br></pre></td></tr></table></figure><p>因为我们使用的是Spark on Yarn，虽然由Yarn负责启动和管理AM以及分配资源，但是Spark有自己的AM实现，当Executor运行起来后，任务的控制是由Driver负责的。而重试上，==Yarn只负责<strong>AM</strong>的重试==。</p><p>另外，在Spark对ApplicationMaster的实现里，Spark提供了参数 spark.yarn.max.executor.failures 来控制Executor的失败次数，当Executor的失败次数达到这个值的时候，整个Spark应该程序就失败了，判断逻辑如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.spark.deploy.yarn.ApplicationMaster.scala</span></span><br><span class="line"><span class="keyword">private</span> val maxNumExecutorFailures = &#123;</span><br><span class="line">    val effectiveNumExecutors =</span><br><span class="line">      <span class="keyword">if</span> (Utils.isStreamingDynamicAllocationEnabled(sparkConf)) &#123;</span><br><span class="line">        sparkConf.get(STREAMING_DYN_ALLOCATION_MAX_EXECUTORS)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (Utils.isDynamicAllocationEnabled(sparkConf)) &#123;</span><br><span class="line">        sparkConf.get(DYN_ALLOCATION_MAX_EXECUTORS)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        sparkConf.get(EXECUTOR_INSTANCES).getOrElse(<span class="number">0</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">// By default, effectiveNumExecutors is Int.MaxValue if dynamic allocation is enabled. We need</span></span><br><span class="line">    <span class="comment">// avoid the integer overflow here.</span></span><br><span class="line">    val defaultMaxNumExecutorFailures = math.max(<span class="number">3</span>,</span><br><span class="line">      <span class="keyword">if</span> (effectiveNumExecutors &gt; Int.MaxValue / <span class="number">2</span>) Int.<span class="function">MaxValue <span class="title">else</span> <span class="params">(<span class="number">2</span> * effectiveNumExecutors)</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    sparkConf.<span class="title">get</span><span class="params">(MAX_EXECUTOR_FAILURES)</span>.<span class="title">getOrElse</span><span class="params">(defaultMaxNumExecutorFailures)</span></span></span><br><span class="line"><span class="function">  &#125;</span></span><br></pre></td></tr></table></figure><p>以上相关Spark属性的定义如下:|属性名|默认值    |解释||--|--|--||<strong>spark.yarn.maxAppAttempts</strong>|YARN配置中的yarn.resourcemanager.am.max-attempts属性的值|提交申请的最大尝试次数, 小于等于YARN配置中的全局最大尝试次数。||<strong>spark.yarn.max.executor.failures</strong>|numExecutors * 2, with minimum of 3|应用程序失败之前的最大执行程序失败次数。|</p><p>而在YARN配置中，我们可以看到：|属性名|默认值    |解释||--|--|--||<strong>yarn.resourcemanager.am.max-attempts</strong>    |2    |最大应用程序尝试次数。 它是所有AM的全局设置。 每个应用程序主机都可以通过API指定其各自的最大应用程序尝试次数，但是单个数字不能超过全局上限。 如果是，资源管理器将覆盖它。 默认数量设置为2，以允许至少一次重试AM.|</p><p>所以默认情况下，spark.yarn.maxAppAttempts的值为2，如果想不进行第二次重试，可以将改值设为<code>1</code>(==注意，0值是无效的，至少为提交一次==)</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记一个FPGrowth的简单例子</title>
      <link href="/2019/05/%E8%AE%B0%E4%B8%80%E4%B8%AAFPGrowth%E7%9A%84%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90.html"/>
      <url>/2019/05/%E8%AE%B0%E4%B8%80%E4%B8%AAFPGrowth%E7%9A%84%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90.html</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Frequent Pattern Mining挖掘频繁项目，项目集，子序列或其他子结构通常是分析大规模数据集的第一步，这是数据挖掘多年来一直活跃的研究课题。这也是关联规则挖掘的常用算法之一。</p><h3 id="关联规则中的一些基本概念"><a href="#关联规则中的一些基本概念" class="headerlink" title="关联规则中的一些基本概念"></a>关联规则中的一些基本概念</h3><ul><li><p>关联规则：用于表示数据内隐含的关联性，一般用X表示先决条件，Y表示关联结果。</p></li><li><p>支持度(Support)：所有项集中{X,Y}出现的可能性。</p></li><li><p>置信度(Confidence)：先决条件X发生的条件下，关联结果Y发生的概率。</p></li></ul><h3 id="Apriori算法"><a href="#Apriori算法" class="headerlink" title="Apriori算法"></a>Apriori算法</h3><p>Apriori算法是常用的关联规则挖掘算法，基本思想是：</p><p>(1) 先搜索出1项集及其对应的支持度，删除低于支持度的项集，得到频繁1项集L1；</p><p>(2) 对L1中的项集进行连接，得到一个候选集，删除其中低于支持度的项集，得到频繁1项集L2；</p><p>...</p><p>迭代下去，一直到无法找到L(k+1)为止，对应的频繁k项集集合就是最后的结果。</p><p>Apriori算法的缺点是对于候选项集里面的每一项都要扫描一次数据，从而需要多次扫描数据，I/O操作多，效率低。为了提高效率，提出了一些基于Apriori的算法，比如FPGrowth算法。</p><h3 id="FP-Growth"><a href="#FP-Growth" class="headerlink" title="FP-Growth"></a>FP-Growth</h3><p>FPGrowth算法为了减少I/O操作，提高效率，引入了一些数据结构存储数据，主要包括项头表、FP-Tree和节点链表。在spark.ml的FP-growth算法主要有以下参数：</p><ul><li><strong>minSupport</strong>：对项目集进行频繁识别的最低支持。 例如，如果一个项目出现在5个交易中的3个中，则它具有3/5 = 0.6的支持。</li><li><strong>minConfidence</strong>：生成关联规则的最小置信度。 置信度表明关联规则经常被发现的频率。 例如，如果在交易项目集X中出现4次，X和Y仅出现2次，则规则X =&gt; Y的置信度则为2/4 = 0.5。 该参数不会影响频繁项集的挖掘，但会指定从频繁项集生成关联规则的最小置信度。</li><li><strong>numPartitions</strong>：用于分发工作的分区数。 默认情况下，不设置参数，并使用输入数据集的分区数。</li></ul><p>而输出的模型则包含：</p><ul><li><strong>freqItemsets</strong>：频繁项集，格式为<code>DataFrame(“items”[Array], “freq”[Long])</code></li><li><strong>associationRules</strong>：以高于minConfidence的置信度生成的关联规则，格式为<code>DataFrame(“antecedent”[Array], “consequent”[Array], “confidence”[Double])</code>.</li><li><strong>transform</strong>：对于itemsCol中的每个事务，transform方法将其项目与每个关联规则的前提进行比较。 如果记录包含特定关联规则的所有前提(规则中的前项)，则该规则将被视为适用，并且其结果将被添加到预测结果中。 transform方法将所有适用规则的结果总结为预测。 预测列与itemsCol具有相同的数据类型，==并且不包含itemsCol中的现有项==。</li></ul><p>==Spark mllib中的<code>AssociationRules</code>方法产生的规则的consequent<strong>只含有一个item</strong>==</p><h3 id="Spark使用示例"><a href="#Spark使用示例" class="headerlink" title="Spark使用示例"></a>Spark使用示例</h3><p>以下为一个spark中FPGrowth使用的简单示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.fpm.FPGrowth</span><br><span class="line"></span><br><span class="line">val dataset = spark.createDataset(Seq(</span><br><span class="line"><span class="string">"D E"</span>,</span><br><span class="line"><span class="string">"A B C"</span>,</span><br><span class="line"><span class="string">"A B C E"</span>,</span><br><span class="line"><span class="string">"B E"</span>,</span><br><span class="line"><span class="string">"C D E"</span>,</span><br><span class="line"><span class="string">"A B C"</span>,</span><br><span class="line"><span class="string">"A B C E"</span>,</span><br><span class="line"><span class="string">"B E"</span>,</span><br><span class="line"><span class="string">"F G"</span>,</span><br><span class="line"><span class="string">"D F"</span>)</span><br><span class="line">).map(t =&gt; t.split(<span class="string">" "</span>)).toDF(<span class="string">"items"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val fpgrowth = <span class="keyword">new</span> FPGrowth().setItemsCol(<span class="string">"items"</span>).setMinSupport(<span class="number">0.3</span>) </span><br><span class="line"><span class="comment">//val fpgrowth = new FPGrowth().setItemsCol("items").setMinSupport(0.3) .setMinConfidence(0.6)</span></span><br><span class="line">val model = fpgrowth.fit(dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Display frequent itemsets.</span></span><br><span class="line">model.freqItemsets.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Display generated association rules.</span></span><br><span class="line">model.associationRules.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// transform examines the input items against all the association rules and summarize the</span></span><br><span class="line"><span class="comment">// consequents as prediction</span></span><br><span class="line">model.transform(dataset).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">//save model</span></span><br><span class="line">model.save(<span class="string">"/tmp/fp.ml"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//load saved model</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.fpm.FPGrowthModel</span><br><span class="line">val savedModel = FPGrowthModel.load(<span class="string">"/tmp/fp.ml"</span>)</span><br></pre></td></tr></table></figure><p>结果如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; model.freqItemsets.show()</span><br><span class="line">+---------+----+</span><br><span class="line">|    items|freq|</span><br><span class="line">+---------+----+</span><br><span class="line">|      [B]|   <span class="number">6</span>|</span><br><span class="line">|      [E]|   <span class="number">6</span>|</span><br><span class="line">|   [E, B]|   <span class="number">4</span>|</span><br><span class="line">|      [C]|   <span class="number">5</span>|</span><br><span class="line">|   [C, E]|   <span class="number">3</span>|</span><br><span class="line">|   [C, B]|   <span class="number">4</span>|</span><br><span class="line">|      [A]|   <span class="number">4</span>|</span><br><span class="line">|   [A, C]|   <span class="number">4</span>|</span><br><span class="line">|[A, C, B]|   <span class="number">4</span>|</span><br><span class="line">|   [A, B]|   <span class="number">4</span>|</span><br><span class="line">|      [D]|   <span class="number">3</span>|</span><br><span class="line">+---------+----+</span><br><span class="line">scala&gt; model.associationRules.show()</span><br><span class="line">+----------+----------+----------+------------------+</span><br><span class="line">|antecedent|consequent|confidence|              lift|</span><br><span class="line">+----------+----------+----------+------------------+</span><br><span class="line">|    [A, C]|       [B]|       <span class="number">1.0</span>|<span class="number">1.6666666666666667</span>|</span><br><span class="line">|    [C, B]|       [A]|       <span class="number">1.0</span>|               <span class="number">2.5</span>|</span><br><span class="line">|       [C]|       [B]|       <span class="number">0.8</span>|<span class="number">1.3333333333333335</span>|</span><br><span class="line">|       [C]|       [A]|       <span class="number">0.8</span>|               <span class="number">2.0</span>|</span><br><span class="line">|    [A, B]|       [C]|       <span class="number">1.0</span>|               <span class="number">2.0</span>|</span><br><span class="line">|       [A]|       [C]|       <span class="number">1.0</span>|               <span class="number">2.0</span>|</span><br><span class="line">|       [A]|       [B]|       <span class="number">1.0</span>|<span class="number">1.6666666666666667</span>|</span><br><span class="line">+----------+----------+----------+------------------+</span><br><span class="line">scala&gt;model.transform(dataset).show()</span><br><span class="line">+------------+----------+</span><br><span class="line">|       items|prediction|</span><br><span class="line">+------------+----------+</span><br><span class="line">|      [D, E]|        []|</span><br><span class="line">|   [A, B, C]|        []|</span><br><span class="line">|[A, B, C, E]|        []|</span><br><span class="line">|      [B, E]|        []|</span><br><span class="line">|   [C, D, E]|    [B, A]|</span><br><span class="line">|   [A, B, C]|        []|</span><br><span class="line">|[A, B, C, E]|        []|</span><br><span class="line">|      [B, E]|        []|</span><br><span class="line">|      [F, G]|        []|</span><br><span class="line">|      [D, F]|        []|</span><br><span class="line">+------------+----------+</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> FPGrowth </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PySpark与Java_Scala交互中的一些常用的Spark对象</title>
      <link href="/2019/05/PySpark%E4%B8%8EJava_Scala%E4%BA%A4%E4%BA%92%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84Spark%E5%AF%B9%E8%B1%A1.html"/>
      <url>/2019/05/PySpark%E4%B8%8EJava_Scala%E4%BA%A4%E4%BA%92%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84Spark%E5%AF%B9%E8%B1%A1.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在PySpark中调用Scala/Java代码时，我们会遇到一些负载的对象需要传递，例如一些spark对象。在环境之间(Python / Java)传递时，我们必须将Spark对象显式地包装/提取到java对象中。下面是一些常用的Spark对象。</p><h2 id="一些常用的Spark对象"><a href="#一些常用的Spark对象" class="headerlink" title="一些常用的Spark对象"></a>一些常用的Spark对象</h2><h3 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h3><p>如果你的Scala代码需要访问SparkContext(sc)，则你的python代码必须传递<code>sc._jsc</code>，并且Scala方法应该接收JavaSparkContext参数并将其解压缩到Scala SparkContext。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">method</span><span class="params">(jsc: JavaSparkContext)</span> </span>= &#123;</span><br><span class="line">    val sc = JavaSparkContext.toSparkContext(jsc)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="SQLContext"><a href="#SQLContext" class="headerlink" title="SQLContext"></a>SQLContext</h3><p>对于SQLContext，我们可以通过发送<code>sqlContext._ssql_ctx</code>从python传递Scala SQLContext。 它可以直接在Scala端使用。</p><h3 id="RDDS"><a href="#RDDS" class="headerlink" title="RDDS"></a>RDDS</h3><p>你可以通过r<code>dd._jrdd</code>将它们从Python传递到Scala。 在Scala端，可以通过访问<code>jrdd.rdd</code>来提取JavaRDD(jrdd)。 而将它转换回Python时，我们可以使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.rdd <span class="keyword">import</span> RDD</span><br><span class="line"></span><br><span class="line">pythonRDD = RDD(jrdd, sc)</span><br></pre></td></tr></table></figure><h3 id="DataFrames"><a href="#DataFrames" class="headerlink" title="DataFrames"></a>DataFrames</h3><p>要从python发送DataFrame(df)，必须传递<code>df._jdf</code>属性。 而将Scala DataFrame返回给python时，可以在python端转换为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> DataFrame</span><br><span class="line"></span><br><span class="line">pythonDf = DataFrame(jdf, sqlContext)</span><br></pre></td></tr></table></figure><p>DataFrames也可以通过使用registerTempTable并通过sqlContext访问它们。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://diogoalexandrefranco.github.io/scala-code-in-pyspark/" target="_blank" rel="noopener">Using Scala code in PySpark applications</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何在PySpark中调用Scala_Java代码</title>
      <link href="/2019/05/%E5%A6%82%E4%BD%95%E5%9C%A8PySpark%E4%B8%AD%E8%B0%83%E7%94%A8Scala_Java%E4%BB%A3%E7%A0%81.html"/>
      <url>/2019/05/%E5%A6%82%E4%BD%95%E5%9C%A8PySpark%E4%B8%AD%E8%B0%83%E7%94%A8Scala_Java%E4%BB%A3%E7%A0%81.html</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>虽然有充分的理由使用Python API开发Spark应用程序，但不可否认的是Scala是Spark的原生语言，有一些功能或第三方库并没有直接提供python版本。那么当我们想要使用一些PySpark不支持的功能，或者只是想在Python应用程序中使用Scala库该怎么办呢？下面的示例展示了如何在PySpark应用程序中调用Scala代码。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>Pyspark在解释器和JVM之间建立了一个geteway ，也就是 Py4J 。我们可以用它来操作Java对象。 下面让我们编写一个最简单的Scala对象：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.test.spark</span><br><span class="line">object SayHello &#123;</span><br><span class="line">    def ditBonjour = println(<span class="string">"Bonjour"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后，我们使用maven或sbt等工具构建它并将其打包为JAR：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mvn package</span><br><span class="line">Building jar: .../target/testspark-0.1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>接下来，我们可以在启动pyspark shell的时候，使用<code>--driver-class-path</code>添加这个jar包。 同时，我们可能还需要在<code>--jars</code>参数包含这个jar包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master yarn --deploy-mode client --jars testspark-0.1.0-SNAPSHOT.jar --driver-class-path testspark-0.1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>然后在交互界面中， 我们可以通过访问spark context（sc）的_jvm属性来访问我们的包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; SayHello = sc._jvm.com.test.spark.SayHello</span><br><span class="line">&gt;&gt;&gt; SayHello.ditBonjour()</span><br><span class="line">Bonjour</span><br></pre></td></tr></table></figure><p>Voilà，我们已经成功地从PySpark调用了我们的第一个Scala方法！</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://aseigneurin.github.io/2016/09/01/spark-calling-scala-code-from-pyspark.html" target="_blank" rel="noopener">Spark - Calling Scala code from PySpark</a><a href="https://diogoalexandrefranco.github.io/scala-code-in-pyspark/" target="_blank" rel="noopener">Using Scala code in PySpark applications</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何在pyspark中调用Scala或Java编写的UDF</title>
      <link href="/2019/05/%E5%A6%82%E4%BD%95%E5%9C%A8python(pyspark)%E4%B8%AD%E8%B0%83%E7%94%A8Scala%E6%88%96Java%E7%BC%96%E5%86%99%E7%9A%84UDF.html"/>
      <url>/2019/05/%E5%A6%82%E4%BD%95%E5%9C%A8python(pyspark)%E4%B8%AD%E8%B0%83%E7%94%A8Scala%E6%88%96Java%E7%BC%96%E5%86%99%E7%9A%84UDF.html</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>启动Python进程的开销不小[1]，但是真正的开销在于将数据序列化到Python中。推荐在Spark中定义UDF时首选Scala或Java，即使UDFs是用Scala/Java编写的，不用担心，我们依然可以在python(pyspark)中使用它们，简单示例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//My_Upper UDF</span></span><br><span class="line"><span class="keyword">package</span> com.test.spark.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.api.java.UDF1</span><br><span class="line"></span><br><span class="line">class MyUpper extends UDF1[String, String] &#123;</span><br><span class="line">  </span><br><span class="line"> <span class="function">override def <span class="title">call</span><span class="params">(input: String)</span>:Sting </span>= input.toUpperCase</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### pyspark --jars [path/to/jar/x.jar]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pre Spark 2.1</span></span><br><span class="line">spark._jvm.com.test.spark.udf.MyUpper.registerUDF(spark._jsparkSession)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.1+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">sqlContext.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.3+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">spark.udf.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your UDF</span></span><br><span class="line">spark.sql(<span class="string">"""SELECT my_upper('abeD123okoj')"""</span>).show()</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]: <a href="https://blog.csdn.net/yolohohohoho/article/details/88662805" target="_blank" rel="noopener">为什么建议在Spark中使用Scala定义UDF</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于org.apache.spark.deploy.SparkSubmitArguments</title>
      <link href="/2019/05/%E5%85%B3%E4%BA%8Eorg.apache.spark.deploy.SparkSubmitArguments.html"/>
      <url>/2019/05/%E5%85%B3%E4%BA%8Eorg.apache.spark.deploy.SparkSubmitArguments.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个类主要功能是从spark-submit命令行中解析并封装提交的的参数。Github源码地址：<a href="https://github.com/apache/spark/blob/05168e725d2a17c4164ee5f9aa068801ec2454f4/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/05168e725d2a17c4164ee5f9aa068801ec2454f4/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala</a></p><h2 id="主要逻辑"><a href="#主要逻辑" class="headerlink" title="主要逻辑"></a>主要逻辑</h2><p><code>defaultSparkProperties</code>载入默认属性, Spark默认属性存在于当前定义的默认文件中。</p><p>使用<code>parse(args.asJava)</code>, 提取命令行提交的参数并对Spark相关属性进行设置</p><p><code>mergeDefaultSparkProperties()</code>从属性文件填充==sparkProperties==所含映射键值对</p><p><code>ignoreNonSparkProperties()</code>,删除==sparkProperties==中不以“spark”开头的键</p><p><code>loadEnvironmentArguments()</code>,使用==sparkProperties==映射和env变量来填充任何缺少的参数</p><p><code>validateArguments()</code>,  确保所有必要的字段都存在，该方法只有在加载所有默认值后才会被调用此方法</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><h3 id="mergeDefaultSparkProperties"><a href="#mergeDefaultSparkProperties" class="headerlink" title="mergeDefaultSparkProperties"></a>mergeDefaultSparkProperties</h3><h3 id="ignoreNonSparkProperties"><a href="#ignoreNonSparkProperties" class="headerlink" title="ignoreNonSparkProperties"></a>ignoreNonSparkProperties</h3><h3 id="loadEnvironmentArguments"><a href="#loadEnvironmentArguments" class="headerlink" title="loadEnvironmentArguments"></a>loadEnvironmentArguments</h3><p>载入主要的spark属性，参考下表。其中</p><ul><li>如果没有给出<code>--class</code>参数，请尝试依据提供的JAR包设置主类</li><li>mastet的全局默认值为 <code>master = Option(master).getOrElse(&quot;local[*]&quot;)</code>。 这些应保持最小，以避免混淆行为。</li><li>在YARN模式下，可以通过<code>SPARK_YARN_APP_NAME</code>设置应用名称</li><li>除非另有说明，否则应默认的action是<strong>SUBMIT</strong><table><thead><tr><th>属性</th><th>获取方法</th></tr></thead><tbody align="left"><tr><td> master</td><td>Option(master).orElse(sparkProperties.get(“spark.master”)).orElse(env.get(“MASTER”)).orNull</td></tr><tr><td>driverExtraClassPath</td><td>Option(driverExtraClassPath).orElse(sparkProperties.get(config.DRIVER_CLASS_PATH.key)).orNull</td></tr><tr><td>driverExtraJavaOptions</td><td>Option(driverExtraJavaOptions).orElse(sparkProperties.get(config.DRIVER_JAVA_OPTIONS.key)).orNull</td></tr><tr><td>driverExtraLibraryPath</td><td>Option(driverExtraLibraryPath).orElse(sparkProperties.get(config.DRIVER_LIBRARY_PATH.key)).orNull</td></tr><tr><td>driverMemory</td><td>Option(driverMemory).orElse(sparkProperties.get(config.DRIVER_MEMORY.key)).orElse(env.get(“SPARK_DRIVER_MEMORY”)).orNull</td></tr><tr><td>driverCores</td><td>Option(driverCores).orElse(sparkProperties.get(config.DRIVER_CORES.key)).orNull</td></tr><tr><td>executorMemory</td><td>Option(executorMemory).orElse(sparkProperties.get(config.EXECUTOR_MEMORY.key)).orElse(env.get(“SPARK_EXECUTOR_MEMORY”)).orNull</td></tr><tr><td>executorCores</td><td>Option(executorCores).orElse(sparkProperties.get(config.EXECUTOR_CORES.key)).orElse(env.get(“SPARK_EXECUTOR_CORES”)).orNull</td></tr><tr><td>totalExecutorCores</td><td>Option(totalExecutorCores).orElse(sparkProperties.get(config.CORES_MAX.key)).orNull</td></tr><tr><td>name</td><td>Option(name).orElse(sparkProperties.get(“<a href="http://spark.app.name" target="_blank" rel="noopener">spark.app.name</a>”)).orNull</td></tr><tr><td>jars</td><td>Option(jars).orElse(sparkProperties.get(config.JARS.key)).orNull</td></tr><tr><td>files</td><td>Option(files).orElse(sparkProperties.get(config.FILES.key)).orNull</td></tr><tr><td>pyFiles</td><td>Option(pyFiles).orElse(sparkProperties.get(config.SUBMIT_PYTHON_FILES.key)).orNull</td></tr><tr><td>ivyRepoPath</td><td>sparkProperties.get(“spark.jars.ivy”).orNull</td></tr><tr><td>ivySettingsPath</td><td>sparkProperties.get(“spark.jars.ivySettings”)</td></tr><tr><td>packages</td><td>Option(packages).orElse(sparkProperties.get(“spark.jars.packages”)).orNull</td></tr><tr><td>packagesExclusions</td><td>Option(packagesExclusions).orElse(sparkProperties.get(“spark.jars.excludes”)).orNull</td></tr><tr><td>repositories</td><td>Option(repositories).orElse(sparkProperties.get(“spark.jars.repositories”)).orNull</td></tr><tr><td>deployMode</td><td>Option(deployMode).orElse(sparkProperties.get(config.SUBMIT_DEPLOY_MODE.key)).orElse(env.get(“DEPLOY_MODE”)).orNull</td></tr><tr><td>numExecutors</td><td>Option(numExecutors).getOrElse(sparkProperties.get(config.EXECUTOR_INSTANCES.key).orNull)</td></tr><tr><td>queue</td><td>Option(queue).orElse(sparkProperties.get(“spark.yarn.queue”)).orNull</td></tr><tr><td>keytab</td><td>Option(keytab).orElse(sparkProperties.get(“spark.kerberos.keytab”)).orElse(sparkProperties.get(“spark.yarn.keytab”)).orNull</td></tr><tr><td>principal</td><td>Option(principal).orElse(sparkProperties.get(“spark.kerberos.principal”)).orElse(sparkProperties.get(“spark.yarn.principal”)).orNull</td></tr><tr><td>dynamicAllocationEnabled</td><td>sparkProperties.get(DYN_ALLOCATION_ENABLED.key).exists(“true”.equalsIgnoreCase)</td></tr></tbody></table></li></ul><h3 id="validateArguments"><a href="#validateArguments" class="headerlink" title="validateArguments"></a>validateArguments</h3><h4 id="validateSubmitArguments"><a href="#validateSubmitArguments" class="headerlink" title="validateSubmitArguments"></a>validateSubmitArguments</h4><h4 id="validateKillArguments"><a href="#validateKillArguments" class="headerlink" title="validateKillArguments"></a>validateKillArguments</h4><h4 id="validateStatusRequestArguments"><a href="#validateStatusRequestArguments" class="headerlink" title="validateStatusRequestArguments"></a>validateStatusRequestArguments</h4><h3 id="isStandaloneCluster"><a href="#isStandaloneCluster" class="headerlink" title="isStandaloneCluster"></a>isStandaloneCluster</h3><h3 id="handle"><a href="#handle" class="headerlink" title="handle"></a>handle</h3><p>通过解析用户选项来填填充相应属性值。</p><h3 id="handleUnknown"><a href="#handleUnknown" class="headerlink" title="handleUnknown"></a>handleUnknown</h3><p>处理无法识别的命令行选项。其中第一个无法识别的选项被视为“主要资源”。 其他所有内容都被视为应用程序参数。</p><h3 id="handleExtraArgs"><a href="#handleExtraArgs" class="headerlink" title="handleExtraArgs"></a>handleExtraArgs</h3><h3 id="printUsageAndExit"><a href="#printUsageAndExit" class="headerlink" title="printUsageAndExit"></a>printUsageAndExit</h3><p>打印帮助并退出</p><h3 id="toSparkConf"><a href="#toSparkConf" class="headerlink" title="toSparkConf"></a>toSparkConf</h3><p>将SparkSubmitArguments转化为SparkConf，如果该属性不存在，则创建新的空配置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[deploy] <span class="function">def <span class="title">toSparkConf</span><span class="params">(sparkConf: Option[SparkConf] = None)</span>: SparkConf </span>= &#123;</span><br><span class="line">    <span class="comment">// either use an existing config or create a new empty one</span></span><br><span class="line">    sparkProperties.foldLeft(sparkConf.getOrElse(<span class="keyword">new</span> SparkConf())) &#123;</span><br><span class="line">      <span class="keyword">case</span> (conf, (k, v)) =&gt; conf.set(k, v)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何在pyspark中与HDFS交互</title>
      <link href="/2019/05/%E5%A6%82%E4%BD%95%E5%9C%A8pyspark%E4%B8%AD%E4%B8%8EHDFS%E4%BA%A4%E4%BA%92.html"/>
      <url>/2019/05/%E5%A6%82%E4%BD%95%E5%9C%A8pyspark%E4%B8%AD%E4%B8%8EHDFS%E4%BA%A4%E4%BA%92.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们经常需要从Spark应用程序执行HDFS操作，无论是在HDFS中列出文件还是删除数据。如果使用scala写spark程序的话，我们可以调用hadoop相关的jar包对hdfs进行操作，但在Python Spark API（PySpark）并不能立即实现这一点，那么在pyspark中我们又该如何操作呢？下面介绍了几种在pyspark中操作HDFS方法。</p><h2 id="使用Java-Gateway"><a href="#使用Java-Gateway" class="headerlink" title="使用Java Gateway"></a>使用Java Gateway</h2><p>即使使用Python编写的Spark应用程序，它也是依赖于JVM，使用Py4J来执行可以与JVM对象交互的Python代码。 Py4J使用JVM和Python解释器之间的Gateway，可以从应用程序的SparkContext（sc）对象访问它，参考代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In python</span></span><br><span class="line"><span class="comment">######</span></span><br><span class="line"><span class="comment"># Get fs handler from java gateway</span></span><br><span class="line"><span class="comment">######</span></span><br><span class="line">URI = sc._gateway.jvm.java.net.URI</span><br><span class="line">Path = sc._gateway.jvm.org.apache.hadoop.fs.Path</span><br><span class="line">FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem</span><br><span class="line">fs = FileSystem.get(URI(<span class="string">"hdfs://somehost:8020"</span>), sc._jsc.hadoopConfiguration())</span><br></pre></td></tr></table></figure><p>虽然这种策略看起来不太优雅，但它很有用，因为它不需要任何第三方库。</p><h2 id="使用第三方库"><a href="#使用第三方库" class="headerlink" title="使用第三方库"></a>使用第三方库</h2><p>如果使用外部库不是问题，那么使PySpark与HDFS交互的另一种方法就是简单地使用原始Python库, 如<code>hdfs</code>包或者<code>snakebite</code>包，示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hdfs <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line"><span class="comment"># The following assumes you have hdfscli.cfg file defining a 'dev' client.</span></span><br><span class="line">client = Config().get_client(<span class="string">'dev'</span>)</span><br><span class="line">files = client.list(<span class="string">'the_dir_path'</span>)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> snakebite.client <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line">client = Client(hdfs_hostname, hdfs_port)</span><br><span class="line">client.delete(<span class="string">'/some-path'</span>, recurse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="使用subprocesses子进程"><a href="#使用subprocesses子进程" class="headerlink" title="使用subprocesses子进程"></a>使用subprocesses子进程</h2><p>我们可以使用subprocesses工具直接完成HDFS交互，这允许Python调用任意shell命令。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line">cmd = <span class="string">'hdfs dfs -ls /user/path'</span>.split() <span class="comment"># cmd must be an array of arguments</span></span><br><span class="line">files = subprocess.check_output(cmd).strip().split(<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">for</span> path <span class="keyword">in</span> files:</span><br><span class="line">  <span class="keyword">print</span> (path)</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://diogoalexandrefranco.github.io/interacting-with-hdfs-from-pyspark/" target="_blank" rel="noopener">Interacting With HDFS from PySpark</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何解决Sparksession/HiveContext访问不到Hive表中新插入的记录, 或者元数据不一致的问题</title>
      <link href="/2019/04/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3Sparksession_HiveContext%E8%AE%BF%E9%97%AE%E4%B8%8D%E5%88%B0Hive%E8%A1%A8%E4%B8%AD%E6%96%B0%E6%8F%92%E5%85%A5%E7%9A%84%E8%AE%B0%E5%BD%95,%20%E6%88%96%E8%80%85%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98.html"/>
      <url>/2019/04/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3Sparksession_HiveContext%E8%AE%BF%E9%97%AE%E4%B8%8D%E5%88%B0Hive%E8%A1%A8%E4%B8%AD%E6%96%B0%E6%8F%92%E5%85%A5%E7%9A%84%E8%AE%B0%E5%BD%95,%20%E6%88%96%E8%80%85%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
      
        <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>在做某个POC项目的测试时候，先pyspark2界面count表A一共有3条，在另一个界面用beeline往这种表插入一条数据，然后继续在beeline中count，此时显示总数为4，插入正常。但是回到pyspark2的界面执行sql语句时候，发现还是3条。</p><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>首先看下以下几个知识点：<strong>Hive Metastore</strong>当使用<code>enableHiveSupport</code>创建SparkSession时，外部目录（也称为Metastore）是==HiveExternalCatalog==。 HiveExternalCatalog使用spark.sql.warehouse.dir目录来获取数据库的位置，并使用javax.jdo.option属性来连接到Hive Metastore数据库。</p><blockquote><p>使用外部Hive Metastore的好处：</p><ul><li>允许多个Spark应用程序（会话）同时访问它</li><li>允许单个Spark应用程序使用表统计信息，而无需在每次执行时运行“ANALYZE TABLE”</li></ul></blockquote><p><strong>CatalogStatistics</strong> CatalogStatistics是存储在外部目录（也称为Metastore）中的表统计信息：</p><ul><li>物理总大小（以字节为单位）</li><li>估计行数（又名行数）</li><li>列统计信息（即列名称及其统计信息）</li></ul><blockquote><p>CatalogStatistics通常存储在Hive Metastore中，称为Hive统计信息，而Statistics是Spark统计信息。</p></blockquote><p>简单来说就是，Spark SQL会缓存Hive Metastore数据。如果更新发生在Spark SQL之外(Hive或者其他外部工具)，那么我们可能会遇到一些意外结果，因为Spark SQL的Hive Metastore版本不是最新的。就比如我遇到的问题，表的count信息没有被正确的获得。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>我们需要手动刷新这些表或者视图以确保元数据一致。</p><p>例子如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// spark is an existing SparkSession</span></span><br><span class="line">spark.catalog.refreshTable(<span class="string">"my_table"</span>)</span><br></pre></td></tr></table></figure><blockquote><p>对于临时或持久的VIEW表，refreshTable请求分析的DataFrame逻辑计划（对于表）刷新自身。对于其他类型的表，refreshTable请求SessionCatalog刷新表元数据（即使表无效）。如果表已被缓存，则refreshTable请求CacheManager取消缓存并再次缓存表DataFrame。</p></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://stackoverflow.com/questions/34661547/unable-to-view-data-of-hive-tables-after-update-in-spark" target="_blank" rel="noopener">unable to view data of hive tables after update in spark</a><a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html" target="_blank" rel="noopener">Metadata Refreshing</a><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Catalog.html" target="_blank" rel="noopener">CatalogStatistics — Table Statistics From External Catalog (Metastore)</a><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-hive-metastore.html" target="_blank" rel="noopener">Hive Metastore</a><a href="http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html" target="_blank" rel="noopener">Hive Tables</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Spark Dataset API中的Typed transformations和Untyped transformations</title>
      <link href="/2019/03/%E5%85%B3%E4%BA%8ESpark%20Dataset%20API%E4%B8%AD%E7%9A%84Typed%20transformations%E5%92%8CUntyped%20transformations.html"/>
      <url>/2019/03/%E5%85%B3%E4%BA%8ESpark%20Dataset%20API%E4%B8%AD%E7%9A%84Typed%20transformations%E5%92%8CUntyped%20transformations.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习Spark源代码的过程中遇到了Typed transformations和Untyped transformations两个概念，整理了以下相关的笔记。对于这两个概念，不知道怎么翻译好，个人理解为强类型转换和弱类型转换，也不知道对不对，欢迎各位大神指正。</p><h2 id="关于Dataset"><a href="#关于Dataset" class="headerlink" title="关于Dataset"></a>关于Dataset</h2><p>Dataset是特定领域对象(domain-specific object)的强类型集合，它可以使用函数或关系运算进行并行转换。 每个Dataset还有一个名为DataFrame的弱类型视图，相当于<code>Dataset[Row]</code>。对于Spark(Scala)，DataFrames只是类型为Row的Dataset。 “Row”类型是Spark中用于计算的，优化过的，in-memory的一种内部表达。</p><p>Dataset上可用的操作分为 <strong>转换(transformation)</strong> 和 <strong>执行(action)</strong> 两种。</p><ul><li>Transformation操作可以产生新的Dataset，如map，filter，select和aggregate（groupBy）等。</li><li>Action操作触发计算和返回结果。 如count，show或写入文件系统等。</li></ul><h2 id="关于Dataset-API"><a href="#关于Dataset-API" class="headerlink" title="关于Dataset API"></a>关于Dataset API</h2><h3 id="Typed-and-Un-typed-APIs"><a href="#Typed-and-Un-typed-APIs" class="headerlink" title="Typed and Un-typed APIs"></a>Typed and Un-typed APIs</h3><p>实质上，在Saprk的结构化API中，可以分成两类，“无类型(untyped)”的DataFrame API和“类型化(typed)”的Dataset API。 确切的说Dataframe并不是”无类型”的, 它们有类型，只是类型检查没有那么严格，只检查这些类型是否在 ==运行时(run-time)== 与schema中指定的类型对齐。 而Dataset在 ==编译时(compile-time)== 就会检查类型是否符合规范。 </p><p>Dataset API仅适用于 ==基于JVM的语言(Scala和Java)==。我们可以使用Scala 中的case class或Java bean来进行类型指定。</p><p>关于不同语言中的可用API可参考下表。</p><table><thead><tr><th>Language</th><th>Main Abstraction</th></tr></thead><tbody><tr><td>Scala</td><td>Dataset[T] &amp; DataFrame (alias for Dataset[Row])</td></tr><tr><td>Java</td><td>Dataset[T]</td></tr><tr><td>Python*</td><td>DataFrame</td></tr><tr><td>R*</td><td>DataFrame</td></tr></tbody></table><blockquote><p>由于Python和R没有<code>compile-time type-safety</code>，因此只有 Untyped API，即DataFrames。</p></blockquote><h2 id="关于Transformations"><a href="#关于Transformations" class="headerlink" title="关于Transformations"></a>关于Transformations</h2><p>转换(transformation)可以被分为:</p><ul><li><strong>强类型转换(Typed transformations)</strong></li><li><strong>弱类型转换(Untyped transformations)</strong><h3 id="Typed-transformations-vs-Untyped-transformations"><a href="#Typed-transformations-vs-Untyped-transformations" class="headerlink" title="Typed transformations vs Untyped transformations"></a>Typed transformations vs Untyped transformations</h3>简单来说，如果转换是弱类型的，它将返回一个Dataframe(==确切的说弱类型转换的返回类型还有 <strong><em>Column</em></strong>,  <strong><em>RelationalGroupedDataset</em></strong>, <strong><em>DataFrameNaFunctions</em></strong>  和 <strong><em>DataFrameStatFunctions</em></strong>  等==)，而强类型转换返回的是一个Dataset。 在源代码中，我们可以看到弱类型转换API的返回类型是Dataframe而不是Dataset，且带有<code>@group untypedrel</code>的注释。 因此，我们可以通过检查该方法的签名来确定它是否是弱类型的(untyped)。<blockquote><p>强类型转换API带有<code>@group typedrel</code>的注释</p></blockquote></li></ul><p>例如Dataset.scala类中的<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L864-L876" target="_blank" rel="noopener">join方法</a>就属于弱类型转换(untyped transformations).</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Join with another `DataFrame`.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Behaves as an INNER JOIN and requires a subsequent join predicate.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> right Right side of the join operation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@group</span> untypedrel</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> 2.0.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">def <span class="title">join</span><span class="params">(right: Dataset[_])</span>: DataFrame </span>= withPlan &#123;</span><br><span class="line">  Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通常，任何更改Dataset列类型或添加新列的的转换是弱类型。 当我们需要修改Dataset的schema时，我们就需要退回到Dataframe进行操作。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/ch04.html" target="_blank" rel="noopener">Structured API Overview</a><a href="http://apache-spark-user-list.1001560.n3.nabble.com/Difference-between-Typed-and-untyped-transformation-in-dataset-API-td34650.html" target="_blank" rel="noopener">Difference-between-Typed-and-untyped-transformation-in-dataset-API</a><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">RDDs vs DataFrames and Datasets</a><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-dataset-operators.html" target="_blank" rel="noopener">spark-sql-dataset-operators</a><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">org.apache.spark.sql.Dataset</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> dataset </tag>
            
            <tag> transformations api </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Kafka Replication机制</title>
      <link href="/2019/03/%E5%85%B3%E4%BA%8EKafka%20Replication%E6%9C%BA%E5%88%B6.html"/>
      <url>/2019/03/%E5%85%B3%E4%BA%8EKafka%20Replication%E6%9C%BA%E5%88%B6.html</url>
      
        <content type="html"><![CDATA[<h3 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h3><ul><li><p>Kafka的replication复制机制是其可靠性的保证，即为每个分区数据提供多个副本。</p></li><li><p>每个Kafka topic包含有多个分区，分区是kafka存储数据的基本单位。==一个分区只能存储在同一个硬盘上==。</p></li><li><p>Kafka保证每一个分区内的消息的顺序，无论这个分区是在线(available)的还是离线的(unavailable)。</p></li><li><p>每个分区拥有多个副本，其中一个副本将被指定为主副本(leader replicas)，其余的为跟随副本(follower)</p></li><li><p>==所有的消息都会写入到主副本，所有的消息都从主要副本读取==，其他的副本只需要保持于主副本同步即可</p></li><li><p>当主副本离线时，其他的副本中的一个将会被推选为新的主副本（一般为该分区副本列表的下一个副本）</p></li><li><p>关于副本是否处于“同步中(in-sync)”的状态的判断标准：</p><ul><li><p>如果它是主副本，那么它是处于“同步中(in-sync)”的状态。</p></li><li><p>如果它是跟随副本， 且拥有以下状态，那么它处于“同步中(in-sync)”：</p><ul><li>它与zookeeper有一个可用的session（在最近6秒内给zk发送过心跳）</li><li>它在最近10秒内从主副本获取过消息</li><li>它在最近10秒从主副本获取过最新的消息</li></ul><p>否则，该副本的状态为“不同步(out-of-sync)”</p></li></ul></li><li><p>当一个同步中的副本出现延迟时，它会影响生产者和消费者的性能。因为只有在所有跟随副本同步完所有消息并且提交后，它们才会继续执行。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一些程序员必备的英语词汇及释义</title>
      <link href="/2019/03/%E4%B8%80%E4%BA%9B%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E7%9A%84%E8%8B%B1%E8%AF%AD%E8%AF%8D%E6%B1%87%E5%8F%8A%E9%87%8A%E4%B9%89.html"/>
      <url>/2019/03/%E4%B8%80%E4%BA%9B%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E7%9A%84%E8%8B%B1%E8%AF%AD%E8%AF%8D%E6%B1%87%E5%8F%8A%E9%87%8A%E4%B9%89.html</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>看书整理的时候遇到一些词汇不知道怎么翻译成中文好，于是整理了一些词汇解释的笔记，整理成如下词汇表</p><h4 id="词汇表"><a href="#词汇表" class="headerlink" title="词汇表"></a>词汇表</h4><div class="table-box"><table cellspacing="0"><tbody><tr><td style="vertical-align:bottom;width:120pt;"><span style="color:#000000;">angle brackets</span></td>            <td style="vertical-align:bottom;width:80pt;"><span style="color:#000000;">尖括号</span></td>            <td style="vertical-align:bottom;width:65pt;"><span style="color:#000000;">iterate over</span></td>            <td style="vertical-align:bottom;width:65pt;"><span style="color:#000000;">迭代</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">array buffers&nbsp;</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">数组缓冲</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">iterator</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">迭代器</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">asynchronous</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">异步的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">jump tables</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">跳转表</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">atom</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">原子</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">lexers</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">词法分析器</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">auxiliary constructors</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">辅助构造器</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">lexical analysis</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">词法分析</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">backslashes</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">反斜杠</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">linked list</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">链表</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">backtracking</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">回溯</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">listeners</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">监听器</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">base class</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">基类</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">markup&nbsp;</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">标记</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">blocking</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">阻塞的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">multiple inheritance</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">多重继承</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">call-by-name</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">传名调用</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">mutators</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">改值器</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">call-by-value</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">传值调用</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">namespace</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">命名空间</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">case classes</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">样例类</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">nested</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">嵌套的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">case objects</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">样例对象</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">notation</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">表示法</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">checked exception</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">受检异常</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">overflow</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">溢出</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">closure</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">闭包</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">override</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">重写/覆盖</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">companion classes</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">伴生类</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">parser trees</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">解析树</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">companion objects</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">伴生对象</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">partial functions</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">偏函数</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">compile-time</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">编译期的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">partially applied functions</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">部分应用函数</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">comprehensions</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">推导式</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">pass-by-name</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">传名的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">concurrency</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">并发</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">pattern matching</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">模式匹配</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">consistency</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">一致性</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">polymorphism</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">多态</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">contravariant</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">逆变的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">primary constructors</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">主构造器</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">control flow</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">控制流转</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">procedures</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">过程</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">covariant</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">协变的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">processes</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">进程</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">currying</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">柯里化</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">projection</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">投影</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">deadlocks</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">死锁</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">properties</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">属性</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">delimited continuation</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">定界延续</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">ragged</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">不规则的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">dependency injection</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">依赖注入</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">random access</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">随机访问</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">deprecated</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">已过时的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">recursive</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">递归的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">destructors</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">析构器</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">reference types</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">引用类型</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">discard</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">丢弃</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">reflective calls</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">反射调用</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">domain-specific languages</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">领域特定语言</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">regex</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">正则</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">duck typing</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">鸭子类型</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">regular expression</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">正则表达式</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">dynamically typed languages</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">动态类型语言</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">runtime</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">运行时的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">elitable</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">可省略的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">sealed class</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">密封类</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">embedded</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">内嵌的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">selectors</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">选取器</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">enumerations</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">枚举</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">self types</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">自身类型</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">evaluate</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">求值</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">self-closing tags</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">自结束的标签</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">exhaustive</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">穷举的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">serialization</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">序列化</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">explicit</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">显式的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">singleton</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">单例</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">extractors</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">提取器</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">square brackets</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">方括号</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">family polymorphism</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">家族多态</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">stack</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">栈</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">for comprehension</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">for 推导式</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">structural types</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">结构类型</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">form</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">范式</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">subclass</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">子类</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">generators</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">生成器</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">superclasses</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">超类</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">generics</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">泛型</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">supertypes</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">超类型</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">hash maps</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">哈希映射</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">synchronous</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">同步的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">hash tables</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">哈希表</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">syntactic sugar</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">语法糖</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">heterogenerous</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">异构的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">tab completion</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">Tab键补全</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">higher-kinded types</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">高等类型</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">tail recursive</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">尾递归</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">higher-order functions</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">高阶函数</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">threads</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">线程</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">immutable</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">不可变的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">transient</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">瞬态的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">implement</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">实现</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">tuples</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">元祖</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">implicit</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">隐式的</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">typesafe</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">类型安全的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">implicit conversion</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">隐式转换</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">unary</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">一元的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">implicit parameters</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">隐式参数</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">unevaluated</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">为求值得</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">inference</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">推断</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">volatile</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">易失的</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">instance</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">实例</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">wildcards</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">通配符</span></td>        </tr><tr><td style="vertical-align:bottom;"><span style="color:#000000;">invoke</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">执行/调用</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">warppers</span></td>            <td style="vertical-align:bottom;"><span style="color:#000000;">包装</span></td>        </tr></tbody></table></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ETL工具Talend最佳实践</title>
      <link href="/2019/03/ETL%E5%B7%A5%E5%85%B7Talend%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html"/>
      <url>/2019/03/ETL%E5%B7%A5%E5%85%B7Talend%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>和Talend这款软件打交道有一段时间了，主要用它来做一些ETL相关的作业开发，以下总结了一些自己配置与开发过程中的最佳实践。</p><h4 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h4><ol><li>可以通过修改Talend Studio 的 <strong>.ini</strong> 配置文件来给其分配更多的内存，例如，以下是我在64位8GB内存的电脑配置的参数<table><tr><td align="left"></td></tr></table></li></ol><p>-vmargs <br>-Xms2014m <br>-Xmx4096m <br>-XX:MaxPermSize=512m <br>-Dfile.encoding=UTF-8 <br></p><ol start="2"><li>在开发过程中一定要注意对Null值得处理</li><li>可以创建Repository Metadata用于数据库连接</li><li>可以使用 ==t&lt;DB&gt;== 的数据连接组件定义数据库连接，并重复使用。</li><li>记得使用 ==t&lt;DB&gt;== 组件来关闭数据库连接</li><li>避免在Talend的组件中在使用硬编码值(hard coding)，使用Talend context 变量代替</li><li>尽可能使用变量代替硬编码</li><li>对于频繁的变换，可以通过创建routines或者functions来减少工作量</li><li>每次关机前记得保存并关闭Talend Studio！！！</li><li>尽可能早的使用tFilterColumns组件过滤去不需要的字段/列</li><li>尽可能早的使用tFilterRows组件过滤去不需要的数据</li><li>使用Select列表达式从数据库获取数据，尽量避免获取不需要的字段</li><li>当作业出现OOM错误时，调整JVM的参数，例如修改Xms和Xmx来分配更多的内存</li><li>通过使用并行化选项来提高作业性能，减少整体的运行时间，如并行化从数据读写数据等</li><li>给Main job起一个有意义的名字</li><li>在定义Sub job时，务必第一时间记录子作业的标题、描述和目的。</li><li>在设计作业尽可能将复杂的作业切割成一个个小作业</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> ETL </tag>
            
            <tag> Talend </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中RDD、DataFrame和DataSet的区别</title>
      <link href="/2019/03/Spark%E4%B8%ADRDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
      <url>/2019/03/Spark%E4%B8%ADRDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>最近同事开始学习使用Spark，问我RDD、DataFrame和DataSet之间有什么区别，以及生产环境中的spark1.6将在不久后被移除，全部使用spark2+。于是今天我就借机整理了以下它们三者之间的异同。</p><h4 id="RDD、DataFrame和DataSet的定义"><a href="#RDD、DataFrame和DataSet的定义" class="headerlink" title="RDD、DataFrame和DataSet的定义"></a>RDD、DataFrame和DataSet的定义</h4><p>在开始Spark RDD与DataFrame与Dataset之间的比较之前，先让我们看一下Spark中的RDD，DataFrame和Datasets的定义：</p><ul><li><strong>Spark RDD</strong> RDD代表弹性分布式数据集。它是记录的只读分区集合。 RDD是Spark的基本数据结构。它允许程序员以容错方式在大型集群上执行内存计算。</li><li><strong>Spark Dataframe</strong> 与RDD不同，数据组以列的形式组织起来，类似于关系数据库中的表。它是一个不可变的分布式数据集合。 Spark中的DataFrame允许开发人员将数据结构(类型)加到分布式数据集合上，从而实现更高级别的抽象。<ul><li><strong>Spark Dataset</strong> Apache Spark中的Dataset是DataFrame API的扩展，它提供了类型安全(type-safe)，面向对象(object-oriented)的编程接口。 Dataset利用Catalyst optimizer可以让用户通过类似于sql的表达式对数据进行查询。</li></ul></li></ul><h4 id="RDD、DataFrame和DataSet的比较"><a href="#RDD、DataFrame和DataSet的比较" class="headerlink" title="RDD、DataFrame和DataSet的比较"></a>RDD、DataFrame和DataSet的比较</h4><h5 id="Spark版本"><a href="#Spark版本" class="headerlink" title="Spark版本"></a>Spark版本</h5><ul><li>RDD – 自Spark 1.0起</li><li>DataFrames – 自Spark 1.3起</li><li>DataSet – 自Spark 1.6起</li></ul><h5 id="数据表示形式"><a href="#数据表示形式" class="headerlink" title="数据表示形式"></a>数据表示形式</h5><ul><li>RDD RDD是分布在集群中许多机器上的数据元素的分布式集合。 RDD是一组表示数据的Java或Scala对象。</li><li>DataFrameDataFrame是命名列构成的分布式数据集合。 它在概念上类似于关系数据库中的表。</li><li>Dataset它是DataFrame API的扩展，提供RDD API的类型安全，面向对象的编程接口以及Catalyst查询优化器的性能优势和DataFrame API的堆外存储机制的功能。</li></ul><h5 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h5><ul><li>RDD 它可以轻松有效地处理结构化和非结构化的数据。 和Dataframe和DataSet一样，RDD不会推断出所获取的数据的结构类型，需要用户来指定它。</li><li>DataFrame 仅适用于结构化和半结构化数据。 它的数据以命名列的形式组织起来。 </li><li>DataSet 它也可以有效地处理结构化和非结构化数据。 它表示行(row)的JVM对象或行对象集合形式的数据。 它通过编码器以表格形式(tabular forms)表示。</li></ul><h5 id="编译时类型安全"><a href="#编译时类型安全" class="headerlink" title="编译时类型安全"></a>编译时类型安全</h5><ul><li><p>RDDRDD提供了一种熟悉的面向对象编程风格，具有编译时类型安全性。</p></li><li><p>DataFrame如果您尝试访问表中不存在的列，则持编译错误。 它仅在运行时检测属性错误。</p></li><li><p>DataSet DataSet可以在编译时检查类型, 它提供编译时类型安全性。[TO-DO 什么是编译时的类型安全]</p><h5 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h5></li><li><p>RDD每当Spark需要在集群内分发数据或将数据写入磁盘时，它就会使用Java序列化。序列化单个Java和Scala对象的开销很昂贵，并且需要在节点之间发送数据和结构。</p></li><li><p>DataFrameSpark DataFrame可以将数据序列化为二进制格式的堆外存储（在内存中），然后直接在此堆内存上执行许多转换。无需使用java序列化来编码数据。它提供了一个Tungsten物理执行后端，来管理内存并动态生成字节码以进行表达式评估。</p></li><li><p>DataSet 在序列化数据时，Spark中的数据集API具有编码器的概念，该编码器处理JVM对象与表格表示之间的转换。它使用spark内部Tungsten二进制格式存储表格表示。数据集允许对序列化数据执行操作并改善内存使用。它允许按需访问单个属性，而不会消灭整个对象。</p></li></ul><h5 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h5><ul><li>RDD创建和销毁单个对象会导致垃圾回收。<ul><li>DataFrame避免在为数据集中的每一行构造单个对象时引起的垃圾回收。</li></ul></li><li>DataSet因为序列化是通过Tungsten进行的，它使用了off heap数据序列化，不需要垃圾回收器来摧毁对象</li></ul><h5 id="效率-内存使用"><a href="#效率-内存使用" class="headerlink" title="效率/内存使用"></a>效率/内存使用</h5><ul><li>RDD在java和scala对象上单独执行序列化时，效率会降低，这需要花费大量时间。</li><li>DataFrame使用off heap内存进行序列化可以减少开销。 它动态生成字节代码，以便可以对该序列化数据执行许多操作。 无需对小型操作进行反序列化。</li><li>DataSet它允许对序列化数据执行操作并改善内存使用。 因此，它可以允许按需访问单个属性，而无需反序列化整个对象。</li></ul><h5 id="编程语言支持"><a href="#编程语言支持" class="headerlink" title="编程语言支持"></a>编程语言支持</h5><ul><li>RDDRDD提供Java，Scala，Python和R语言的API。 因此，此功能为开发人员提供了灵活性。</li><li>DataFrameDataFrame同样也提供Java，Scala，Python和R语言的API</li><li>DataSet Dataset 的一些API目前仅支持Scala和Java，对Python和R语言的API在陆续开发中</li></ul><h5 id="聚合操作-Aggregation"><a href="#聚合操作-Aggregation" class="headerlink" title="聚合操作(Aggregation)"></a>聚合操作(Aggregation)</h5><ul><li>RDDRDD API执行简单的分组和聚合操作的速度较慢。</li><li>DataFrameDataFrame API非常易于使用。 探索性分析更快，在大型数据集上创建汇总统计数据。</li><li>DataSet在Dataset中，对大量数据集执行聚合操作的速度更快。</li></ul><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ul><li>当我们需要对数据集进行底层的转换和操作时， 可以选择使用RDD</li><li>当我们需要高级抽象时，可以使用DataFrame和Dataset API。</li><li>对于非结构化数据，例如媒体流或文本流，同样可以使用DataFrame和Dataset API。</li><li>我们可以使用DataFrame和Dataset 中的高级的方法。 例如，filter, maps, aggregation, sum, SQL queries以及通过列访问数据等如果您不关心在按名称或列处理或访问数据属性时强加架构（例如列式格式）。另外，如果我们想要在编译时更高程度的类型安全性。</li></ul><p>RDD提供更底层功能， DataFrame和Dataset则允许创建一些自定义的结构，拥有高级的特定操作，节省空间并高速执行。 </p><p>为了确保我们的代码能够尽可能的利用Tungsten优化带来的好处，推荐使用Scala的 Dataset API（而不是RDD API）。</p><p>Dataset即拥有DataFrame带来的relational transformation的便捷，也拥有RDD中的functional transformation的优势。 </p><p>参考资料<a href="https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/" target="_blank" rel="noopener">apache-spark-rdd-vs-dataframe-vs-dataset</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何为Spark集群配置--num-executors， -  executor-memory和--execuor-cores</title>
      <link href="/2019/03/Spark%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D.html"/>
      <url>/2019/03/Spark%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D.html</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在我们提交spark程序时，应该如何为Spark集群配置--num-executors， -  executor-memory和--execuor-cores 呢？</p><h4 id="一些资源参数设置的基本知识"><a href="#一些资源参数设置的基本知识" class="headerlink" title="一些资源参数设置的基本知识"></a>一些资源参数设置的基本知识</h4><ul><li><strong>Hadoop / Yarn / OS Deamons</strong> 当我们使用像Yarn这样的集群管理器运行spark应用程序时，会有几个守护进程在后台运行，如NameNode，Secondary NameNode，DataNode，JobTracker和TaskTracker等。因此，在指定num-executors时，我们需要确保为这些守护进程留下足够的核心（至少每个节点约1 CPU核）以便顺利运行。</li><li><strong>Yarn ApplicationMaster（AM）</strong>ApplicationMaster负责协调来自ResourceManager的资源，并与NodeManagers一起执行container并监控其资源消耗。如果我们在YARN上运行Spark，那么我们需要预估运行AM所需要的资源（至少1024MB和1 CPU核）。</li><li><strong>HDFS吞吐量</strong>HDFS客户端遇到大量并发线程会出现一些bug。一般来说，每个executors最多可以实现5个任务的完全写入吞吐量，因此最好将每个executors的核心数保持在该数量之下。</li><li><strong>MemoryOverhead</strong>JVM还需要一些off heap的内存，请参考下图中描绘的Spark和YARN中内存属性的层次结构， <table><tr><td><img src="/2019/03/Spark应用程序的资源分配/./20190301133032273.png" width="60%" alt="test"></td></tr><tr><td colspan="1"><div>Credit: <a href="https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/l" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/</a></div></td></tr></table></li></ul><p>简单来说，有以下两个公式：</p> <table align="left"><tr><td align="left">每个executor从YARN请求的内存   =  <b>spark-executor-memory + spark.yarn.executor.memoryOverhead </b></td></tr></table> <table align="left"><tr><td align="left"> spark.yarn.executor.memoryOverhead = <b>Max(384MB, 7% of spark.executor-memory)</b></td></tr></table><p>例如当我设置  --executor-memory=20时， 我们实际请求了</p><pre><code>20GB + memoryOverhead = 20 + 7% of 20GB = ~23GB。</code></pre><blockquote><p>运行具有executors大内存的通常会导致<font color="black"><b>过多的GC延迟。</b></font><br>运行较小的executors（例如，1G &amp; 1 CPU core）则会浪费 ==单个JVM中运行多个任务==所带来的优点。</p></blockquote><h4 id="不同配置的优劣分析"><a href="#不同配置的优劣分析" class="headerlink" title="不同配置的优劣分析"></a>不同配置的优劣分析</h4><p>现在，假设我们的集群配置如下 <table align="left"><tr><td align="left"> 10 Nodes <br>16 cores per Node<br>64GB RAM per Node</td></tr></table></p><h5 id="第一种方法：使用较小的executors"><a href="#第一种方法：使用较小的executors" class="headerlink" title="第一种方法：使用较小的executors"></a>第一种方法：使用较小的executors</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">`--num-executors` = `在这种方法中，我们将为每个核心分配一个executor`</span><br><span class="line">                  = `集群的总核心数`</span><br><span class="line">                  = `每个节点的核心数 * 集群的总节点数` </span><br><span class="line">                  =  16 x 10 = 160</span><br><span class="line">                  </span><br><span class="line">`--executor-cores`  = 1 (`每个executor分配的核心数目`)</span><br><span class="line"></span><br><span class="line">`--executor-memory` = `每个executor分配的内存数`</span><br><span class="line">                    = `每个节点内存总数数/每个节点上分配的executor数`</span><br><span class="line">                    = 64GB/16 = 4GB</span><br></pre></td></tr></table></figure><p><strong>分析</strong>：由于每个executor只分配了一个核，我们将无法利用在同一个JVM中运行多个任务的优点。 此外，共享/缓存变量（如广播变量和累加器）将在节点的每个核心中复制16次。 最严重的就是，我们没有为Hadoop / Yarn守护程序进程留下足够的内存开销，我们还忘记了将ApplicationManagers运行所需要的开销加入计算。 </p><h5 id="第二种方法：使用较大的executors"><a href="#第二种方法：使用较大的executors" class="headerlink" title="第二种方法：使用较大的executors"></a>第二种方法：使用较大的executors</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">`--num-executors` = `在这种方法中，我们将为每个节点分配一个executor`</span><br><span class="line">                  = `集群的总节点数`</span><br><span class="line">                  = 10</span><br><span class="line">                    </span><br><span class="line">`--executor-cores` = `每个节点一个executor意味着该节点的所有核心都分配给一个执executor`</span><br><span class="line">                   = `每个节点的总核心数`</span><br><span class="line">                   = 16</span><br><span class="line">                     </span><br><span class="line">`--executor-memory` = `每个executor分配的内存数`</span><br><span class="line">                    = `每个节点内存总数数/每个节点上分配的executor数`</span><br><span class="line">                    = 64GB/1 = 64GB</span><br></pre></td></tr></table></figure><p><strong>分析：</strong>每个executor都有16个核心，由于HDFS客户端遇到大量并发线程会出现一些bug，即HDFS吞吐量会受到影响。同时过大的内存分配也会导致过多的GC延迟。 </p><h5 id="第三种方法：使用优化的executors"><a href="#第三种方法：使用优化的executors" class="headerlink" title="第三种方法：使用优化的executors"></a>第三种方法：使用优化的executors</h5><ul><li>基于上面提到的建议，让我们为每个执行器分配5个核心, 即<code>--executor-cores = 5</code></li><li>为每个节点留出1个核心用于Hadoop / Yarn守护进程, 即每个节点可用的核心数 = 16-1 = 15。 因此，群集中核心的可用总数= 15 x 10 = 150</li><li>--num-executors =（群集中核心的可用总数/每个executors分配的核心数）= 150/5 = 30</li><li>为ApplicationManager留下预留1个executors的资源，  即<code>--num-executors = 29</code></li><li>每个节点的executors数目 = 30/10 = 3</li><li>群集中每个节点的可使用的总内存数  64GB  - 1GB = 63GB</li><li>每个executor的内存= 64GB / 3 = 21GB</li><li>预留的 off heap overhead = 21GB * 7％ ~  1.47G </li><li>所以，实际的<code>--executor-memory = 21  -  1.47G ~ 19GB</code></li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/" target="_blank" rel="noopener">how-to-tune-your-apache-spark-jobs-part-2</a><a href="https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html" target="_blank" rel="noopener">distribution_of_executors_cores_and_memory_for_spark_application</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一些常用的Spark SQL调优技巧</title>
      <link href="/2019/03/%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84Spark%20SQL%E8%B0%83%E4%BC%98%E6%8A%80%E5%B7%A7.html"/>
      <url>/2019/03/%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%9A%84Spark%20SQL%E8%B0%83%E4%BC%98%E6%8A%80%E5%B7%A7.html</url>
      
        <content type="html"><![CDATA[<h5 id="一些常用的Spark-SQL调优技巧"><a href="#一些常用的Spark-SQL调优技巧" class="headerlink" title="一些常用的Spark SQL调优技巧"></a>一些常用的Spark SQL调优技巧</h5><ol><li><p>使用缓存表在sparksql中，当我们创建表时，我们可以通过调用<code>spark.catalog.cacheTable(&quot;tableName&quot;)</code> 或者 <code>dataFrame.cache()</code>的方式将表缓存起来。这样Spark SQL将仅扫描所需的列，并自动调整压缩以最小化内存使用和GC压力。当你不需要缓存时，可以通过使用<code>spark.catalog.uncacheTable(&quot;tableName&quot;)</code>将其移除缓存。</p><p> 此外，我们还可以通过设置<code>spark.sql.inMemoryColumnarStorage.batchSize</code>来调整列缓存batch的大，例如在提交spark作业时候，指定以下参数：</p><pre><code>--conf &quot;spark.sql.inMemoryColumnarStorage.batchSize=10000&quot;</code></pre><p> 较大的batch size可以提高内存利用率和压缩率，但在缓存数据时<font color="red"><b>存在OOM风险。</b></font></p></li><li><p>调整Shuffle分区我可以通过调整在对数据进行shuffle或者aggregation操作时的分区数目来提高性能。分区数和reduce任务数是相同的。如果reducer具有资源密集型操作，那么增加shuffle分区将增加并行性，同时还可以更好地利用资源并最小化每个任务的负载。我们可以通过设置<code>spark.sql.shuffle.partitions</code>来调整spark sql作业中的shuffle分区数(默认值为200).例如在提交spark作业时候，指定以下参数：</p><pre><code>--conf &quot;spark.sql.shuffle.partitions=2000&quot;</code></pre></li><li><p>使用Broadcast JoinBroadcast Join接类似于Hive中的Map Join，其中较小的表将被加载到分布式缓存中，并且连接操作可以作为Map Only操作来完成。 默认情况下，Spark SQL中会启用广播连接。当然，我们也可以指定以下参数将其关闭：</p><pre><code>--conf“spark.sql.autoBroadcastJoinThreshold = -1”</code></pre><p> 这个参数是表示Broadcast Join时广播表大小的阈值，<code>-1</code>即可以理解为关闭Broadcast Join。当然当我们开启Broadcast Join时，也可以修改其数值来增加在执行连接操作时可以广播的表大小的最大值。 默认值为10 MB，例如我们可以通过以下设置将其大小更改为50MB：</p><pre><code>--conf &quot;spark.sql.autoBroadcastJoinThreshold = 50485760&quot;</code></pre></li></ol><p>参考资料：<a href="https://mapr.com/support/s/article/Spark-Troubleshooting-guide-Spark-SQL-Examples-of-commonly-used-Spark-SQLTuning-properties?language=en_US" target="_blank" rel="noopener">Spark-Troubleshooting-guide-Spark-SQL-Examples-of-commonly-used-Spark-SQLTuning-properties</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用sendmail发送邮件及附件</title>
      <link href="/2019/02/%E4%BD%BF%E7%94%A8sendmail%E5%91%BD%E4%BB%A4%E5%8F%91%E9%80%81%E9%99%84%E4%BB%B6.html"/>
      <url>/2019/02/%E4%BD%BF%E7%94%A8sendmail%E5%91%BD%E4%BB%A4%E5%8F%91%E9%80%81%E9%99%84%E4%BB%B6.html</url>
      
        <content type="html"><![CDATA[<p>在自动化中经常需要将日志文件发送到指定用户组，于是记录一下使用sendmail发送邮件及附件的shell脚本模板</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">MAILFROM="noreply@`hostname -f`"</span><br><span class="line">MAILTO="mail.to@hotmail.com"</span><br><span class="line">SUBJECT="Sendmail templete test"</span><br><span class="line">ATTACHMENT="XXXXXXX20190227.log"</span><br><span class="line">MAILPART=`uuidgen` ## Generates Unique ID as boundary</span><br><span class="line">MAILPART_BODY=`uuidgen` ## Generates Unique ID as boundary</span><br><span class="line"></span><br><span class="line">(</span><br><span class="line"> echo "From: $MAILFROM"</span><br><span class="line"> echo "To: $MAILTO"</span><br><span class="line"> echo "Subject: $SUBJECT"</span><br><span class="line"> echo "MIME-Version: 1.0"</span><br><span class="line"> echo "Content-Type: multipart/mixed; boundary=\"$MAILPART\""</span><br><span class="line"> echo ""</span><br><span class="line"> echo "--$MAILPART"</span><br><span class="line"> echo "Content-Type: multipart/alternative; boundary=\"$MAILPART_BODY\""</span><br><span class="line"> echo ""</span><br><span class="line"> echo "--$MAILPART_BODY"</span><br><span class="line"> echo "Content-Type: text/plain; charset=UTF-8"</span><br><span class="line"> echo "This is TEXT part and below is HTML part"</span><br><span class="line"> echo "--$MAILPART_BODY"</span><br><span class="line"> echo "Content-Type: text/html; charset=UTF-8"</span><br><span class="line"> echo ""</span><br><span class="line"> echo "&lt;html&gt;&lt;body&gt;&lt;div&gt;THIS IS HTML PART&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;"</span><br><span class="line"> echo "--$MAILPART_BODY--"</span><br><span class="line"></span><br><span class="line"> echo "--$MAILPART"</span><br><span class="line"> echo 'Content-Type: text/plain; name="'$(basename $ATTACHMENT)'"'</span><br><span class="line"> echo "Content-Transfer-Encoding: base64"</span><br><span class="line"> echo ""</span><br><span class="line"> openssl base64 &lt; $ATTACHMENT;</span><br><span class="line"> echo "--$MAILPART--"</span><br><span class="line">)  | sendmail -t</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
            <tag> sendmail </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark.sql.shuffle.partitions 和 spark.default.parallelism 的区别</title>
      <link href="/2019/02/spark.sql.shuffle.partitions%20%E5%92%8C%20spark.default.parallelism%20%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
      <url>/2019/02/spark.sql.shuffle.partitions%20%E5%92%8C%20spark.default.parallelism%20%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
      
        <content type="html"><![CDATA[<p>在关于spark任务并行度的设置中，有两个参数我们会经常遇到，spark.sql.shuffle.partitions 和 spark.default.parallelism, 那么这两个参数到底有什么区别的？</p><p>首先，让我们来看下它们的定义</p><table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody><tr><td>spark.sql.shuffle.partitions</td><td>200</td><td align="left" width="200">Configures the number of partitions to use when shuffling data for <b>joins</b> or <b>aggregations</b>.</td></tr><tr><td>spark.default.parallelism</td><td align="left">For distributed shuffle operations like <strong>reduceByKey</strong> and <strong>join</strong>, the largest number of partitions in a parent RDD. <br><br>For operations like parallelize with no parent RDDs, it depends on the cluster manager: <br> <b>- Local mode:</b> number of cores on the local machine <br> <b>- Mesos fine grained mode</b>: 8 <br><b>- Others:</b> total number of cores on all executor nodes or 2, whichever is larger</td><td align="left">Default number of partitions in RDDs returned by transformations like <b>join</b>, <b>reduceByKey</b>, and parallelize when not set by user.</td></tr></tbody></table><p>看起来它们的定义似乎也很相似，但在实际测试中，</p><ul><li>spark.default.parallelism只有在处理RDD时才会起作用，对Spark SQL的无效。</li><li>spark.sql.shuffle.partitions则是对sparks SQL专用的设置</li></ul><p>我们可以在提交作业的通过 <code>--conf</code> 来修改这两个设置的值，方法如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --conf spark.sql.shuffle.partitions=20 --conf spark.default.parallelism=20</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Parallelism </tag>
            
            <tag> Tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>将时间戳(timestamp)转换为MongoDB中的ObjectId</title>
      <link href="/2019/02/%E5%B0%86%E6%97%B6%E9%97%B4%E6%88%B3(timestamp)%E8%BD%AC%E6%8D%A2%E4%B8%BAMongoDB%E4%B8%AD%E7%9A%84ObjectId.html"/>
      <url>/2019/02/%E5%B0%86%E6%97%B6%E9%97%B4%E6%88%B3(timestamp)%E8%BD%AC%E6%8D%A2%E4%B8%BAMongoDB%E4%B8%AD%E7%9A%84ObjectId.html</url>
      
        <content type="html"><![CDATA[<h5 id="什么是ObjectId"><a href="#什么是ObjectId" class="headerlink" title="什么是ObjectId"></a>什么是ObjectId</h5><p>ObjectId是MongoDB文档的默认主键，通常位于插入文档的_id字段中。例如：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"_id"</span>: ObjectId(<span class="string">"507f1f77bcf86cd799439011"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ObjectId是一个12字节的二进制BSON类型字符串， 由以下几部分构成：</p><ul><li>1-4字节：UNIX时间戳</li><li>5-7字节：表示运行MongoDB的机器</li><li>8-9字节：表示生成此_id的进程</li><li>10-12字节：由一个以随机数为起始的计数器生成的值</li></ul><h5 id="ObjectId的构造方法"><a href="#ObjectId的构造方法" class="headerlink" title="ObjectId的构造方法"></a>ObjectId的构造方法</h5><p>构造器ObjectId()接受的参数可以是：</p><ul><li><p>不提供参数</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ObjectId = <span class="built_in">require</span>(<span class="string">'mongodb'</span>).ObjectID</span><br><span class="line"><span class="keyword">var</span> id = <span class="keyword">new</span> ObjectId();</span><br></pre></td></tr></table></figure></li><li><p>12字节的字符串</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ObjectId = <span class="built_in">require</span>(<span class="string">'mongodb'</span>).ObjectID</span><br><span class="line"><span class="keyword">var</span> id = <span class="keyword">new</span> ObjectId(<span class="string">"aaaabbbbcccc"</span>);</span><br></pre></td></tr></table></figure></li><li><p>24字节的16进制字符表示</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ObjectId = <span class="built_in">require</span>(<span class="string">'mongodb'</span>).ObjectID</span><br><span class="line"><span class="keyword">var</span> id = <span class="keyword">new</span> ObjectId(<span class="string">"507f1f77bcf86cd799439011"</span>);</span><br></pre></td></tr></table></figure></li></ul><h5 id="ObjectId实例方法"><a href="#ObjectId实例方法" class="headerlink" title="ObjectId实例方法"></a>ObjectId实例方法</h5><p>ObjectId常用的实例方法有:</p><ul><li><p>getTimestamp()</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ObjectId = <span class="built_in">require</span>(<span class="string">'mongodb'</span>).ObjectID</span><br><span class="line"><span class="keyword">var</span> id = <span class="keyword">new</span> ObjectId();</span><br><span class="line"><span class="built_in">console</span>.log(id.getTimestamp())</span><br></pre></td></tr></table></figure></li><li><p>toHexString()</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ObjectId = <span class="built_in">require</span>(<span class="string">'mongodb'</span>).ObjectID</span><br><span class="line"><span class="keyword">var</span> id = <span class="keyword">new</span> ObjectId(<span class="string">"aaaabbbbcccc"</span>);</span><br><span class="line"><span class="built_in">console</span>.log(id.toHexString())</span><br></pre></td></tr></table></figure></li></ul><h5 id="ObjectId与Timestamp的转换"><a href="#ObjectId与Timestamp的转换" class="headerlink" title="ObjectId与Timestamp的转换"></a>ObjectId与Timestamp的转换</h5><p>以下介绍几种语言中将日期转换为ObjectId的方法</p><h6 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curr_dt='2019-02-25'</span><br><span class="line">curr_ts='$(printf '%x' $(date -d "$curr_dt" +%s))'</span><br><span class="line">curr_oid=$(printf '%-24s' $&#123;curr_ts&#125; |sed 's/ /0/g')</span><br></pre></td></tr></table></figure><h6 id="python"><a href="#python" class="headerlink" title="python"></a>python</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dateutil.parser <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">from</span> bson <span class="keyword">import</span> ObjectId</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">curr_dt = parse(<span class="string">'2019-02-25'</span>)</span><br><span class="line">curr_oid = ObjectId(hex(int(time.mktime(curr_dt.timetuple())))[<span class="number">2</span>:] + <span class="string">'0'</span>*<span class="number">16</span>)</span><br></pre></td></tr></table></figure><h6 id="javascript"><a href="#javascript" class="headerlink" title="javascript"></a>javascript</h6><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curr_dt=<span class="keyword">new</span> <span class="built_in">Date</span>()</span><br><span class="line">curr_oid = <span class="built_in">Math</span>.floor(curr_dt.getTime() / <span class="number">1000</span>).toString(<span class="number">16</span>) + <span class="string">"0000000000000000"</span>;</span><br></pre></td></tr></table></figure><p>除此之前，也可以访问<a href="https://steveridout.github.io/mongo-object-time/" target="_blank" rel="noopener">在线转换器</a></p>]]></content>
      
      
      <categories>
          
          <category> mongodb </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mongodb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CA Autosys — condition依赖条件</title>
      <link href="/2019/02/CA%20Autosys(2)%20%E2%80%94%20condition%E4%BE%9D%E8%B5%96%E6%9D%A1%E4%BB%B6.html"/>
      <url>/2019/02/CA%20Autosys(2)%20%E2%80%94%20condition%E4%BE%9D%E8%B5%96%E6%9D%A1%E4%BB%B6.html</url>
      
        <content type="html"><![CDATA[<h3 id="Dependent-Jobs-具有依赖的作业"><a href="#Dependent-Jobs-具有依赖的作业" class="headerlink" title="Dependent Jobs 具有依赖的作业"></a>Dependent Jobs 具有依赖的作业</h3><p>在Autosys中，作业(Job)可以取决于其他作业的完成结果。 具有依赖的作业和简单作业之间的唯一区别是它依赖于另一个或多个作业。 </p><p>我们可以通过<strong>condition</strong>属性来指定作业依赖关系。我们还可以在作业的依赖关系中添加时间限制，即<strong>回看条件</strong>(look-back condition)。</p><p>更多关于condition属性的介绍，请移步<a href="https://support.ca.com/cadocs/0/CA%20Workload%20Automation%20AE%20Release%2011%203%206%20-%20Public%20Access-ENU/Bookshelf_Files/PDF/WA_AE_User_ENU.pdf" target="_blank" rel="noopener">官方文档</a></p><p>现在让我们来创建一个具有依赖关系的作业JIL定义如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ *** EOD_post *** /</span><br><span class="line">insert_job: EOD_post</span><br><span class="line">job_type: cmd</span><br><span class="line">machine: prod</span><br><span class="line">condition: s(EOD_watch)</span><br><span class="line">command: $HOME/POST</span><br></pre></td></tr></table></figure><p>以上我们定了一个名为EOD_POST的cmd作业，它的依赖关系为 <em>s(EOD_watch)</em>, 即依赖于EOD_watch作业的成功。换句话说，只有EOD_watch这个作业的完成状态为success，EOD_POST才会被启动。</p><h4 id="look-back-condition-回看条件"><a href="#look-back-condition-回看条件" class="headerlink" title="look-back condition 回看条件"></a>look-back condition 回看条件</h4><p>我们还可以在作业的依赖关系中添加时间限制，即<strong>回看条件</strong>(look-back condition)。</p><p>在这里我们举个例子来加深对这个概念的理解。例如我们有一个作业test_sample_04，其定义如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">insert_job: test_sample_04</span><br><span class="line">job_type: cmd</span><br><span class="line">machine: localhost</span><br><span class="line">command: sleep 10</span><br><span class="line">condition: s(test_sample_01,12.00) AND f(test_sample_02,24.00) AND</span><br><span class="line">s(test_sample_03)</span><br></pre></td></tr></table></figure><p>只有condition中所有条件时满足时，命令作业test_sample_04才会被启动，即满足：</p><ul><li>最近一次运行的test_sample_01的完成状态为success, 且完成时间在过去12小时内。</li><li>最近一次运行的test_sample_02的完成状态为failure, 且完成时间在过去24小时内。小时。</li><li>最近一次运行test_sample_03为成功即可（对其结束时间没有限制，可以是任意时间）</li></ul><p>回看条件的时间依赖的表示方法为 HH.MM （HH为小时数，MM为分钟数,  如上面例子中的<code>s(test_sample_01,12.00)</code></p><p>look-back condition还有有一种特殊用法,<code>condition : s(dep_job, 0)</code>, 让我们再举一个例子来加深理。我们先定义一个cmd作业test_sample_05：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">insert_job: test_sample_05</span><br><span class="line">job_type: cmd</span><br><span class="line">machine: localhost</span><br><span class="line">command: sleep 10</span><br><span class="line">condition: s(test_sample_04)</span><br></pre></td></tr></table></figure><p>它只要检测到test_sample_04完成并具有success状态就会启动。</p><p>然后我们更新它的condition依赖条件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">update_job: test_sample_05</span><br><span class="line">condition: s(test_sample_04, 0)</span><br></pre></td></tr></table></figure><p>这里我们在condition中增加了回看条件依赖<code>0</code>，此时，autosys会先检测test_sample_05上一次的完成时间，比如昨天11点, 然后它会去检测test_sample_05所依赖的test_sample_04的完成时间，只有test_sample_04最后一次的完成时间在test_sample_05上一次的完成之间之后，新的test_sample_05才会被启动。例如，上一次test_sample_05完成时间是11点，而最近一次test_sample_04的完成时间是昨天10点，则其条件不满足。再比如，最近一次test_sample_04的完成时间是昨天11点05分，晚于上一次test_sample_05完成时间(11点)，则其条件满足，test_sample_05会被启动。可以参考下表：</p><table><thead><tr><th>last run of test_sample_05(Dependent)</th><th>last run of test_sample_04 (Dependency)</th><th>condition</th></tr></thead><tbody><tr><td>11:00 AM (yestoday)</td><td>10:55 AM (yestoday)</td><td>Not satisfied ❌</td></tr><tr><td>11:00 AM (yestoday)</td><td>11:05 AM (yestoday)</td><td>Satisfied ✅</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> autosys </category>
          
      </categories>
      
      
        <tags>
            
            <tag> autosys </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么建议在Spark中使用Scala定义UDF</title>
      <link href="/2019/02/why-use-scala-udf.html"/>
      <url>/2019/02/why-use-scala-udf.html</url>
      
        <content type="html"><![CDATA[<p>虽然在Pyspark中，驱动程序是一个python进程，但是它创建的SparkSession对象以及其他DataFrames或者RDDs等都是利用Python封装过的 ==JVM对象== 。简单地说，虽然控制程序是Python，但它实际上是python代码告诉集群上的分布式Scala程序该做什么。 数据存储在JVM的内存中，并由Scala代码进行转换。</p><p>将这些对象从JVM内存中取出并将它们转换为Python可以读取的形式（称为序列化和反序列化）的过程开销是很大的。 一般情况下，将计算结果收集回Python驱动程序通常针对低容量样本，并且不经常进行，因此这种开销相对不被注意。 但是，如果程序在集群中的对整个数据集的Python和JVM对象之间来回转换时，性能将会受到显著影响。</p><p><img src="/2019/02/why-use-scala-udf/./sparkudf.png" alt="Credit:  https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9"></p><p>在上图中，Python程序的指令（1）被转换为Spark执行计划，并通过SparkSession JVM对象（2）传递给集群中不同机器上的两个执行程序（3）。 执行程序通常会从外部源（如HDFS）加载数据，在内存中执行某些转换，然后将数据写回外部存储。 数据将在程序的生命周期内保留在JVM（3）中。</p><p>而使用Python UDF时，数据必须经过几个额外的步骤。 首先，数据必须从Java（4）序列化，这样运行UDF所在的Python进程才可以将其读入（5）。 然后，Python运算完的结果经过一些列序列化和反序列化然后返回到JVM。</p><p>那么我们该如何优化呢？我们可以直接使用Scala来编写Spark UDF。Scala UDF可以直接在执行程序的JVM中运行，因此数据将跳过两轮序列化和反序列化，处理的效率将会比使用Python UDF高的多。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>启动Python进程的开销不小，但是真正的开销在于将数据序列化到Python中。推荐在Spark中定义UDF时首选Scala或Java，即使UDFs是用Scala/Java编写的，不用担心，我们依然可以在python(pyspark)中使用它们，简单实例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### pyspark --jars [path/to/jar/x.jar]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pre Spark 2.1,</span></span><br><span class="line">spark._jvm.com.test.spark.udf.MyUpper.registerUDF(spark._jsparkSession)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.1+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">sqlContext.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.3+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">spark.udf.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your UDF</span></span><br><span class="line">spark.sql(<span class="string">"""SELECT my_upper('abeD123okoj')"""</span>).show()</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9" target="_blank" rel="noopener">Using Scala UDFs in PySpark</a></p><p><a href="http://shop.oreilly.com/product/0636920034957.do" target="_blank" rel="noopener">[BOOK] Spark - The Definitive Guide</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Scala </tag>
            
            <tag> UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac上解决访问github慢之懒人版</title>
      <link href="/2019/02/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E4%B9%8B%E6%87%92%E4%BA%BA%E7%89%88.html"/>
      <url>/2019/02/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E4%B9%8B%E6%87%92%E4%BA%BA%E7%89%88.html</url>
      
        <content type="html"><![CDATA[<p>写了一个简单脚本用来解决Mac上访问github慢的问题，基本思路如下：</p><ol><li>访问 <a href="http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo" target="_blank" rel="noopener">http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo</a> 获取github的IP地址</li><li>在/etc/hosts中加入查询到的IP和域名 （需要root 权限）</li><li>在终端在输以下指令刷新DNS（需要root 权限）</li></ol><p>运行以下shell脚本即可，需要root权限。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>get the fastest github ip</span><br><span class="line">fast_ip=`curl http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo 2&gt;/dev/null| grep -E -o "([0-9]&#123;1,3&#125;[\.])&#123;3&#125;[0-9]&#123;1,3&#125;" |sed -n '2p'`;</span><br><span class="line">echo $fast_ip;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>Add or replace the $fast_ip in /etc/hosts</span><br><span class="line">if grep "www.github.com" /etc/hosts &gt;/dev/null;then</span><br><span class="line">echo "github dns exists, replace it with the latest one $fast_ip";</span><br><span class="line">sed -i -e "s|[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;  *www\.github\.com|$fast_ip www\.github\.com|"  /etc/hosts;</span><br><span class="line">else</span><br><span class="line">echo "github dns does not exist, replace it with the latest one $fast_ip";</span><br><span class="line">echo -e "\n#Github\n$fast_ip www.github.com\n" &gt;&gt; /etc/hosts;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">killall -HUP mDNSResponder;say DNS cache has been flushed;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac共享主机网络给虚拟机</title>
      <link href="/2019/02/Mac%E5%85%B1%E4%BA%AB%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%99%E8%99%9A%E6%8B%9F%E6%9C%BA.html"/>
      <url>/2019/02/Mac%E5%85%B1%E4%BA%AB%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%99%E8%99%9A%E6%8B%9F%E6%9C%BA.html</url>
      
        <content type="html"><![CDATA[<p>因工作需要需要且身边没有windows系统的笔记本，无奈只好在mac上利用虚拟机安装一个win7系统作为临时过渡。我使用的虚拟机软件是Parallels Desktop（以下简称PD）<img src="/2019/02/Mac共享主机网络给虚拟机/./2019021611181380.png" width="30%" alt>PD提供三种不同网络模式供用户选择:</p><ul><li>共享网络（推荐）</li><li>桥接网络</li><li>Host-Only网络<img src="/2019/02/Mac共享主机网络给虚拟机/./20190216110149123.png" width="60%" alt></li></ul><p>各种网络模式的区别请移步<a href="https://kb.parallels.com/en/4948" target="_blank" rel="noopener">官方文档</a></p><p>一开始我并没有任何设置，直接使用默认的<strong>共享网络</strong>模式，使用过程中出现了有时候连得上有时候连不上的情况。后来经过一番搜索发现即使用共享网络模式，也需要一些简单的设置。具体步骤如下：</p><ol><li><p>在PD的偏好设置中进行网络设置，添加(+)端口转发规则如下：</p><img src="/2019/02/Mac共享主机网络给虚拟机/./20190216112753690.png" width="60%" alt></li><li><p>在虚拟机的网络设置中使用<strong>共享网络</strong>模式</p><img src="/2019/02/Mac共享主机网络给虚拟机/./20190216113314497.png" width="60%" alt></li><li><p>重启虚拟机即可</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac上解决访问github慢问题</title>
      <link href="/2019/02/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E9%97%AE%E9%A2%98.html"/>
      <url>/2019/02/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E9%97%AE%E9%A2%98.html</url>
      
        <content type="html"><![CDATA[<ol><li>访问 <a href="http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo" target="_blank" rel="noopener">http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo</a> 获取github的IP地址</li><li>在/etc/hosts中加入查询到的IP和域名 （需要root 权限）</li><li>在终端在输以下指令刷新DNS（需要root 权限）  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo killall -HUP mDNSResponder;say DNS cache has been flushed</span><br></pre></td></tr></table></figure></li></ol><p> 或者也可以参见<a href="https://blog.csdn.net/yolohohohoho/article/details/87647036" target="_blank" rel="noopener">懒人版代码</a></p><p>其他Mac相关问题：<a href="https://blog.csdn.net/yolohohohoho/article/details/87892412" target="_blank" rel="noopener">brew update慢的解决方法</a><a href="https://blog.csdn.net/yolohohohoho/article/details/87893368" target="_blank" rel="noopener">conda install慢的解决方法</a></p>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VirtualBox共享MacOS的VPN</title>
      <link href="/2019/02/VirtualBox%E5%85%B1%E4%BA%ABMacOS%E7%9A%84VPN.html"/>
      <url>/2019/02/VirtualBox%E5%85%B1%E4%BA%ABMacOS%E7%9A%84VPN.html</url>
      
        <content type="html"><![CDATA[<p>在Mac上装了一个Cloudera的quickstart版本到virtualbox里面发现无法共享主机的VPN，简单搜索了一下，只需要做一些基本的配置就可以了。</p><h4 id="设置主机SS的HTTP-代理"><a href="#设置主机SS的HTTP-代理" class="headerlink" title="设置主机SS的HTTP 代理"></a>设置主机SS的HTTP 代理</h4><p>如图：</p><img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122645771.png" width="60%" alt><h4 id="设置虚拟机网络连接模式"><a href="#设置虚拟机网络连接模式" class="headerlink" title="设置虚拟机网络连接模式"></a>设置虚拟机网络连接模式</h4><p>选择桥接模式,并选择WiFI(Airport).<img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122348636.png" width="60%" alt></p><h4 id="设置虚拟机实例网络连接"><a href="#设置虚拟机实例网络连接" class="headerlink" title="设置虚拟机实例网络连接"></a>设置虚拟机实例网络连接</h4><p>首先，查看主机的网络链接信息<img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122402569.png" width="60%" alt></p><p>然后进入实例的网络连接设置</p><p>添加并创建网络连接</p><img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122427601.png" width="60%" alt><img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122439284.png" width="60%" alt><p>因为选用桥接模式，手动为该实例添加一个和主机在同一范围的IP，并根据主机的TCP/IP信息填写网关，配置如下：<img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122450764.png" width="60%" alt>测试在虚拟机中ping主机的内网IP：<img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122500875.png" width="60%" alt>然后配置HTTP-PROXY：</p><ul><li>IP为主机内网IP</li><li>端口为之前SS设置的监听端口号</li></ul><img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122525464.png" width="60%" alt>测试<img src="/2019/02/VirtualBox共享MacOS的VPN/./20190216122617322.png" width="60%" alt><p><strong>Voilà Voilà</strong></p>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TD笔记 | Teradata数据压缩</title>
      <link href="/2019/02/%5BTD%E7%AC%94%E8%AE%B0%5DTeradata%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html"/>
      <url>/2019/02/%5BTD%E7%AC%94%E8%AE%B0%5DTeradata%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html</url>
      
        <content type="html"><![CDATA[<p>工作上需要研究Teradata CLOB类型，因为去看了官方文档，自己做了点笔记如下：</p><h3 id="Teradata数据压缩"><a href="#Teradata数据压缩" class="headerlink" title="Teradata数据压缩"></a>Teradata数据压缩</h3><h4 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h4><p>本章描述了几种数据压缩选项，它能够帮助你减少磁盘空间的使用，在某种情况下，还可以提高I/O性能。</p><ul><li>多值压缩（MVC)</li><li>算法压缩（ALC）</li><li>行压缩</li><li>行标题压缩</li><li>自动压缩</li><li>哈希索引和连接索引行压缩</li><li>块级压缩（BLC) </li></ul><p>压缩的目标是利用最少的位数(bits)来准确的表示信息。压缩方法可分为物理方法和逻辑方法。物理方法独立于数据本身意义对其进行重新编码， 而逻辑方法则通过一个更紧凑的集合来替换。</p><p>压缩通过在单位物理容量中存储更多的逻辑数据来降低存储成本。压缩产生更小的行，因此每个可以数据块存储更多行以减少数据块数量。</p><p>压缩还可以提高系统性能，因为每个查询返回更少的物理数据，同时压缩过的数据在内存中保持压缩状态，因此FSG[1]缓存可容纳更多行，从而减少磁盘I/O的大小。</p><p>[1]FSG cache: File Segment cache, a Teradata caching approach.</p><p>算法压缩可以是有损或者是无损的，这取决于所选用的的算法。</p><p>TD的压缩一个很小的初始成本，但是即使对于小表的查询，主要选择的压缩方法能过减小表的大小，这就是一个净赢。</p><h4 id="块级压缩"><a href="#块级压缩" class="headerlink" title="块级压缩"></a>块级压缩</h4><p>数据块是I/O基本物理单位，用于定义TD如何处理数据。当你指定了块级压缩选项，TD将以压缩格式存储数据来减少存储空间。</p><p>BLC可以应用到这几种类型的表：</p><ul><li>主要数据，回退，甚至是无法重新启动的表</li></ul><p>BLC还可以应用于这几种类型的子表：</p><ul><li>BLOB, CLOB, XML, JOIN INDEX, HASH INDEX和Reference index.</li></ul><p>BLC独立应用于其他任何应用于相同数据的压缩类型。BLC可以使用更多的CPU来动态压缩和解压数据，所以查询性能是否随BLC而增强取决于性能是否受I/O带宽或CPU使用率的限制。</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Teradata </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用sed根据变量值注释掉文件中相匹配的记录行</title>
      <link href="/2018/03/%E4%BD%BF%E7%94%A8sed%E6%A0%B9%E6%8D%AE%E5%8F%98%E9%87%8F%E5%80%BC%E6%B3%A8%E9%87%8A%E6%8E%89%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9B%B8%E5%8C%B9%E9%85%8D%E7%9A%84%E8%AE%B0%E5%BD%95%E8%A1%8C.html"/>
      <url>/2018/03/%E4%BD%BF%E7%94%A8sed%E6%A0%B9%E6%8D%AE%E5%8F%98%E9%87%8F%E5%80%BC%E6%B3%A8%E9%87%8A%E6%8E%89%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9B%B8%E5%8C%B9%E9%85%8D%E7%9A%84%E8%AE%B0%E5%BD%95%E8%A1%8C.html</url>
      
        <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>有批量的profile文件需要更新，因为其中某些环境变量的值需要更新，于是利用sed命令写了如下函数，方便以后使用。</p><h2 id="代码展示"><a href="#代码展示" class="headerlink" title="代码展示"></a>代码展示</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unset -f add_var_to_file</span><br><span class="line">function add_var_to_file()&#123;</span><br><span class="line">env_var=$1</span><br><span class="line">new_value=$2</span><br><span class="line">tgt_file=$3</span><br><span class="line"><span class="meta">#</span> comment out old env lines</span><br><span class="line">sed -e "/ $env_var/ s/^#*/#/" -i $tgt_file;</span><br><span class="line"><span class="meta">#</span> add new env variable line</span><br><span class="line">curr_time=`date +"%Y-%m-%d %H:%M:%S"`;</span><br><span class="line">echo ""&gt;&gt; $tgt_file</span><br><span class="line">echo "# updated at $curr_time" &gt;&gt; $tgt_file;</span><br><span class="line">echo "export $env_var=$new_value;" &gt;&gt;$tgt_file;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sed </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell中获取脚本的绝对路径$( cd _$( dirname _${BASH_SOURCE[0]}_ )_ &amp;&amp; pwd).</title>
      <link href="/2018/02/Shell%E4%B8%AD%E8%8E%B7%E5%8F%96%E8%84%9A%E6%9C%AC%E7%9A%84%E7%BB%9D%E5%AF%B9%E8%B7%AF%E5%BE%84$(%20cd%20_$(%20dirname%20_$%7BBASH_SOURCE%5B0%5D%7D_%20)_%20&amp;&amp;%20pwd).html"/>
      <url>/2018/02/Shell%E4%B8%AD%E8%8E%B7%E5%8F%96%E8%84%9A%E6%9C%AC%E7%9A%84%E7%BB%9D%E5%AF%B9%E8%B7%AF%E5%BE%84$(%20cd%20_$(%20dirname%20_$%7BBASH_SOURCE%5B0%5D%7D_%20)_%20&amp;&amp;%20pwd).html</url>
      
        <content type="html"><![CDATA[<h5 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h5><p>我们可以在bash中使用以下命令获取所执行脚本的绝对路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">DIR=$( cd "$(dirname "$&#123;BASH_SOURCE[0]&#125;")" &amp;&amp; pwd);</span><br><span class="line">echo $DIR</span><br></pre></td></tr></table></figure><h5 id="代码解释"><a href="#代码解释" class="headerlink" title="代码解释"></a>代码解释</h5><p><strong>BASH_SOURCE[0]</strong> - 等价于 BASH_SOURCE ,取得当前执行的 shell 文件所在的路径及文件名<strong>dirname</strong>  - 去除文件名中的非目录部分，仅显示与目录有关的部分<strong>$()</strong>      - 相当于 `command`, 即获取command命令的结果<strong>&amp;&amp;</strong> - 逻辑运算符号，只有当&amp;&amp;左边运行成功时才会运行&amp;&amp;右边的命令</p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>该命令获取脚本源文件的文件路径名，获取其目录部分，然后 <strong><em>cd</em></strong> 到该目录，使用 <strong><em>pwd</em></strong> 获取当前目录的==完整路径==，然后将这个路径的值赋给变量 <strong><em>DIR</em></strong>。</p><p>例如 /tmp/test/test.sh 内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>！/bin/bash</span><br><span class="line"></span><br><span class="line">echo "$&#123;BASH_SOURCE[0]&#125;"</span><br><span class="line">echo "$&#123;BASH_SOURCE&#125;"</span><br><span class="line">echo "$(dirname "$&#123;BASH_SOURCE[0]&#125;")"</span><br><span class="line">echo "$( cd "$(dirname "$&#123;BASH_SOURCE[0]&#125;")" &amp;&amp; pwd)"</span><br></pre></td></tr></table></figure><p>若在当前目录 /tmp 下执行 bash ./test/test.sh ,输出为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lestat@Lestats-MBP:/tmp$ bash ./test/test.sh</span><br><span class="line">test/test.sh</span><br><span class="line">test/test.sh</span><br><span class="line">test</span><br><span class="line">/tmp/test</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux中find命令的用法举例</title>
      <link href="/2018/02/Linux%E4%B8%ADfind%E5%91%BD%E4%BB%A4%E7%9A%84%E7%94%A8%E6%B3%95%E4%B8%BE%E4%BE%8B.html"/>
      <url>/2018/02/Linux%E4%B8%ADfind%E5%91%BD%E4%BB%A4%E7%9A%84%E7%94%A8%E6%B3%95%E4%B8%BE%E4%BE%8B.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>find是Linux系统中最重要和最常用的命令之一，它可以根据不同的条件来查找文件，例如权限、拥有者、修改日期/时间、文件大小等等。</p><p>其基本语法如下：</p><pre><code>$ find [path] [option] [expression]</code></pre><p>下面我来总结一下自己常用的一些用法和例子</p><h2 id="find用法举例"><a href="#find用法举例" class="headerlink" title="find用法举例"></a>find用法举例</h2><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><h4 id="列出当前目录和子目录下的所有文件"><a href="#列出当前目录和子目录下的所有文件" class="headerlink" title="列出当前目录和子目录下的所有文件"></a>列出当前目录和子目录下的所有文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find</span><br><span class="line"><span class="meta">$</span> #等同于 find .</span><br></pre></td></tr></table></figure><h4 id="查找特定目录下的文件"><a href="#查找特定目录下的文件" class="headerlink" title="查找特定目录下的文件"></a>查找特定目录下的文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> #list all files under $HOME dir.</span><br><span class="line"><span class="meta">$</span> find $HOME</span><br></pre></td></tr></table></figure><h4 id="查找特定文件名的文件"><a href="#查找特定文件名的文件" class="headerlink" title="查找特定文件名的文件"></a>查找特定文件名的文件</h4><p>使用 <code>-name</code>选项来查找特定文件名的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> #list all files(or sub directories) whose filename starts with `test_` under $HOME dir.</span><br><span class="line"><span class="meta">$</span> find $HOME -name "test_*" </span><br><span class="line"><span class="meta">$</span># list out all .py files under $HOME</span><br><span class="line"><span class="meta">$</span> find $HOME -name "*.py"</span><br></pre></td></tr></table></figure><p>如果要忽略大小写，则使用 ==-iname== 选项，而不是 -name 选项。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -iname "*.Csv"</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> #list all files(or sub directories) whose filename starts with `test_` under $HOME dir with maxdepth=1 mindepth =1 .</span><br><span class="line"><span class="meta">$</span> find $HOME -name "test_*"  -maxdepth 1 -mindepth 1</span><br></pre></td></tr></table></figure><h4 id="查找特定的文件类型"><a href="#查找特定的文件类型" class="headerlink" title="查找特定的文件类型"></a>查找特定的文件类型</h4><p>我们可以用 ==-type== 来指定需要查找的文件类型，例如 </p><ul><li>-type f 用来查找文件，</li><li>-type d 用来查找文件夹/目录</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -type f -name "test_"</span><br></pre></td></tr></table></figure><h4 id="在多个目录下查找"><a href="#在多个目录下查找" class="headerlink" title="在多个目录下查找"></a>在多个目录下查找</h4><p>指定多个目录路径即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME /tmp -type f -name "test_"</span><br></pre></td></tr></table></figure><h4 id="反向查找"><a href="#反向查找" class="headerlink" title="反向查找"></a>反向查找</h4><p>我们可以用 ==not== 或者 ==！(感叹号)== 来表达一个 “<strong>非</strong>”的逻辑，用来查找不满足条件的所有文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>list out all files that are not .txt </span><br><span class="line"><span class="meta">$</span> find $HOME -not -name "*.txt"</span><br></pre></td></tr></table></figure><h3 id="进阶用法"><a href="#进阶用法" class="headerlink" title="进阶用法"></a>进阶用法</h3><h4 id="限制目录查找的深度"><a href="#限制目录查找的深度" class="headerlink" title="限制目录查找的深度"></a>限制目录查找的深度</h4><p>find 命令默认会递归查找整个目录树，而这非常消耗时间和资源。我们可以通过设定 ==-maxdepth== 和 ==-mindepth 选项来限制目录查找的深度的范围。</p><h4 id="查找指定权限的文件"><a href="#查找指定权限的文件" class="headerlink" title="查找指定权限的文件"></a>查找指定权限的文件</h4><p>我们可以通过指定 ==-perm== 选项来查找具有特定权限的文件。下面的示例中查找了所有具有 0755 权限的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find . -type f -perm 0755</span><br></pre></td></tr></table></figure><p><strong>查找只读文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find . -type f -perm /u=r</span><br></pre></td></tr></table></figure><p><strong>查找可执行文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find . -type f -perm /a=x</span><br></pre></td></tr></table></figure><h4 id="基于文件拥有者和用户组的查找"><a href="#基于文件拥有者和用户组的查找" class="headerlink" title="基于文件拥有者和用户组的查找"></a>基于文件拥有者和用户组的查找</h4><p>我们可以使用 ==-user== 和 ==-group== 选项来指定查找文件的特定owner和group</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find . -user lestat -staff</span><br></pre></td></tr></table></figure><h4 id="基于日期和时间的查找"><a href="#基于日期和时间的查找" class="headerlink" title="基于日期和时间的查找"></a>基于日期和时间的查找</h4><p>我们还可以基于日期和时间进行查找。例如我们想查找出去一些旧文件时，我们就可以使用这个选项。</p><blockquote><p>与时间相关的选项有  <font color="darkblue"><b>--amin, -atime, -cmin, -ctime, -mmin, -mtime</b></font>    它们之间的差别可以参考<a href="https://www.quora.com/What-is-the-difference-between-mtime-atime-and-ctime" target="_blank" rel="noopener">What-is-the-difference-between-mtime-atime-and-ctime</a></p></blockquote><p><strong>查找 N 天之内修改过的文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -mtime 7</span><br></pre></td></tr></table></figure><p><strong>查找 N 天之内被访问过的文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -atime -7</span><br></pre></td></tr></table></figure><p><strong>查找某段时间范围内被修改过内容的文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -mtime +5 -mtime -10</span><br></pre></td></tr></table></figure><p><strong>查找过去的 N 分钟内状态发生改变的文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -cmin -60</span><br></pre></td></tr></table></figure><p><strong>查找过去的 1 小时内被修改过内容的文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -mmin -60</span><br></pre></td></tr></table></figure><h4 id="基于文件大小的查找"><a href="#基于文件大小的查找" class="headerlink" title="基于文件大小的查找"></a>基于文件大小的查找</h4><p> <strong>查找指定大小的文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -size 50M</span><br></pre></td></tr></table></figure><p><strong>查找大小在一定范围内的文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -size +50M -size -100M</span><br></pre></td></tr></table></figure><h4 id="找出空文件"><a href="#找出空文件" class="headerlink" title="找出空文件"></a>找出空文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME  -type f -empty</span><br></pre></td></tr></table></figure><h3 id="高级操作-TO-DO"><a href="#高级操作-TO-DO" class="headerlink" title="高级操作 TO-DO"></a>高级操作 TO-DO</h3><p>find 命令不仅可以通过特定条件来查找文件，还可以对查找到的文件使用任意linux命令进行操作, 例如 <strong><em>ls</em></strong>，<strong><em>mv</em></strong>，<strong><em>rm</em></strong> 等</p><h4 id="find-ls"><a href="#find-ls" class="headerlink" title="find + ls"></a>find + ls</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -exec ls -ld &#123;&#125; \;</span><br></pre></td></tr></table></figure><h4 id="find-mv"><a href="#find-mv" class="headerlink" title="find + mv"></a>find + mv</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME name "tmp_*" -exec mv -t /tmp &#123;&#125; +</span><br></pre></td></tr></table></figure><h4 id="find-rm"><a href="#find-rm" class="headerlink" title="find + rm"></a>find + rm</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> find $HOME -name "tmp_*" -exec rm -r -f &#123;&#125; \;</span><br></pre></td></tr></table></figure><h2 id="参考来源"><a href="#参考来源" class="headerlink" title="参考来源"></a>参考来源</h2><p><a href="https://www.tecmint.com/35-practical-examples-of-linux-find-command/" target="_blank" rel="noopener">35-practical-examples-of-linux-find-command/</a></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> find </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux中sed命令的用法举例</title>
      <link href="/2018/01/Linux%E4%B8%ADsed%E5%91%BD%E4%BB%A4%E7%9A%84%E7%94%A8%E6%B3%95%E4%B8%BE%E4%BE%8B.html"/>
      <url>/2018/01/Linux%E4%B8%ADsed%E5%91%BD%E4%BB%A4%E7%9A%84%E7%94%A8%E6%B3%95%E4%B8%BE%E4%BE%8B.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>sed - stream editor，流编辑器，用于对输入流（文件或来自管道的输入）执行基本文本转换，是Linux系统中最重要和最常用的命令之一。</p><h3 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h3><p>其基本语法如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sed [OPTION]... &#123;script-only-if-no-other-script&#125; [input-file]...</span><br></pre></td></tr></table></figure><h3 id="常用选项"><a href="#常用选项" class="headerlink" title="常用选项"></a>常用选项</h3><table><thead><tr><th>选项</th><th>说明</th></tr></thead><tbody><tr><td><strong><em>-n, --quiet, --silent</em></strong></td><td>静默模式, 只输出被 sed 处理过的行;</td></tr><tr><td><strong><em>-e script, --expression=script</em></strong></td><td>指定命令用于执行, 可以使用多个 -e 执行多个命令</td></tr><tr><td><strong><em>-f script-file, --file=script-file</em></strong></td><td>指定含有命令的脚本用于执行</td></tr><tr><td><strong><em>-r, --regexp-extended</em></strong></td><td>让脚本支持拓展的正则表达式语法, 如 ==+, ?, |, ()== 等</td></tr><tr><td><strong><em>-i  --in-place</em></strong></td><td>直接在指定的文件里修改编辑替换, 不在标准输出中输出任何内容</td></tr></tbody></table><h3 id="用法举例"><a href="#用法举例" class="headerlink" title="用法举例"></a>用法举例</h3><h4 id="打印"><a href="#打印" class="headerlink" title="打印"></a>打印</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> #打印最后一行</span><br><span class="line"><span class="meta">$</span> sed -n '$p' geekfile.txt</span><br><span class="line"><span class="meta">$</span> #打印指定范围行,如第1行到第3行</span><br><span class="line"><span class="meta">$</span> sed -n '1,3p' geekfile.txt</span><br><span class="line"><span class="meta">$</span> #打印从第2行开始到以test结尾的行之间的每一行</span><br><span class="line"><span class="meta">$</span> sed -n '2,/test$/p' geekfile.txt</span><br></pre></td></tr></table></figure><h4 id="替换"><a href="#替换" class="headerlink" title="替换"></a>替换</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> #替换行中第1次出现的pattern</span><br><span class="line"><span class="meta">$</span> sed 's/unix/linux/' geekfile.txt</span><br><span class="line"><span class="meta">$</span> #替换行中第n次出现的pattern</span><br><span class="line"><span class="meta">$</span> sed 's/unix/linux/2' geekfile.txt</span><br><span class="line"><span class="meta">$</span> #替换行中所有出现的pattern</span><br><span class="line"><span class="meta">$</span> sed 's/unix/linux/g' geekfile.txt</span><br><span class="line"><span class="meta">$</span> #替换行中第n次及之后出现的所有pattern,如第三次及之后</span><br><span class="line"><span class="meta">$</span> sed 's/unix/linux/3g' geekfile.txt</span><br><span class="line"><span class="meta">$</span> # 替换指定行,如第三行</span><br><span class="line"><span class="meta">$</span> sed '3 s/unix/linux/' geekfile.txt</span><br><span class="line"><span class="meta">$</span> #替换指定范围的行，如1到3行</span><br><span class="line"><span class="meta">$</span> sed '1,3 s/unix/linux/' geekfile.txt</span><br><span class="line"><span class="meta">$</span> #替换指定范围的行，如第2行到最后一行，$表示最后一行</span><br><span class="line"><span class="meta">$</span> sed '2,$ s/unix/linux/' geekfile.txt</span><br><span class="line"><span class="meta">$</span> #只打印替换过的行</span><br><span class="line"><span class="meta">$</span> sed 's/unix/linux/p' geekfile.txt</span><br></pre></td></tr></table></figure><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> #删除第5行</span><br><span class="line"><span class="meta">$</span> sed '5d' filename.txt</span><br><span class="line"><span class="meta">$</span> #删除最后一行</span><br><span class="line"><span class="meta">$</span> sed '$d' filename.txt</span><br><span class="line"><span class="meta">$</span> #删除指定范围的行, 如第3行到第6行</span><br><span class="line"><span class="meta">$</span> sed '3,6d' filename.txt</span><br><span class="line"><span class="meta">$</span> #删除指定范围的行, 如第3行到最后一行</span><br><span class="line"><span class="meta">$</span> sed '3,$d' filename.txt</span><br><span class="line"><span class="meta">$</span> #删除pattern所匹配的行，如删除含有abc的行</span><br><span class="line"><span class="meta">$</span> sed '/abc/d' filename.txt</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.geeksforgeeks.org/sed-command-in-linux-unix-with-examples/" target="_blank" rel="noopener">sed-command-in-linux-unix-with-examples</a></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sed </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tr命令用法总结</title>
      <link href="/2018/01/tr%E5%91%BD%E4%BB%A4%E7%94%A8%E6%B3%95%E6%80%BB%E7%BB%93.html"/>
      <url>/2018/01/tr%E5%91%BD%E4%BB%A4%E7%94%A8%E6%B3%95%E6%80%BB%E7%BB%93.html</url>
      
        <content type="html"><![CDATA[<h3 id="功能简介"><a href="#功能简介" class="headerlink" title="功能简介"></a>功能简介</h3><p>Translate or Delete characters, 主要用来转换或者删除字符。</p><h3 id="命令格式"><a href="#命令格式" class="headerlink" title="命令格式"></a>命令格式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tr [OPTION]... SET1 [SET2]</span><br></pre></td></tr></table></figure><p>其中SET为：</p><ul><li>CHAR1-CHAR2， 表示范围。只能由小到大，不能由大到小。 如0-9，a-z</li><li>常见的反斜杠转义字符，比如&#39;\n&#39;, &#39;\t&#39;, &#39;\r&#39;之类的</li><li>[:alnum:] ：所有字母字符与数字-</li><li>[:digit:] ：所有数字</li><li>[:lower:] ：所有小写字母</li><li>[:upper:] ：所有大写字母</li><li>[:space:] ：所有水平与垂直空格符<h3 id="常用选项"><a href="#常用选项" class="headerlink" title="常用选项"></a>常用选项</h3><table><thead><tr><th>名称</th><th>英文解释</th><th>中文解释</th></tr></thead><tbody><tr><td>-c, -C, --complement</td><td>first complement SET1</td><td>用SET1中字符的补集替换SET1，这里的字符集为ASCII。</td></tr><tr><td>-d, --delete</td><td>delete characters in SET1, do not translate</td><td>删除文件中所有在SET1中出现的字符</td></tr><tr><td>-s, --squeeze-repeats</td><td>replace each input sequence of  a  repeated  character  that  is listed in SET1 with a single occurrence of that character</td><td>删除文件中重复并且在SET1中出现的字符，只保留一个</td></tr></tbody></table></li></ul><h3 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a>常见用法</h3><h4 id="大小写转换"><a href="#大小写转换" class="headerlink" title="大小写转换"></a>大小写转换</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo “hello” | tr [a-z] [A-Z];</span><br><span class="line">echo "hello" | tr [:lower:] [:upper:]</span><br></pre></td></tr></table></figure><h4 id="将Windows文件中的CRLF换为LF"><a href="#将Windows文件中的CRLF换为LF" class="headerlink" title="将Windows文件中的CRLF换为LF"></a>将Windows文件中的CRLF换为LF</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat $myfile | tr -d '\015'</span><br></pre></td></tr></table></figure><h4 id="删除Windows文件中的-39-M-39-字符"><a href="#删除Windows文件中的-39-M-39-字符" class="headerlink" title="删除Windows文件中的&#39;^M&#39;字符"></a>删除Windows文件中的&#39;^M&#39;字符</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat $myfile | tr -d "\r"</span><br></pre></td></tr></table></figure><h4 id="删除连续的字符"><a href="#删除连续的字符" class="headerlink" title="删除连续的字符"></a>删除连续的字符</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo "hhhhhh" | tr -s [:alnum:]</span><br></pre></td></tr></table></figure><h4 id="删除数字"><a href="#删除数字" class="headerlink" title="删除数字"></a>删除数字</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo "hello 123 world 45345" |tr -d '0-9'</span><br></pre></td></tr></table></figure><h4 id="只保留数字"><a href="#只保留数字" class="headerlink" title="只保留数字"></a>只保留数字</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo "hello 123 world 45345" |tr -dc  '0-9'</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 数据仓库设计基础总结</title>
      <link href="/2017/12/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93.html"/>
      <url>/2017/12/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93.html</url>
      
        <content type="html"><![CDATA[<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li>关系模型、多维模型和Data Vault模型是三种常见的数据仓库模型。</li><li>数据结构、完整性约束和SQL语言是关系模型的三个要素。</li><li>规范化是通过应用范式规则实现的。第一范式(1NF)要求保持数据的原子性、第二范式(2NF)消除了部分依赖、第三范式(3NF)消除了传递依赖。关系模 型的数据仓库一般要求满足3NF。</li><li>事实、维度、粒度是维度模型的三个核心概念。</li><li>维度模型的四步设计法是选择业务流程、声明粒度、确定维度、确定事实。 </li><li>星型模式和雪花模式是维度模型的两种逻辑表示。对星型模式进一步规范化，就形成了雪花模式。</li><li>Data Vault模型有中心表(Hub)、链接表(Link)、附属表(Satellite)三个主要组成部分。中心表记录业务主键，链接表记录业务关系，附属表记录业务描述。 </li><li>Data Vault不区分数据在业务层面的正确与错误，它保留操作型系统的所有时间的所有数据，装载数据时不做数据验证、清洗等工作。 </li><li>数据集市是部门级的、面向单一主题域的数据仓库。</li><li>数据集市的复杂度和需要处理的数据都小于数据仓库，因此更容易建立与维护。 </li><li>实施一个数据仓库项目的主要步骤是:定义范围、确认需求、逻辑设计、物理设计、装载数据、访问数据、管理维护。<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2>[Book]Hadoop构建数据仓库实践, 第2章第6节 —— 小结</li></ol>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 数据仓库实施步骤</title>
      <link href="/2017/12/%20%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AE%9E%E6%96%BD%E6%AD%A5%E9%AA%A4.html"/>
      <url>/2017/12/%20%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AE%9E%E6%96%BD%E6%AD%A5%E9%AA%A4.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>实施一个数据仓库项目的主要步骤是:定义项目范围、收集并确认业务需求和技术需求、逻辑设计、物理设计、从源系统向数据仓库装载数据、使数据可以被访问以辅助决策、管理和维护数据仓库。</p><h2 id="定义范围"><a href="#定义范围" class="headerlink" title="定义范围"></a><strong>定义范围</strong></h2><p>在实施数据仓库前，需要制定一个开发计划。这个计划的关键输入是信息需求和数据仓库用户的优先级。当这些信息被定义和核准后，就可以制作一个交付物列表，并给数据仓库开发团队分配相应的任务。</p><p>首要任务是定义项目的范围。项目范围定义了一个数据仓库项目的边界。典型的范围定义是组织、地区、应用、业务功能的联合表示。定义范围时通常需要权衡考虑 资源(人员、系统、预算等)、进度(项目的时间和里程碑要求)、功能(数据仓库承诺达到的能力)三方面的因素。定义好清晰明确的范围，并得到所有项目干系人的 一致认可，对项目的成功是非常重要的。项目范围是设定正确的期望值、评估成本、估计风险、制定开发优先级的依据。</p><h2 id="确定需求"><a href="#确定需求" class="headerlink" title="确定需求"></a><strong>确定需求</strong></h2><p>数据仓库项目的需求可以分为业务需求和技术需求。 </p><ul><li><strong>定义业务需求</strong>建立数据仓库的主要目的是为组织赋予从全局访问数据的能力。数据的细节程度必须能够满足用户执行分析的需求，并且数据应该被表示为用户能够理解的业务术 语。对数据仓库中数据的分析将辅助业务决策，因此，作为数据仓库的设计者，应该清楚业务用户是如何做决策的，在决策过程中提出了哪些问题，以及哪些数据是回答 这些问题所需要的。与业务人员进行面对面的沟通，是理解业务流程的好方式。沟通的结果是使数据仓库的业务需求更加明确。在为数据仓库收集需求的过程中，还要考 虑设计要能适应需求的变化。<ul><li><strong>定义技术需求</strong>数据仓库的数据来源是操作型系统，这些系统日复一日地处理着各种事务活动。操作型系统大都是联机事务处理系统。数据仓库会从多个操作型源系统抽取数据。但 是，一般不能将操作型系统里的数据直接迁移到数据仓库，而是需要一个中间处理过程，这就是所谓的ETL过程。需要知道如何清理操作型数据，如何移除垃圾数据，如 何将来自多个源系统的相同数据整合在一起。另外，还要确认数据的更新频率。例如，如果需要进行长期的或大范围的数据分析，可能就不需要每天装载数据，而是每周 或每月装载一次。注意，更新频率并不决定数据的细节程度，每周汇总的数据有可能每月装载(当然这种把数据转换和数据装载分开调度的做法并不常见)。在数据仓库 设计的初始阶段，需要确定数据源有哪些、数据需要做哪些转换以及数据的更新频率是什么。</li></ul></li></ul><h2 id="逻辑设计"><a href="#逻辑设计" class="headerlink" title="逻辑设计"></a><strong>逻辑设计</strong></h2><p>定义了项目的范围和需求，就有了一个基本的概念设计。下面就要进入数据仓库的逻辑设计阶段。逻辑设计过程中，需要定义特定数据的具体内容，数据之间的关系，支持数据仓库的系统环境等，本质是发现逻辑对象之间的关系。</p><ul><li><strong>建立需要的数据列表</strong>细化业务用户的需求以形成数据元素列表。很多情况下，为了得到所需的全部数据，需要适当扩展用户需求或者预测未来的需要，一般从主题域涉及的业务因素入 手。例如，销售主题域的业务因素可能是客户、地区、产品、促销等。然后建立每个业务因素的元素列表，依据也是用户提出的需求。最后通过元素列表，标识出业务因 素之间的联系。这些工作完成后，应该已经获得了如下的信息:原始的或计算后的数据元素列表;数据的属性，比如是字符型的还是数字型的;合理的数据分组，比如国 家、省市、区县等分成一组，因为它们都是地区元素;数据之间的关系，比如国家、省市、区县的包含关系等。</li><li><strong>识别数据源</strong>现在已经有了需要的数据列表，下面的问题是从哪里可以得到这些数据，以及要得到这些数据需要多大的成本。需要把上一步建立的数据列表映射到操作型系统上。 应该从最大最复杂的源系统开始，在必要时再查找其他源系统。数据的映射关系可能是直接的或间接的，比如销售源系统中，商品的单价和折扣价可以直接获得，而折扣 百分比就需要计算得到。通常维度模型中的维度表可以直接映射到操作型源系统，而事实表的度量则映射到源数据在特定粒度级别上聚合计算后的结果。某些数据的获得 需要较高的成本，例如，用户想要得到促销相关的销售数据就不那么容易，因为促销期的定义从时间角度看是不连续的。</li><li><strong>制作实体关系图</strong>逻辑设计的交付物是实体关系图(entity-relationship diagram，简称ERD)和对它的说明文档(数据字典)。实体对应关系数据库中的表，属性对应关系数据库中的列。 ERD传统上与高度规范化的关系模型联系密切，但该技术在维度模型中也被广泛使用。在维度模型的ERD中，实体由事实表和维度表组成，关系体现为在事实表中引用维 度表的主键。因此先要确认哪些信息属于中心事实表，哪些信息属于相关的维度表。维度模型中表的规范化级别通常低于关系模型中的表。</li></ul><h2 id="物理设计"><a href="#物理设计" class="headerlink" title="物理设计"></a><strong>物理设计</strong></h2><p>物理设计指的是将<strong>逻辑设计的对象集合，转化为一个物理数据库，包括所有的表、索引、约束、视图等。</strong> 物理数据库结构需要优化以获得最佳的性能。每种数据库产品都有自己特别的优化方法，这些优化对查询性能有极大的影响。比较通用的数据仓库优化方法有位图索引和表分区。位图索引和表分区。<strong>位图索引对索引列的每个不同值建立一个位图。和普通的B树索引相比，位图索引占用的空间小，创建速度快。但由于并发的DML操作会锁定整个位图段的大量数据行，所以位图索引不适用于频繁更新的事务处理系统</strong>。而数据仓库对最终用户来说是一个只读系 统，其中某些维度的值基数很小，这样的场景非常适合利用位图索引优化查询。遗憾的是有些数据库管理系统如MySQL，还没有位图索引功能。大部分数据库系统都可以对表进行分区。表分区是将一个大表按照一定的规则分解成多个分区，每个表分区可以定义独立的物理存储参数。<strong>将不同分区存储到不同的磁盘上，查询表中数据时可以有效分布I/O操作，缓解系统压力</strong>。分区还有一个很有用的特性，叫做分区消除。<strong>查询数据的时候，数据库系统的优化器可以通过适当的查询条件过滤掉一些分区，从而避免扫描所有数据，提高查询效率，这就是分区消除</strong>。除了性能优化，数据仓库系统的可扩展性也非常重要。简单地说，可扩展性就是能够处理更大规模业务的特性。从技术上讲，可扩展性是一种通过增加资源，使服务 能力得到线性扩展的能力。比方说，一台服务器在满负荷时可以为一万个用户同时提供服务，当用户数增加到两万时，只需要再增加一台服务器，就能提供相同性能的服 务。成功的数据仓库会吸引越来越多的用户访问。随着时间的推移，数据量会越来越大，因此在做数据仓库物理设计时，出于可扩展性的考虑，应该把对硬件、软件、网 络带宽的依赖降到最低。第3章会详细讨论数据仓库在Hadoop上的扩展性问题。</p><h2 id="装载数据"><a href="#装载数据" class="headerlink" title="装载数据"></a><strong>装载数据</strong></h2><p>这个步骤实际上涉及整个ETL过程。需要执行的任务包括:</p><ul><li>源和目标结构之间建立映射关系;</li><li>从源系统抽取数据;</li><li>对数据进行清洗和转换;</li><li>将数据装载进数据仓库;</li><li>创建并存储元数据。 <h2 id="访问数据"><a href="#访问数据" class="headerlink" title="访问数据"></a><strong>访问数据</strong></h2>访问步骤是要使数据仓库的数据可以被使用，使用的方式包括: <strong>数据查询、数据分析、建立报表图表、数据发布等</strong>。根据采用的数据仓库架构，可能会引入数据集市的创建。通常，最终用户会使用图形化的前端工具向数据库提交查询，并显示查询结果。访问步骤需要执行以下任务:</li><li>为前端工具建立一个中间层。在这个中间层里，把数据库结构和对象名转化成业务术语，这样最终用户就可以使用与特定功能相关的业务语言同数据仓库交互。</li><li>管理和维护这个业务接口。 </li><li>建立和管理数据仓库里的中间表和汇总表。建立这些表完全是出于性能原因。中间表一般是在原始表上添加过滤条件获得的数据集合，汇总表则是对原始表进行聚 合操作后的数据集合。这些表中的记录数会远远小于原始表，因此前端工具在这些表上的查询会执行得更快。<h2 id="管理维护"><a href="#管理维护" class="headerlink" title="管理维护"></a><strong>管理维护</strong></h2>这个步骤涵盖在数据仓库整个生命周期里的管理和维护工作。这步需要执行的任务包括:确保对数据的安全访问、管理数据增长、优化系统以获得更好的性能、保证系统的可用性和可恢复性等。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[Book]Hadoop构建数据仓库实践, 第2章第5节 ——  数据仓库实施步骤</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 数据集市</title>
      <link href="/2017/12/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B8%82.html"/>
      <url>/2017/12/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B8%82.html</url>
      
        <content type="html"><![CDATA[<h3 id="数据集市"><a href="#数据集市" class="headerlink" title="数据集市"></a>数据集市</h3><h4 id="数据集市的概念"><a href="#数据集市的概念" class="headerlink" title="数据集市的概念"></a>数据集市的概念</h4><p>数据集市是数据仓库的一种简单形式，通常由组织内的业务部门自己建立和控制。一个数据集市面向单一主题域，如销售、财务、市场等。数据集市的数据源可以是操作型系统(独立数据集市)，也可以是企业级数据仓库(从属数据集市)。</p><h4 id="数据集市与数据仓库的区别"><a href="#数据集市与数据仓库的区别" class="headerlink" title="数据集市与数据仓库的区别"></a>数据集市与数据仓库的区别</h4><p>不同于数据集市，数据仓库处理整个组织范围内的多个主题域，通常是由组织内的核心单位，如IT部门承建，所以经常被称为中心数据仓库或企业数据仓库。数据仓库需要集成很多操作型源系统中的数据。由于数据集市的复杂度和需要处理的数据都小于数据仓库，因此更容易建立与维护。表2-19总结了数据仓库与数据集市的主要区别。</p><img src="/2017/12/数据集市/./20190529145216578.png" alt><h4 id="数据集市设计"><a href="#数据集市设计" class="headerlink" title="数据集市设计"></a>数据集市设计</h4><p>数据集市主要用于部门级别的分析型应用，数据大都是经过了汇总和聚合操作，粒度级别较高。数据集市一般采用维度模型设计方法，数据结构使用星型模式或雪花模式。 正如前面所介绍的，设计维度模型先要确定维度表、事实表和数据粒度级别，下一步是使用主外键定义事实表和维度表之间的关系。数据集市中的主键最好使用系统生成的自增的单列数字型代理键。模型建立好之后，设计ETL步骤抽取操作型源系统的数据，经过数据清洗和转换，最终装载进数据集市中的维度表和事实表中。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[Book]Hadoop构建数据仓库实践, 第2章第4节 —— 数据集市</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 雪花模式</title>
      <link href="/2017/12/%E9%9B%AA%E8%8A%B1%E6%A8%A1%E5%BC%8F.html"/>
      <url>/2017/12/%E9%9B%AA%E8%8A%B1%E6%A8%A1%E5%BC%8F.html</url>
      
        <content type="html"><![CDATA[<h3 id="雪花模式"><a href="#雪花模式" class="headerlink" title="雪花模式"></a>雪花模式</h3><p>雪花模式是一种多维模型中表的逻辑布局，其实体关系图有类似于雪花的形状，因此得名。与星型模式相同，雪花模式也是<strong>由事实表和维度表所组成</strong>。所谓 的“雪花化”就是将星型模式中的维度表进行规范化处理。当所有的维度表完成规范化后，就形成了以事实表为中心的雪花型结构，即雪花模式。<strong>将维度表进行规范化的具体做法是，把低基数的属性从维度表中移除并形成单独的表</strong>。基数指的是一个字段中不同值的个数，如主键列具有唯一值，所以有最高的基数，而像性别这样的列基数就很低。</p><p>在雪花模式中，一个维度被规范化成多个关联的表，而在星型模式中，每个维度由一个单一的维度表所表示。一个规范化的维度对应一组具有层次关系的维度表，而事实表作为雪花模式里的子表，存在具有层次关系的多个父表。</p><p>星型模式和雪花模式都是建立维度数据仓库或数据集市的常用方式，适用于加快查询速度比高效维护数据的重要性更高的场景。<strong>这些模式中的表没有特别的规范化， 一般都被设计成一个低于第三范式的级别</strong>。</p><h4 id="1-数据规范化与存储"><a href="#1-数据规范化与存储" class="headerlink" title="1. 数据规范化与存储"></a>1. 数据规范化与存储</h4><p>规范化的过程就是将维度表中重复的组分离成一个新表，以减少数据冗余的过程。正因为如此，规范化不可避免地增加了表的数量。在执行查询的时候，不得不连接更多的表。但是规范化减少了存储数据的空间需求，而且提高了数据更新的效率。</p><p>从存储空间的角度看，典型的情况是维度表比事实表小很多。这就使得雪花化的维度表相对于星型模式来说，在存储空间上的优势没那么明显了。举例来说，假设在 220个区县的200个商场，共有100万条销售记录。星型模式的设计会产生1,000,200条记录，其中事实表1,000,000条记录，商场维度表有200条记录，每个区县信息作为商场 的一个属性，显式地出现在商场维度表中。在规范化的雪花模式中，会建立一个区县维度表，该表有220条记录，商场表引用区县表的主键，有200条记录，事实表没有变 化，还是1,000,000条记录，总的记录数是1,000,420(1,000,000+200+220)。在这种特殊情况(作为子表的商场记录数少于作为父表的区县记录数)下，星型模式所需的空 间反而比雪花模式要少。如果商场有10,000个，情况就不一样了，星型模式的记录数是1,010,000，雪花模式的记录数是1,010,220，从记录数上看，还是雪花模型多。但 是，星型模式的商场表中会有10,000个冗余的区县属性信息，而在雪花模式中，商场表中只有10,000个区县的主键，而需要存储的区县属性信息只有220个，当区县的属性 很多时，会大大减少数据存储占用的空间。有些数据库开发者采取一种折中的方式，底层使用雪花模型，上层用表连接建立视图模拟星型模式。这种方法既通过对维度的规范化节省了存储空间，同时又对用户屏蔽了查询的复杂性。但是当外部的查询条件不需要连接整个维度表时，这种方法会带来性能损失。</p><h4 id="2-优点"><a href="#2-优点" class="headerlink" title="2. 优点"></a>2. 优点</h4><p>雪花模式是和星型模式类似的逻辑模型。实际上，星型模式是雪花模式的一个特例(维度没有多个层级)。<strong>某些条件下，雪花模式更具优势: 一些OLAP多维数据库建模工具专为雪花模型进行了优化。 规范化的维度属性节省存储空间。</strong></p><h4 id="3-缺点"><a href="#3-缺点" class="headerlink" title="3. 缺点"></a>3. 缺点</h4><p>雪花模型的主要缺点是维度属性规范化增加了查询的连接操作和复杂度。相对于平面化的单表维度，多表连接的查询性能会有所下降。但雪花模型的查询性能问题近年来随着数据浏览工具的不断优化而得到缓解。</p><p>和具有更高规范化级别的事务型模式相比，雪花模式并不确保数据完整性。<strong>向雪花模式的表中装载数据时，一定要有严格的控制和管理，避免数据的异常插入或更 新。</strong></p><h4 id="4-示例"><a href="#4-示例" class="headerlink" title="4.示例"></a>4.示例</h4><p>图2-4显示的是将图2-3的星型模式规范化后的雪花模式。日期维度分解成季度、月、周、日期四个表。产品维度分解成产品分类、产品两个表。由商场维度分解出一个地区表。</p><img src="/2017/12/雪花模式/./2019052914190436.png" alt>下面所示的查询语句的结果等价于前面星型模式的查询，可以明显看到此查询比星型模式的查询有更多的表连接<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> g.city,<span class="keyword">sum</span> (f.units_sold)</span><br><span class="line"><span class="keyword">from</span> fact_sales f</span><br><span class="line"><span class="keyword">inner</span> joindim _date d <span class="keyword">on</span> f.date_id = d.id</span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> dim_store s <span class="keyword">on</span> f.store_id = s.id</span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> dim_geography g <span class="keyword">on</span> s.geography_id = g.id</span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> dim_product p <span class="keyword">on</span> f.product_id = p.id</span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> dim_product_category c <span class="keyword">on</span> p.product_category_id = c.id</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">d.year = <span class="number">2015</span> <span class="keyword">and</span> c.product_category = <span class="string">'mobile'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> g.city;</span><br></pre></td></tr></table></figure><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[Book]Hadoop构建数据仓库实践, 第2章第2节 —— 维度数据模型</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 星型模式</title>
      <link href="/2017/12/%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F.html"/>
      <url>/2017/12/%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F.html</url>
      
        <content type="html"><![CDATA[<h2 id="星型模式"><a href="#星型模式" class="headerlink" title="星型模式"></a>星型模式</h2><p>星型模式是维度模型最简单的形式，也是数据仓库以及数据集市开发中使用最广泛的形式。星型模式由事实表和维度表组成，一个星型模式中可以有一个或多个事实表，每个事实表引用任意数量的维度表。星型模式的物理模型像一颗星星的形状，中心是一个事实表，围绕在事实表周围的维度表表示星星的放射状分支，这就是星型模式这个名字的由来。</p><p>星型模式将业务流程分为事实和维度。事实包含业务的度量，是定量的数据，如销售价格、销售数量、距离、速度、重量等是事实。维度是对事实数据属性的描述， 如日期、产品、客户、地理位置等是维度。<strong>一个含有很多维度表的星型模式有时被称为蜈蚣模式</strong>，显然这个名字也是因其形状而得来的。蜈蚣模式的维度表往往只有很少的几个属性，这样可以简化对维度表的维护，但查询数据时会有更多的表连接，严重时会使模型难于使用，<strong>因此在设计中应该尽量避免蜈蚣模式</strong>。</p><h3 id="1-事实表"><a href="#1-事实表" class="headerlink" title="1. 事实表"></a>1. 事实表</h3><p>事实表记录了特定事件的<strong>数字化</strong>的考量，<strong>一般由数字值和指向维度表的外键组成</strong>。通常会把事实表的<strong>粒度级别设计得比较低</strong>，使得事实表可以记录很原始的操作型事件，但这样做的负面影响是累加大量记录可能会更耗时。事实表有以下三种类型:</p><ul><li>事务事实表。记录特定事件的事实，如销售。</li><li>快照事实表。记录给定时间点的事实，如月底账户余额。 </li><li>累积事实表。记录给定时间点的聚合事实，如当月的总的销售金额。一般需要给事实表设计一个代理键作为每行记录的唯一标识。代理键是由系统生成的主键，它不是应用数据，没有业务含义，对用户来说是透明的。<h3 id="2-维度表"><a href="#2-维度表" class="headerlink" title="2. 维度表"></a>2. 维度表</h3>维度表的记录数通常比事实表少，但每条记录包含有大量用于描述事实数据的属性字段。维度表可以定义各种各样的特性，以下是几种最长用的维度表:</li><li>时间维度表。描述星型模式中记录的事件所发生的时间，具有所需的最低级别的时间粒度。数据仓库是随时间变化的数据集合，需要记录数据的历史，因此每个数 据仓库都需要一个时间维度表。</li><li>地理维度表。描述位置信息的数据，如国家、省份、城市、区县、邮编等。</li><li>产品维度表。描述产品及其属性。</li><li>人员维度表。描述人员相关的信息，如销售人员、市场人员、开发人员等。 范围维度表。描述分段数据的信息，如高级、中级、低级等。</li></ul><p><strong>通常给维度表设计一个单列、整型数字类型的代理键，映射业务数据中的主键。</strong>业务系统中的主键本身可能是自然键，也可能是代理键。自然键指的是由现实世界中已经存在的属性组成的键，如身份证号就是典型的自然键。</p><h3 id="3-优点"><a href="#3-优点" class="headerlink" title="3. 优点"></a>3. 优点</h3><p>星型模式<strong>是非规范化的</strong>，在星型模式的设计开发过程中，不受应用于事务型关系数据库的范式规则的约束。星型模式的优点如下:</p><ul><li>简化查询。查询数据时，星型模式的连接逻辑比较简单，而从高度规范化的事务模型查询数据时，往往需要更多的表连接。 </li><li>简化业务报表逻辑。与高度规范化的模式相比，由于查询更简单，因此星型模式简化了普通的业务报表(如每月报表)逻辑。 </li><li>获得查询性能。星型模式可以提升只读报表类应用的性能。快速聚合。基于星型模式的简单查询能够提高聚合操作的性能。 便于向立方体提供数据。星型模式被广泛用于高效地建立OLAP立方体，几乎所有的OLAP系统都提供ROLAP模型(关系型OLAP)，它可以直接将星型模式中的数 据当作数据源，而不用单独建立立方体结构。</li></ul><h3 id="4-缺点"><a href="#4-缺点" class="headerlink" title="4. 缺点"></a>4. 缺点</h3><p>星型模式的主要缺点是不能保证数据完整性。一次性地插入或更新操作可能会造成数据异常，而这种情况在规范化模型中是可以避免的。星型模式的数据装载， 一般都是以高度受控的方式，用批处理或准实时过程执行的，以此来抵消数据保护方面的不足。</p><p>星型模式的另一个缺点是对于分析需求来说不够灵活。它更偏重于为特定目的建造数据视图，因此实际上很难进行全面的数据分析。<strong>星型模式不能自然地支持业务实体的多对多关系，需要在维度表和事实表之间建立额外的桥接表</strong>。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[Book]Hadoop构建数据仓库实践, 第2章第2节 —— 维度数据模型</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 维度数据模型</title>
      <link href="/2017/12/%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B.html"/>
      <url>/2017/12/%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B.html</url>
      
        <content type="html"><![CDATA[<p>维度数据模型简称<strong>维度模型(Dimensional modeling, DM)</strong>，是一套技术和概念的集合，用于数据仓库设计。不同于关系数据模型，维度模型不一定要引入关系数据库。 在逻辑上相同的维度模型，可以被用于多种物理形式，比如维度数据库或是简单的平面文件。根据数据仓库大师Kimball的观点，<strong>维度模型是一种趋向于支持最终用户对数据仓库进行查询的设计技术，是围绕性能和易理解性构建的</strong>。尽管关系模型对于事务处理系统表现非常出色，但它并不是面向最终用户的。</p><p><strong>事实</strong>和<strong>维度</strong>是两个维度模型中的核心概念。<strong>事实表示对业务数据的度量，而维度是观察数据的角度。事实通常是数字类型的，可以进行聚合和计算，而维度通常是一 组层次关系或描述信息，用来定义事实。</strong> 例如，销售金额是一个事实，而销售时间、销售的产品、购买的顾客、商店等都是销售事实的维度。维度模型按照业务流程领域即<strong>主题域</strong>建立，例如进货、销售、库存、配送等。不同的主题域可能共享某些维度，为了提高数据操作的性能和数据一致性，需要使用一致性维度，例如几个主题域间共享维度的复制。术语“一致性维度”源自Kimball，指的是具有相同属性和内容的维度。</p><h2 id="维度数据模型建模过程"><a href="#维度数据模型建模过程" class="headerlink" title="维度数据模型建模过程"></a>维度数据模型建模过程</h2><p>维度模型通常以一种被称为<strong>星型模式</strong>的方式构建。所谓星型模式，就是以一个事实表为中心，周围环绕着多个维度表。还有一种模式叫做<strong>雪花模式</strong>，是对维度做进一步规范化后形成的。本节后面会讨论这两种模式。一般使用下面的过程构建维度模型:</p><ol><li>选择业务流程</li><li>声明粒度</li><li>确认维度<ol start="4"><li>确认事实</li></ol></li></ol><p>这种使用四步设计法建立维度模型的过程，有助于保证维度模型和数据仓库的可用性。</p><h3 id="1-选择业务流程"><a href="#1-选择业务流程" class="headerlink" title="1. 选择业务流程"></a>1. 选择业务流程</h3><p>确认哪些业务处理流程是数据仓库应该覆盖的，是维度方法的基础。因此，建模的第一个步骤是描述需要建模的业务流程。例如，需要了解和分析一个零售店的销售 情况，那么与该零售店销售相关的所有业务流程都是需要关注的。为了描述业务流程，可以简单地使用纯文本将相关内容记录下来，或者使用“业务流程建模标 注”(BPMN)方法，也可以使用统一建模语言(UML)或其他类似的方法。</p><h3 id="2-声明粒度"><a href="#2-声明粒度" class="headerlink" title="2. 声明粒度"></a>2. 声明粒度</h3><p>确定了业务流程后，下一步是声明维度模型的粒度。这里的粒度用于确定事实中表示的是什么，例如，一个零售店的顾客在购物小票上的一个购买条目。在选择维度 和事实前必须声明粒度，因为每个候选维度或事实必须与定义的粒度保持一致。在一个事实所对应的所有维度设计中强制实行粒度一致性是保证数据仓库应用性能和易用 性的关键。从给定的业务流程获取数据时，原始粒度是最低级别的粒度。建议从原始粒度数据开始设计，因为原始记录能够满足无法预期的用户查询。汇总后的数据粒度 对优化查询性能很重要，但这样的粒度往往不能满足对细节数据的查询需求。不同的事实可以有不同的粒度，但同一事实中不要混用多种不同的粒度。维度模型建立完成 之后，还有可能因为获取了新的信息，而回到这步修改粒度级别。</p><h3 id="3-确认维度"><a href="#3-确认维度" class="headerlink" title="3. 确认维度"></a>3. 确认维度</h3><p>设计过程的第三步是确认模型的维度。维度的粒度必须和第二步所声明的粒度一致。维度表是事实表的基础，也说明了事实表的数据是从哪里采集来的。典型的维度都是名词，如日期、商店、库存等。维度表存储了某一维度的所有相关数据，例如，日期维度应该包括年、季度、月、周、日等数据。 </p><h3 id="4-确认事实"><a href="#4-确认事实" class="headerlink" title="4. 确认事实"></a>4. 确认事实</h3><p>确认维度后，下一步也是维度模型四步设计法的最后一步，就是确认事实。这一步识别数字化的度量，构成事实表的记录。它是和系统的业务用户密切相关的，因为 用户正是通过对事实表的访问获取数据仓库存储的数据。大部分事实表的度量都是数字类型的，可累加，可计算，如成本、数量、金额等。</p><h2 id="维度规范化"><a href="#维度规范化" class="headerlink" title="维度规范化"></a>维度规范化</h2><p>与关系模型类似，维度也可以进行规范化。对维度的规范化(又叫雪花化)，可以去除冗余属性，是对非规范化维度做的规范化处理，在下面介绍雪花模型时，会看 到维度规范化的例子。一个非规范化维度对应一个维度表，规范化后，一个维度会对应多个维度表，维度被严格地以子维度的形式连接在一起。实际上，在很多情况下， 维度规范化后的结构等同于一个低范式级别的关系型结构。</p><p>  设计维度数据模型时，会因为如下原因而不对维度做规范化处理:</p><ul><li>规范化会增加表的数量，使结构更复杂。</li><li>不可避免的多表连接，使查询更复杂。</li><li>不适合使用位图索引。 </li><li>查询性能原因。分析型查询需要聚合计算或检索很多维度值，此时第三范式的数据库会遭遇性能问题。如果需要的仅仅是操作型报表，可以使用第三范式，因为操 作型系统的用户需要看到更细节的数据。</li></ul><p>正如在前面关系模型中提到的，对于是否应该规范化的问题存在一些争论。总体来说，当多个维度共用某些通用的属性时，做规范化会是有益的。例如，客户和供应 商都有省、市、区县、街道等地理位置的属性，此时分离出一个地区属性就比较合适。</p><h2 id="维度数据模型的特点"><a href="#维度数据模型的特点" class="headerlink" title="维度数据模型的特点"></a>维度数据模型的特点</h2><ul><li>易理解。相对于规范化的关系模型，维度模型容易理解且更直观。在维度模型中，信息按业务种类或维度进行分组，这会提高信息的可读性，也方便了对于数据含义的解释。简化的模型也让系统以更为高效的方式访问数据库。关系模型中，数据被分布到多个离散的实体中，对于一个简单的业务流程，可能需要很多表联合在一起才能表示。<ul><li>高性能。<strong>维度模型更倾向于非规范化，因为这样可以优化查询的性能。</strong> 介绍关系模型时多次提到，规范化的实质是减少数据冗余，以优化事务处理或数据更新的性能。维度设计的整体观点是要简化和加速查询。</li><li>可扩展。维度模型是可扩展的。由于<strong>维度模型允许数据冗余</strong>，因此当向一个维度表或事实表中添加字段时，不会像关系模型那样产生巨大的影响，带来的结果就是更容易容纳不可预料的新增数据。这种新增可以是单纯地向表中增加新的数据行而不改变表结构，也可以是在现有表上增加新的属性。基于数据仓库的查询和应用不需 要过多改变就能适应表结构的变化，老的查询和应用会继续工作而不会产生错误的结果。但是对于规范化的关系模型，由于表之间存在复杂的依赖关系，改变表结构前一 定要仔细考虑。</li></ul></li></ul><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>[Book]Hadoop构建数据仓库实践, 第2章第2节 —— 维度数据模型</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 数据仓库简介小结</title>
      <link href="/2017/12/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%AE%80%E4%BB%8B%E5%B0%8F%E7%BB%93.html"/>
      <url>/2017/12/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%AE%80%E4%BB%8B%E5%B0%8F%E7%BB%93.html</url>
      
        <content type="html"><![CDATA[<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>数据仓库是一个<strong>面向主题的、集成的、随时间变化的、非易失的</strong>数据集合，用于支持管理者的决策过程。 </li><li>数据仓库中的粒度是指数据的细节或汇总程度，细节程度越高，粒度级别越低。</li><li>数据仓库的数据来自各个业务应用系统。 </li><li>很多因素导致直接访问业务系统无法进行全局数据分析的工作，这也是需要一个数据仓库的原因所在。 </li><li>操作型系统是一类专门用于<strong>管理面向事务</strong>的应用信息系统，而分析型系统是一种<strong>快速回答多维分析查询</strong>的实现方式，两者在很多方面存在差异。</li><li>构成数据仓库系统的主要组成部分有数据源、<strong>ODS</strong>、中心数据仓库、分析查询引擎、ETL、元数据管理和自动化调度。 </li><li>主要的数据仓库架构有独立数据集市、从属数据集市、Inmon企业信息工厂、Kimball多维数据仓库、混合型数据仓库。 </li><li>ETL是建立数据仓库最重要的处理过程，也是最体现工作量的环节。</li><li>Kettle是常用的开源ETL工具，现在还有Talend等比较常用的ETL工具。</li><li>数据仓库的基本需求是安全性、可访问性、自动化，对数据的要求是准确性、时效性、历史可追溯性。</li></ol><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[Book]Hadoop构建数据仓库实践, 第1章第6节 —— 小结</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 数据仓库需求</title>
      <link href="/2017/12/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E9%9C%80%E6%B1%82.html"/>
      <url>/2017/12/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E9%9C%80%E6%B1%82.html</url>
      
        <content type="html"><![CDATA[<h3 id="基本需求"><a href="#基本需求" class="headerlink" title="基本需求"></a>基本需求</h3><p>数据仓库的目的就是能够让用户方便地访问大量数据，允许用户查询和分析其中的业务信息。这就要求数据仓库必须是安全的、可访问的和自动化的。</p><h4 id="1-安全性"><a href="#1-安全性" class="headerlink" title="1. 安全性"></a>1. 安全性</h4><p>数据仓库中含有机密和敏感的数据。为了能够使用这些数据，必须有适当的授权机制。这意味着只有被授权的用户才能访问数据，这些用户在享有特权的同时，也有责任保证数据的安全。</p><p>增加安全特性会影响到数据仓库的性能，因此必须提早考虑数据仓库的安全需求。当数据仓库已经建立完成并开始使用后，此时再应用安全特性会比较困难。在数据 仓库的设计阶段，我们就应该进行如下的安全性考虑:</p><ul><li>数据仓库中的数据对于最终用户是只读的，任何人都不能修改其中的数据，这是由数据的非易失性所决定的。 </li><li>划分数据的安全等级，如公开的、机密、秘密、绝密等。 </li><li>制定访问控制方案，决定哪些用户可以访问哪些数据。</li><li>设计授予、回收、变更用户访问权限的方法。</li><li><strong>添加对数据访问的审计功能</strong>。 </li></ul><h4 id="2-可访问性"><a href="#2-可访问性" class="headerlink" title="2. 可访问性"></a>2. 可访问性</h4><p>能够快速准确地分析所需要的数据是辅助决策支持的关键。有了数据的支持，业务就可以根据市场和客户的情况做出及时地调整。这就要求用户能够有效地查找、理 解和使用数据。数据应该是随时可访问的。</p><p>数据的可访问性是一个IT技术的通用特性。这里数据可访问性指的是用户访问和检索数据的能力。数据仓库的最终用户通常是业务人员、管理人员或者数据分析师。 他们对组织内的相关业务非常熟悉，对数据的理解也很透彻，但是他们大都不是IT技术专家。这就要求我们在设计数据仓库的时候，将用户接口设计得尽量友好和简单， 使得没有技术背景的用户同样可以轻易查询到他们需要的数据。</p><h4 id="3-自动化"><a href="#3-自动化" class="headerlink" title="3. 自动化"></a>3. 自动化</h4><p>这里的自动化有狭义和广义两个层面的理解。狭义的自动化指的是数据仓库相关作业的自动执行。比如ETL过程、报表生成、数据传输等处理，都可以<strong>周期性定时自动完成</strong>。广义的数据仓库自动化指的是在保证数据质量和数据一致性的前提下，加速数据仓库系统开发周期的过程。整个数据仓库生命周期的自动化，从对源系统分析到 ETL，再到数据仓库的建立、测试和文档化，可以帮助加快产品化进程，降低开发和管理成本，提高数据质量。</p><h3 id="数据需求"><a href="#数据需求" class="headerlink" title="数据需求"></a>数据需求</h3><p>通过数据仓库，<strong>既可以周期性地回答已知的问题(如报表等)，也可以进行即席查询(ad-hoc queries)。</strong> </p><ul><li>报表最基本的需求就是对预定义好的一系列查询条件、查询 内容，排序条件等进行组合，查询数据，把结果用表格或图形的形式展现出来。</li><li>而所谓的即席查询不是预定义好的，而是在执行时才确定的。换句话说，即席查询是指那 些用户在使用系统时，根据自己当时的需求定义的查询。数据库管理员使用命令行或客户端软件，连接数据库系统执行各种各样的查询语句，是最为常见的一种即席查询方式。而理想的数据仓库系统，允许业务或分析人员也可以通过系统执行这样的自定义查询。为了满足需求，数据仓库中的数据需要确保准确性、时效性和历史可追溯 性。<h4 id="1-准确性"><a href="#1-准确性" class="headerlink" title="1. 准确性"></a>1. 准确性</h4>想要数据仓库实施成功，业务用户必须信任其中的数据。这就意味着他们应该能知道数据从哪来，何时抽取，怎么转换的。更重要的是，他们需要访问原始数据来确定如何解决数据差异问题。实际上ETL过程应该总是在数据仓库的某个地方(如ODS)保留一份原始数据的复制。<h4 id="2-时效性"><a href="#2-时效性" class="headerlink" title="2. 时效性"></a>2. 时效性</h4>用户的时效性要求差异很大。有些用户需要数据精确到毫秒级，而有些用户只需要几分钟、几小时甚至几天前的数据就可以了。数据仓库是分析型系统，用于决策支持，所以实践中一般不需要很强的实时性，以一天作为时间粒度是比较常见的。<h4 id="3-历史可追溯性"><a href="#3-历史可追溯性" class="headerlink" title="3.历史可追溯性"></a>3.历史可追溯性</h4>数据仓库更多的价值体现在它能够辅助随时间变化的趋势分析，并帮助理解业务事件(如特殊节日促销等)与经营绩效之间的关系。</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[Book]Hadoop构建数据仓库实践, 第1章第5节 —— 数据仓库需求</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 数据仓库架构</title>
      <link href="/2017/12/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84.html"/>
      <url>/2017/12/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84.html</url>
      
        <content type="html"><![CDATA[<h4 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h4><p>“架构”是什么?这个问题从来就没有一个准确的答案。在软件行业，一种被普遍接受的架构定义是指系统的一个或多个结构。结构中包括软件的构建(构建是指软件的设计与实现)，构建的外部可以看到属性以及它们之间的相互关系。这里参考此定义，把数据仓库架构理解成构成数据仓库的组件及其之间的关系，那么就有了如图1-1 所示的数据仓库架构图。</p><img src="/2017/12/数据仓库架构/./20190527100810521.png" alt><p>图中显示的整个数据仓库环境包括操作型系统和数据仓库系统两大部分。操作型系统的数据由各种形式的业务数据组成，这其中可能有关系数据库、TXT或CSV文 件、HTML或XML文档，还可能存在外部系统的数据，比如网络爬虫抓取来的互联网数据等，==数据可能是结构化、半结构化、非结构化的==。这些数据经过==抽取、转换和装载(ETL)过程==进入数据仓库系统。</p><p>这里把ETL过程分成了抽取和转换装载两个部分。==<strong>抽取过程负责从操作型系统获取数据</strong>，<strong>该过程一般不做数据聚合和汇总</strong>==，但是会按照主题进行集成，物理上是将操作型系统的数据<strong>全量</strong>或<strong>增量</strong>复制到数据仓库系统的<strong>RDS</strong>中。==转换装载过程并将数据进行清洗、过滤、汇总、统一格式化等一系列转换操作，使数据转为适合查询的格式， 然后装载进数据仓库系统的<strong>TDS</strong>中==。传统数据仓库的基本模式是用一些过程将操作型系统的数据抽取到文件，然后另一些过程将这些文件转化成MySQL或Oracle这样的关系数据库的记录。最后，第三部分过程负责把数据导入进数据仓库。</p><ul><li>RDS(RAW DATA STORES)是原始数据存储的意思。将原始数据保存到数据仓库里是个不错的想法。ETL过程的bug或系统中的其他错误是不可避免的，保留原始数 据使得追踪并修改这些错误成为可能。有时数据仓库的用户会有查询细节数据的需求，这些细节数据的粒度与操作型系统的相同。有了RDS，这种需求就很容易实现，用 户可以查询RDS里的数据而不必影响业务系统的正常运行。这里的RDS实际上是起到了操作型数据存储(ODS)的作用，关于ODS相关内容本小节后面会有详细论述。</li><li>TDS(TRANSFORMED DATA STORES)意为转换后的数据存储。这是真正的数据仓库中的数据。大量的用户会在经过转换的数据集上处理他们的日常查询。如果前 面的工作做得好，这些数据将被以保证最重要的和最频繁的查询能够快速执行的方式构建。</li><li>这里的原始数据存储和转换后的数据存储是<strong>逻辑概念</strong>，==它们可能物理存储在一起，也可能分开==。当原始数据存储和转换后的数据存储物理上分开时，它们不必使用同样的软硬件。传统数据仓库中，原始数据存储通常是本地文件系统，原始数据被组织进相应的目录中，这些目录是基于数据从哪里抽取或何时抽取建立(例如以日期作为文件或目录名称的一部分); 转换后的数据存储一般是某种关系数据库。</li></ul><p><strong>自动化调度组件</strong>的作用是自动定期重复执行ETL过程。不同角色的数据仓库用户对数据的更新频率要求也会有所不同，财务主管需要每月的营收汇总报告，而销售人员想看到每天的产品销售数据。作为通用的需求，所有数据仓库系统都应该能够建立周期性自动执行的工作流作业。传统数据仓库一般利用操作系统自带的调度功能(如 Linux的cron或Windows的计划任务)实现作业自动执行。</p><p><strong>数据目录</strong>有时也被称为<strong>元数据存储</strong>，它可以提供一份数据仓库中数据的清单。用户通过它应该可以快速解决这些问题:</p><ul><li>什么类型的数据被存储在哪里</li><li>数据集的构建有何区别</li><li>数据最后的访问或更新时间等。</li></ul><p>此外还可以通过数据目录感知数据是如何被操作和转换的。==一个好的数据目录是让用户体验到系统易用性的关键==。查询引擎组件负责实际执行用户查询。传统数据仓库中，它可能是存储转换后数据的(Oracle、MySQL等关系数据库系统内置的)查询引擎，还可能是以固定时间间隔向其导入数据的OLAP立方体，如Essbase cube。用户界面指的是最终用户所使用的接口程序。可能是一个GUI软件，如BI套件的中的客户端软件，也可能就是一个浏览器</p><h4 id="主要数据仓库架构"><a href="#主要数据仓库架构" class="headerlink" title="主要数据仓库架构"></a>主要数据仓库架构</h4><p>在数据仓库技术演化过程中，产生了几种主要的架构方法，包括数据集市架构、Inmon企业信息工厂架构、Kimball数据仓库架构和混合型数据仓库架构。</p><h5 id="1-数据集市架构"><a href="#1-数据集市架构" class="headerlink" title="1. 数据集市架构"></a>1. 数据集市架构</h5><p>==数据集市是<strong>按主题域</strong>组织的数据集合==，用于支持部门级的决策。有两种类型的数据集市:独立数据集市和从属数据集市。</p><p>独立数据集市集中于部门所关心的单一主题域，数据以部门为基础部署，无须考虑企业级别的信息共享与集成。例如，制造部门、人力资源部门和其他部门都各自有 他们自己的数据集市。独立数据集市从一个主题域或一个部门的多个事务系统获取数据，用以支持特定部门的业务分析需要。一个独立数据集市的设计既可以使用实体关 系模型，也可以使用多维模型。数据分析或商业智能工具直接从数据集市查询数据，并将查询结果显示给用户。一个典型的独立数据集市架构如图1-2所示。</p><p>因为一个部门的业务相对于整个企业要简单，数据量也小得多，所以部门的独立数据集市具有周期短、见效快的特点。如果从企业整体的视角来观察这些数据集市， 你会看到每个部门使用不同的技术，建立不同的ETL的过程，处理不同的事务系统，而在多个独立的数据集市之间还会存在数据的交叉与重叠，甚至会有数据不一致的情 况。从业务角度看，当部门的分析需求扩展，或者需要分析跨部门或跨主题域的数据时，独立数据市场会显得力不从心。而当数据存在歧义，比如同一个产品，在A部门 和B部门的定义不同时，将无法在部门间进行信息比较。</p><img src="/2017/12/数据仓库架构/./20190527101600206.png" alt><p>另外一种数据集市是从属数据集市。如Bill Inmon所说，从属数据集市的数据来源于数据仓库。==数据仓库里的数据经过整合、重构、汇总后传递给从属数据集市==。从属数据集市的架构如图1-3所示。<img src="/2017/12/数据仓库架构/./20190527101725230.png" alt></p><p>建立从属数据集市的好处主要有:</p><ul><li><strong>性能</strong>:当数据仓库的查询性能出现问题，可以考虑建立几个从属数据集市，将查询从数据仓库移出到数据集市。 </li><li><strong>安全</strong>:每个部门可以完全控制他们自己的数据。 </li><li><strong>数据一致</strong>:因为每个数据集市的数据来源都是同一个数据仓库，有效消除了数据不一致的情况。</li></ul><p>Inmon企业信息工厂架构Inmon企业信息工厂架构如图1-4所示，我们来看图中的组件是如何协同工作的。<img src="/2017/12/数据仓库架构/./20190527102102113.png" alt></p><ul><li><strong>应用系统</strong>: 这些应用是组织中的操作型系统，用来支撑业务。它们收集业务处理过程中产生的销售、市场、材料、物流等数据，并将数据以多种形式进行存储。操作型系统也叫源系统，为数据仓库提供数据。 </li><li><strong>ETL过程</strong>: ETL过程从操作型系统抽取数据，然后将数据转换成一种标准形式，最终将转换后的数据装载到企业级数据仓库中。ETL是周期性运行的批处理过程。 </li><li><strong>企业级数据仓库</strong>: 是该架构中的核心组件。正如Inmon数据仓库所定义的，企业级数据仓库是一个细节数据的集成资源库。其中的数据以最低粒度级别被捕获，存储在满足三范式设计的关系数据库中。 </li><li><strong>部门级数据集市:</strong> 是面向主题数据的部门级视图，数据从企业级数据仓库获取。数据在进入部门数据集市时可能进行聚合。数据集市使用多维模型设计，用于数据 分析。重要的一点是，所有的报表工具、BI工具或其他数据分析应用都从数据集市查询数据，而不是直接查询企业级数据仓库。</li></ul><h5 id="2-Kimball数据仓库架构"><a href="#2-Kimball数据仓库架构" class="headerlink" title="2. Kimball数据仓库架构"></a>2. Kimball数据仓库架构</h5><p>Kimball数据仓库架构如图1-5所示。<img src="/2017/12/数据仓库架构/./20190527101846515.png" alt></p><p>对比上一张图可以看到，==Kimball与Inmon两种架构的主要区别在于核心数据仓库的设计和建立==。Kimball的数据仓库<strong>包含高粒度</strong>的企业数据，使用多维模型设计，==这也意味着数据仓库由星型模式的维度表和事实表构成==。分析系统或报表工具可以直接访问多维数据仓库里的数据。在此架构中的数据集市也与Inmon中的不同。==这里的数据集市是一个逻辑概念，只是多维数据仓库中的主题域划分，并没有自己的物理存储，也可以说是虚拟的数据集市==。</p><h5 id="3-混合型数据仓库架构"><a href="#3-混合型数据仓库架构" class="headerlink" title="3. 混合型数据仓库架构"></a>3. 混合型数据仓库架构</h5><p>混合型数据仓库架构如图1-6所示。<img src="/2017/12/数据仓库架构/./20190527102345389.png" alt></p><p>所谓的混合型结构，指的是在一个数据仓库环境中，联合使用Inmon和Kimball两种架构。从架构图可以看到，这种架构将Inmon方法中的数据集市部分替换成了一个多维数据仓库，而数据集市则是多维数据仓库上的逻辑视图。使用这种架构的好处是，==既可以利用规范化设计消除数据冗余，保证数据的粒度足够细;又可以利用多维结构 更灵活地在企业级实现报表和分析==。</p><h4 id="操作数据存储"><a href="#操作数据存储" class="headerlink" title="操作数据存储"></a>操作数据存储</h4><p>操作数据存储又称为<strong>ODS</strong>，是Operational Data Store的简写，其定义是这样的: ==一个面向主题的、集成的、可变的、当前的细节数据集合，用于支持企业对于即时性的、操作性的、集成的全体信息的需求==。对比1.1节中数据仓库的定义不难看出，操作型数据存储在某些方面具有类似于数据仓库的特点，但在另一些方面又显著不同于数据仓库。</p><ul><li>像数据仓库一样，是面向主题的。</li><li>像数据仓库一样，其数据是完全集成的。 </li><li>数据是当前的，这与数据仓库存储历史数据的性质明显不同。ODS具有最少的历史数据(一般是30天到60天)，而尽可能接近实时地展示数据的状态。 </li><li>数据是可更新的，这是与静态数据仓库又一个很大的区别。ODS就如同一个事务处理系统，当新的数据流进ODS时，受其影响的字段被新信息覆盖。 </li><li>数据几乎完全是细节数据，仅具有少量的动态聚集或汇总数据。通常将ODS设计成包含事务级的数据，即包含该主题域中最低粒度级别的数据。 </li><li>在数据仓库中，几乎没有针对其本身的报表，报表均放到数据集市中完成;与此不同，在ODS中，业务用户频繁地直接访问ODS。</li></ul><p>在一个数据仓库环境中，ODS具有如下几个作用:</p><ul><li>==充当<strong>业务系统</strong>与<strong>数据仓库</strong>之间的<strong>过渡区</strong>==。数据仓库的数据来源复杂，可能分布在不同的数据库，不同的地理位置，不同的应用系统之中，而且由于数据形式的多样 性，数据转换的规则往往极为复杂。如果直接从业务系统抽取数据并做转换，不可避免地会对业务系统造成影响。而ODS中存放的数据从数据结构、数据粒度、数据之间的逻辑关系上都与业务系统基本保持一致，因此抽取过程只需简单的数据复制而基本不再需要做数据转换，大大降低了复杂性，同时最小化对业务系统的侵 入。</li><li>转移部分业务系统细节查询的功能。==某些原来由业务系统产生的报表、细节数据的查询能够在ODS中进行，从而降低业务系统的查询压力==。 </li><li>完成数据仓库中不能完成的一些功能。用户有时会要求数据仓库查询最低粒度级别的细节数据，而数据仓库中存储的数据一般都是聚合或汇总过的数据，并不存储每笔交易产生的细节数据。这时就需要把细节数据查询的功能转移到ODS来完成，而且ODS的数据模型是按照面向主题的方式组织的，可以方便地支持多维分析。 ==即数据仓库从宏观角度满足企业的决策支持要求，而ODS层则从微观角度反映细节交易数据或者低粒度的数据查询要求。==</li></ul><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>[Book]Hadoop构建数据仓库实践, 第1章第3节 —— 操作型系统与分析型系统</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 操作型系统与分析型系统</title>
      <link href="/2017/12/%E6%93%8D%E4%BD%9C%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B8%8E%E5%88%86%E6%9E%90%E5%9E%8B%E7%B3%BB%E7%BB%9F.html"/>
      <url>/2017/12/%E6%93%8D%E4%BD%9C%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B8%8E%E5%88%86%E6%9E%90%E5%9E%8B%E7%B3%BB%E7%BB%9F.html</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><ul><li><strong>操作型系统</strong>完成组织的核心业务，例如下订单、更新库存、记录支付信息等。这些系统是事务型的，核心目标是尽可能快地处理事务，同时维护数据的一致性和完整性。</li><li><strong>分析型系统</strong>的主要作用是通过数据分析评估组织的业务经营状况，并进一步辅助决策。</li></ul><h4 id="操作型系统"><a href="#操作型系统" class="headerlink" title="操作型系统"></a>操作型系统</h4><h5 id="操作型系统的特性"><a href="#操作型系统的特性" class="headerlink" title="操作型系统的特性"></a>操作型系统的特性</h5><p>操作型系统是一类专门用于管理<strong>面向事务</strong>的应用的信息系统。</p><p>事务是工作于数据库管理系统(或类似系统)中的一个逻辑单元，该逻辑单元中的操作被以一种独立于其他事务的可靠方式所处理。==事务一般代表着数据改变==，它提 供“<strong>all-or-nothing</strong>”操作，就是说事务中的一系列操作要么完全执行，要么完全不执行。在数据库中使用事务主要出于两个目的:</p><ol><li>保证工作单元的可靠性。当数据库系统异常宕机时，其中执行的操作或者已经完成或者只有部分完成，很多没有完成的操作此时处于一种模糊状态。在这种情况 下，数据库系统必须能够恢复到数据一致的正常状态。</li><li>提供并发访问数据库的多个程序间的隔离。如果没有这种隔离，程序得到的结果很可能是错误的。</li></ol><blockquote><p>根据事务的定义，引申出事务具有<strong>原子性、一致性、隔离性、持久性</strong>的特点，也就是数据库领域中常说的事务的<strong>ACID</strong>特性。</p></blockquote><ul><li><p><strong>原子性</strong>指的是事务中的==一系列操作<strong>或全执行或不执行</strong>==，这些操作是不可再分的。==<strong>原子性可以防止数据被部分修改</strong>==。银行账号间转账是一个事务原子性的例子。简单地说，从 A账号向B账号转账有两步操作:A账号提取，B账号存入。这两个操作以原子性事务执行，使数据库保持一致的状态，即使这两个操作的任何一步失败了，总的金额数不 会减少也不会增加。</p></li><li><p><strong>一致性</strong>数据库系统中的一致性是指任何数据库事务只能以允许的方式修改数据。任何数据库写操作必须遵循既有的规则，包括约束、级联、触发器以及它们的任意组合。一致性并不保证应用程序逻辑的正确性，但它能够保证不会因为程序错误而使数据库产生违反规则的结果</p></li><li><p><strong>隔离性</strong>在数据库系统中，隔离性决定了其他用户所能看到的事务完整性程度。例如，一个用户正在生成一个采购订单，并且已经生成了订单主记录，但还没有生成订单条目 明细记录。此时订单主记录能否被其他并发用户看到呢? 这就是由隔离级别决定的。数据库系统中，按照由低到高一般有<strong>读非提交、读提交、可重复读、串行化</strong>等几种隔 离级。数据库系统并不一定实现所有的隔离级别，如Oracle数据库只实现了读提交和串行化，而MySQL数据库则提供这全部四种隔离级别。</p><blockquote><p>隔离级越低，多用户同时访问数据的能力越高，但同时也会增加<strong>脏读、丢失更新等并发</strong>操作的负面影响。相反，高隔离级降低了并发影响，但需要使用更多的系统资源，也增加了事务被阻塞的可能性。</p></blockquote></li><li><p><strong>持久性</strong>数据库系统的持久性保证已经提交的事务是永久保存的。例如，如果一个机票预订报告显示一个座位已经订出，那么即使系统崩溃，被订了的座位也会一直保持被订出的状态。持久性可以通过在事务提交时将事务日志刷新至永久性存储介质来实现。</p></li></ul><p><strong>操作型系统</strong>通常是<strong>高并发、高吞吐量</strong>的系统，具有<strong>大量检索、插入、更新操作</strong>，事务数量大， 但每个事务影响的数据量相对较小。这样的系统很适合在线应用，这些应用有成千上万用户在同时使用，并要求能够立即响应用户请求。操作型系统常被整合到面向服务的架构(SOA)和Web服务里。对操作型系统应用的主要要求是高可用、高速度、高并发、可恢复和保证数据一致性，在各种互联网应用层出不穷的今天，这些系统要求 是显而易见的。</p><h5 id="1-操作型系统的数据库操作"><a href="#1-操作型系统的数据库操作" class="headerlink" title="1. 操作型系统的数据库操作"></a>1. 操作型系统的数据库操作</h5><p>在数据库使用上，操作型系统常用的操作是增、改、查，并且通常是插入与更新密集型的，同时会对数据库进行大量并发查询，而删除操作相对较少。==操作型系统一般都直接在数据库上修改数据，没有中间过渡区==。 </p><h5 id="2-操作型系统的数据库设计"><a href="#2-操作型系统的数据库设计" class="headerlink" title="2.操作型系统的数据库设计"></a>2.操作型系统的数据库设计</h5><p>==<strong>操作型系统的特征是大量短的事务，并强调快速处理查询</strong>==。每秒事务数是操作型系统的一个有效度量指标。针对以上这些特点，数据库设计一定要满足系统的要求。</p><p> 在数据库逻辑设计上，操作型系统的应用数据库大都使用规范化设计方法，<strong>通常要满足第三范式</strong>。这是因为规范化设计能最大限度地数据冗余，因而提供更快更高效的方式执行数据库写操作。</p><p>在数据库物理设计上，应该依据系统所使用的数据库管理系统的具体特点，做出相应的设计，毕竟每种数据库管理系统在实现细节上还是存在很大差异的。下面就以Oracle数据库为例，简要说明在设计操作型系统数据库时应该考虑的问题。</p><ul><li>调整回滚段。回滚段是数据库的一部分，其中记录着最终被回滚的事务的行为。这些回滚段信息可以提供读一致性、回滚事务和数据库恢复。 </li><li>合理使用聚簇。聚簇是一种数据库模式，其中包含有共用一列或多列的多个表。数据库中的聚簇表用于提高连接操作的性能。 </li><li>适当调整数据块大小。数据块大小应该是操作系统块大小的倍数，并且设置上限以避免不必要的I/O。 </li><li>设置缓冲区高速缓存大小。合理的缓存大小能够有效避免不必要的磁盘I/O。</li><li>动态分配表空间。 </li><li>合理划分数据库分区。分区最大的作用是能在可用性和安全性维护期间保持事务处理的性能。 </li><li>SQL优化。有效利用数据库管理系统的优化器，使用最佳的数据访问路径。 </li><li>避免过度使用索引。大量的数据修改会给索引维护带来压力，从而对整个系统的性能产生负面影响。</li></ul><p>==以上所讲的操作型系统都是以数据库系统为核心，而数据库系统为了保持<strong>ACID</strong>特性，本质上是单一集中式系统==。在当今这个信息爆炸的时代，集中式数据库往往已无法支撑业务的需要(从某订票网站和某电商网站的超大瞬时并发量来看，这已是一个不争的事实)。这就给操作型系统带来新的挑战。<strong>分布式事务、去中心化、CAP与最终一致性</strong>等一系列新的理论和技术为解决系统扩展问题应运而生。</p><h4 id="分析型系统"><a href="#分析型系统" class="headerlink" title="分析型系统"></a>分析型系统</h4><p>在计算机领域，分析型系统是 ==<strong>一种快速回答多维分析查询的实现方式</strong>==。它也是更广泛范畴的所谓<strong>商业智能</strong>的一部分(商业智能还包含数据库、报表系统、数据挖掘、数据可视化等研究方向)。分析型系统的典型应用包括销售业务分析报告、市场管理报告、业务过程管理(BPM)、预算和预测、金融分析报告及其类似的应用。</p><h5 id="1-分析型系统的数据库操作"><a href="#1-分析型系统的数据库操作" class="headerlink" title="1.分析型系统的数据库操作"></a>1.分析型系统的数据库操作</h5><p>在数据库层面，分析型系统操作被定义成==少量的事务，复杂的查询，处理归档和历史数据==。这些数据很少被修改，从数据库抽取数据是最多的操作，也是识别这种系 统的关键特征。分析型数据库基本上都是读操作。</p><h5 id="2-分析型系统的数据库设计"><a href="#2-分析型系统的数据库设计" class="headerlink" title="2.分析型系统的数据库设计"></a>2.分析型系统的数据库设计</h5><p>分析型系统的特征是==相对少量的事务，但查询通常<strong>非常复杂</strong>并且会包含聚合计算==，例如今年和去年同时期的数据对比、百分比变化趋势等。分析型数据库中的数据一般来自于一个<strong>企业级数据仓库</strong>，是整合过的历史数据。对于分析型系统，吞吐量是一个有效的性能度量指标。</p><p>在数据库逻辑设计上，分析型数据库使用<strong>多维数据模型</strong>，==通常是设计成星型模式或雪花模式==。</p><p>在数据库物理设计上，依然以Oracle数据库为例，简要说明在设计分析型系统数据库时应该考虑的一些问题。</p><ul><li>表分区。可以独立定义表分区的物理存储属性，将不同分区的数据存放到多个物理文件上，这样做一方面可以分散I/O;另一方面，当数据量非常大时，方便数据 维护;再有就是利用分区消除查询数据时，不用扫描整张表，从而提高查询性能。 </li><li>位图索引。当查询条件中包含低基数(不同值很少，例如性别)的列，尤其是包含有这些列上的or、and或not这样的逻辑运算时，或者从有大量行的表中返回大量 的行时，应考虑位图索引。 </li><li>物化视图。物化视图物理存储查询所定义的数据，能够自动增量刷新数据，并且可以利用查询重写特性极大地提高查询速度，是分析型系统常用的技术。 </li><li>并行化操作。可以在查询大量数据时执行并行化操作，这样会导致多个服务器进程为同一个查询语句工作，使用该查询可以快速完成，但是会耗费更多的资源。</li></ul><h4 id="操作型系统和分析型系统对比"><a href="#操作型系统和分析型系统对比" class="headerlink" title="操作型系统和分析型系统对比"></a>操作型系统和分析型系统对比</h4><img src="/2017/12/操作型系统与分析型系统/./20190526224455389.png" alt><ul><li>首先两种系统的侧重点不同。操作型系统更适合对已有数据的更新，所以是日常处理工作或在线系统的选择。相反，分析型系统提供在大量存储数据上的分析能力， 所以这类系统更适合报表类应用。分析型系统通常是查询历史数据，这有助于得到更准确的分析报告。</li><li>其次因为这两种系统的目标完全不同，所以为了得到更好的性能，使用的数据模型和设计方法也不同。操作型系统数据库通常使用规范化设计，为普通查询和数据修 改提供更好的性能。另一方面，分析型数据库具有典型的数据仓库组织形式。</li><li>基于这两个主要的不同点，我们可以推导出两种系统其他方面的区别。操作型系统上的查询更小，而分析型系统上执行的查询要复杂得多。所以操作型系统会比分析 型系统快很多。</li><li>操作型系统的数据会持续更新，并且更新会立即生效。而分析型系统的数据更新，是由预定义的处理作业同时装载大量的数据集合，并且在装载前需要做数据转换， 因此整个数据更新过程需要很长的执行时间。</li><li>由于操作型系统要做到绝对的数据安全和可用性，所以需要实施复杂的备份系统。基本的全量备份和增量备份都是必须要做的。而分析型系统只需要偶尔执行数据备 份即可，这一方面是因为这类系统一般不需要保持持续运行，另一方面数据还可以从操作型系统重复装载。两种系统的空间需求显然都依赖于它们所存储的数据量。分析型系统要存储大量的历史数据，因此需要更多的存储空间。</li></ul><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>[Book]Hadoop构建数据仓库实践, 第一章第二节 —— 操作型系统与分析型系统</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记 —— 什么是数据仓库</title>
      <link href="/2017/12/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93.html"/>
      <url>/2017/12/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93.html</url>
      
        <content type="html"><![CDATA[<h3 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h3><p>本质上，数据仓库试图提供一种从操作型系统到决策支持环境的数据流架构模型。数据仓库概念的提出，是为了解决和这个数据流相关的各种问题，主要是解决多重数据复制带来的高成本问题。</p><h4 id="数据仓库的定义"><a href="#数据仓库的定义" class="headerlink" title="数据仓库的定义"></a>数据仓库的定义</h4><p>数据仓库之父Bill Inmon在1991年出版的Building the Data Warehouse 一书中首次提出了被广为认可的数据仓库定义。Inmon将数据仓库描述为一个<strong>面向主题的、集成的、随时间变化的、非易失的数据集合</strong>，用于<strong>支持管理者的决策过程</strong>。</p><ul><li><p><strong>面向主题</strong>传统的操作型系统是围绕组织的功能性应用进行组织的，而数据仓库是面向主题的。主题是一个抽象概念，简单地说就是<strong>与业务相关的数据的类别</strong>，每一个主题基本 对应一个宏观的分析领域。数据仓库被设计成辅助人们分析数据。例如，一个公司要分析销售数据，就可以建立一个专注于销售的数据仓库，使用这个数据仓库，就可以回答类似于“去年谁是我们这款产品的最佳用户”这样的问题。这个场景下的销售，就是一个数据主题，而这种通过划分主题定义数据仓库的能力，就使得数据仓库是面向 主题的。主题域是对某个主题进行分析后确定的主题的边界，如客户、销售、产品都是主题域的例子</p></li><li><p><strong>集成</strong>集成的概念与面向主题是密切相关的。还用销售的例子，假设公司有多条产品线和多种产品销售渠道，而每个产品线都有自己独立的销售数据库。此时要想从公司层面整体分析销售数据，==必须将多个分散的数据源统一成一致的、无歧义的数据格式后，再放置到数据仓库中==。因此数据仓库必须能够解决诸如产品命名冲突、计量单位不一致等问题。当完成了这些数据整合工作后，该数据仓库就可称为是集成的。</p></li><li><p><strong>随时间变化</strong>为了发现业务变化的趋势、存在的问题，或者新的机会，需要分析大量的历史数据。这与联机事务处理(OLTP)系统形成鲜明的对比。联机事务处理反应的是当前时间点的数据情况，要求高性能、高并发和极短的响应时间，出于这样的需求考虑，联机事务处理系统中一般都将数据依照活跃程度分级，把历史数据迁移到归档数据库 中。而数据仓库关注的是数据随时间变化的情况，并且能反映在过去某个时间点的数据是怎样的。换句话说，数据仓库中的数据是 ==<strong>反映了某一历史时间点的数据快照</strong>==，这也就是术语“随时间变化”的含义。当然，任何一个存储结构都不可能无限扩展，数据也不可能只入不出地永久驻留在数据仓库中，==它在数据仓库中也有自己的生命周期。到了一定时候，数据会从数据仓库中移除。移除的方式可能是将细节数据汇总后删除、将老的数据转储到大容量介质后删除和直接物理删除等==。</p></li><li><p><strong>非易失</strong>非易失指的是，一旦进入到数据仓库中，数据就不应该再有改变。==操作型环境中的数据一般都会频繁更新，而在数据仓库环境中一般并不进行数据更新==。当改变的操作型数据进入数据仓库时会产生新的记录，这样就保留了数据变化的历史轨迹。也就是说，==数据仓库中的数据基本是静态的==。这是一个不难理解的逻辑概念。数据仓库的目的就是要根据曾经发生的事件进行分析，如果数据是可修改的，将使历史分析变得没有意义。</p></li><li><p><strong>粒度</strong>数据仓库还有一个非常重要的概念就是<strong>粒度</strong>。粒度问题遍布于数据仓库体系结构的各个部分。粒度是指数据的细节或汇总程度，细节程度越高，粒度级别越低。例如，单个事务是低粒度级别，而全部一个月事务的汇总就是高粒度级别。数据粒度一直是数据仓库设计需要重点思考的问题。在早期的操作型系统中，当细节数据被更新时，几乎总是将其存放在最低粒度级别上;而在数据仓库环境中，通常都不这样做。==例如，如果数据被装载进数据仓库的频率是每天一次，那么一天之内的数据更新将被忽略==。粒度之所以是数据仓库环境的关键设计问题，是因为它极大地影响数据仓库的数据量和可以进行的查询类型。==粒度级别越低，数据量越大，查询的细节程度越高，查 询范围越广泛，反之亦然==。==大多数情况下，数据会以很低的粒度级别进入数据仓库，如日志类型的数据或单击流数据，此时应该对数据进行编辑、过滤和汇总，使其适应数据仓库环境的粒度级别==。如果得到的数据粒度级别比数据仓库的高，那将意味着在数据存入数据仓库前，开发人员必须花费大量设计和资源来对数据进行拆分。</p></li></ul><h4 id="建立数据仓库的原因"><a href="#建立数据仓库的原因" class="headerlink" title="建立数据仓库的原因"></a>建立数据仓库的原因</h4><p>现在你应该已经熟悉了数据仓库的概念，那么数据仓库里的数据从哪里来呢?通常数据仓库的数据来自各个业务应用系统。业务系统中的数据形式多种多样，可能是:</p><ul><li>Oracle、MySQL、SQL Server等关系数据库里的<strong>结构化数据</strong>，</li><li>可能是文本、CSV等<strong>平面文件</strong></li><li>Word、Excel文档中的<strong>非结构化数据</strong>，</li><li>还可能是HTML、XML等自描述的<strong>半结构化数据</strong>。这些业务数据经过一系列的<strong>数据抽取、转换、清洗</strong>，最终以一种统一的格式装载进数据仓库。数据仓库里的数据作为分析用的数据源，提供给后面的即席查询、 分析系统、数据集市、报表系统、数据挖掘系统等。</li></ul><p>从以上描述可以看到，从存储的角度看，数据仓库里的数据实际上已经存在于业务应用系统中，那么为什么不能直接操作业务系统中的数据用于分析，而要使用数据仓库呢?实际上在数据仓库技术出现前，有很多数据分析的先驱者已经发现，简单的“直接访问”方式很难良好工作，这样做的失败案例数不胜数。</p><p>下面列举一些直接访问业务系统无法工作的原因:</p><ul><li>某些业务数据由于安全或其他因素不能直接访问。 </li><li>业务系统的版本变更很频繁，每次变更都需要重写分析系统并重新测试。 </li><li>很难建立和维护汇总数据来源于多个业务系统版本的报表。 </li><li>业务系统的列名通常是硬编码，有时仅仅是无意义的字符串，这让编写分析系统更加困难。 </li><li>业务系统的数据格式，如日期、数字的格式不统一。 </li><li>业务系统的表结构为事务处理性能而优化，有时并不适合查询与分析。 </li><li>没有适当的方式将有价值的数据合并进特定应用的数据库。</li><li>没有适当的位置存储元数据。</li><li>用户需要看到的显示数据字段，有时在数据库中并不存在。 </li><li>通常事务处理的优先级比分析系统高，所以如果分析系统和事务处理运行在同一硬件之上，分析系统往往性能很差。 </li><li>有误用业务数据的风险。</li><li>极有可能影响业务系统的性能。</li></ul><p>==在辅助战略决策层面，数据仓库的重要性更加凸显。==</p><p>下面简单总结一下使用数据仓库的好处:</p><ul><li>将多个数据源集成到单一数据存储，因此可以使用单一数据查询引擎展示数据。 </li><li>缓解在事务处理数据库上因执行大查询而产生的资源竞争问题。 </li><li>维护历史数据。 通过对多个源系统的数据整合，使得在整个企业的角度存在统一的中心视图。 </li><li>通过提供一致的编码和描述，减少或修正坏数据问题，提高数据质量。 </li><li>一致性地表示组织信息。 </li><li>提供所有数据的单一通用数据模型，而不用关心数据源。 </li><li>重构数据，使数据对业务用户更有意义。 </li><li>向复杂分析查询交付优秀的查询性能，同时不影响操作型系统。 </li><li>开发决策型查询更简单。<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3>[Book]Hadoop构建数据仓库实践, 第一章第一节 —— 什么是数据仓库</li></ul>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DWH </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
