<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>记录下本周末搭建个人博客的过程Mac+Hexo+GitHubPages</title>
      <link href="/2019/05/26/%E8%AE%B0%E5%BD%95%E4%B8%8B%E6%9C%AC%E5%91%A8%E6%9C%AB%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%9A%84%E8%BF%87%E7%A8%8BMac+Hexo+GitHubPages/"/>
      <url>/2019/05/26/%E8%AE%B0%E5%BD%95%E4%B8%8B%E6%9C%AC%E5%91%A8%E6%9C%AB%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%9A%84%E8%BF%87%E7%A8%8BMac+Hexo+GitHubPages/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前本来有一个个人博客，但是因为太懒没有维护，就来投奔CSDN了。这几天突然一时兴起，让好好弄一下自己的个人博客，因为CSDN的广告实在是....一言难尽...搜索了一般，选取一个比较简单的实现方式：即Hexo+GitHub Pages以下记录如果搭建个人博客网站 <a href="lestatzhang.com">lestatzhang.com</a>的过程</p><h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><ol><li>安装Git</li><li>安装Node.js</li><li>安装Hexo</li><li>博客初始化</li><li>将本地博客与GitHub关联</li><li>切换Hexo主题：Next</li><li>Goddady购买个人域名</li><li>绑定个人域名</li><li>其他TO-DO</li></ol><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><p>先查看是否已经安装Git <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lestat@Lestats-MBP:~$ git --version</span><br><span class="line">git version 2.14.3 (Apple Git-98)</span><br></pre></td></tr></table></figure></p><p> 如果Mac没有安装git可以通过Homebrew安装 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install git</span><br></pre></td></tr></table></figure></p><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><p> 如果Mac没有安装Node.js可以通过Homebrew安装 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install node</span><br></pre></td></tr></table></figure></p><p>中间有可能因为一些依赖库需要更新你的Xcode的Command Line Tools</p><p>我安装的版本如下：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lestat@Lestats-MBP:~$ node -v</span><br><span class="line">v12.3.1</span><br><span class="line">lestat@Lestats-MBP:~$ npm -v</span><br><span class="line">6.9.0</span><br></pre></td></tr></table></figure></p><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>Node.js和Git都安装成功后开始安装Hexo<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install -g hexo-cli</span><br></pre></td></tr></table></figure></p><h3 id="博客初始化"><a href="#博客初始化" class="headerlink" title="博客初始化"></a>博客初始化</h3><p>创建你本地的博客文件夹，比如我的就是 lestatzhang， 然后进入该文件夹，利用hexo进行初始化<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd lestatzhang;</span><br><span class="line">hexo init;</span><br></pre></td></tr></table></figure></p><p>执行下述命令安装npm。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install;</span><br></pre></td></tr></table></figure></p><p>执行hexo命令生成本地网页文件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure></p><p>执行hexo命令开启本地服务器<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure></p><p>然后我们就通过 <a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a> 查看本地博客。</p><h3 id="将本地博客与GitHub关联"><a href="#将本地博客与GitHub关联" class="headerlink" title="将本地博客与GitHub关联"></a>将本地博客与GitHub关联</h3><p>编辑站点配置文件<code>_config.yml</code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi _config.yml</span><br></pre></td></tr></table></figure></p><p>打开后到文档最后部分，配置deploy设置如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: https://github.com/lestatzhang/lestatzhang.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></p><p>然后为hexo配置git部署服务：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p><p>运行hexo命令，将在lestatzhang下生成静态文件并上传到git服务器。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure></p><p>若未关联GitHub，执行hexo d时会提示输入GitHub账号用户名和密码，即:<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">username for 'https://github.com':</span><br><span class="line">password for 'https://github.com':</span><br></pre></td></tr></table></figure></p><p><code>hexo d</code>执行成功后便可通过 <a href="https://lestatzhang.github.io" target="_blank" rel="noopener">https://lestatzhang.github.io</a> 访问博客，看到的内容和本地页面一致。</p><p>如果需要开启ssh，我们可以在Github中配置ssh keys。具体步骤可以参考<a href="https://help.github.com/en/articles/connecting-to-github-with-ssh" target="_blank" rel="noopener">Connecting to GitHub with SSH</a></p><h3 id="切换Hexo主题：Next"><a href="#切换Hexo主题：Next" class="headerlink" title="切换Hexo主题：Next"></a>切换Hexo主题：Next</h3><p>Hexo允许我们为自己的站点配置自己喜欢的主题, 在这里我选择一个个人比较喜欢的主题: <strong>hexo-theme-next</strong>。 安装过程如下：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd lestatzhang;</span><br><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure></p><p>编辑站点配置文件_config.yml，将theme的值从landscape更改为next将blog目录下_config.yml里的theme的名称landscape更改为next。</p><p>然后重新生成站点文件,并查看<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g  </span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure></p><h3 id="Godaddy购买个人域名"><a href="#Godaddy购买个人域名" class="headerlink" title="Godaddy购买个人域名"></a>Godaddy购买个人域名</h3><p>在Godday上买了一个自己的域名 <a href="lestatzhang.com">lestatzhang.com</a></p><h3 id="绑定个人域名"><a href="#绑定个人域名" class="headerlink" title="绑定个人域名"></a>绑定个人域名</h3><p>Godaddy的配置可以参考如下图片<img src="./godadday.png" alt></p><p>然后在next主题中source文件夹中创建CNAME文件，然后将个人域名 <a href="lestatzhang.com">lestatzhang.com</a>添加进CNAME之后重新部署网站。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd themes/next/source/</span><br><span class="line">echo "lestatzhang.com" &gt; CNAME</span><br><span class="line">cd ../../../'</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p><h3 id="TO-DO"><a href="#TO-DO" class="headerlink" title="TO-DO"></a>TO-DO</h3><p>具体博客搭建的步骤就这些了，后面主要是如何对网站页面/主题进行优化的过程。 TO-DO</p>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于kafka中的反序列化</title>
      <link href="/2019/05/12/%E5%85%B3%E4%BA%8Ekafka%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"/>
      <url>/2019/05/12/%E5%85%B3%E4%BA%8Ekafka%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><strong>Kafka生产者</strong>需要序列化程序将==对象转换为字节数组==，然后发送到Kafka。 同样，Kafka消费者需要使用<strong>反序列化器</strong>将从Kafka收到的==字节数组转换为Java对象==。 在前面的示例中，我们假设每个消息的键和值都是字符串，我们在消费者配置中使用了默认的StringDeserializer。</p><p>在第3章关于Kafka生产者的过程中，我们了解了如何自定义序列化类型以及如何使用Avro和AvroSerializer根据模式定义生成Avro对象，然后在向Kafka生成消息时对其进行序列化。 我们现在将介绍如何为自己的对象创建自定义反序列化器以及如何使用Avro及其反序列化器。</p><p>很明显，用于向Kafka生成事件的序列化程序必须与消耗事件时将使用的反序列化程序匹配。 假如我们使用IntSerializer进行序列化，然后使用StringDeserializer进行反序列化，这很有可能出现意想不到的结果。 这意味着作为开发人员，你需要跟踪用于写入每个主题的序列化程序，并确保每个主题仅包含你使用的反序列化程序可以解析的数据。 这是使用Avro和Schema Repository进行序列化和反序列化的好处之一 —— AvroSerializer可以确保写入特定主题的所有数据都与主题的模式兼容，这意味着它可以通过匹配的反序列化器和模式进行反序列化 。 生产者或消费者方面的兼容性错误将通过适当的错误消息轻松捕获，这意味着我们不需要尝试调试字节数组以查找序列化错误。</p><p>我们将首先快速展示如何编写自定义反序列化器，即使这是不太常用的方法，然后我们将继续讨论如何使用Avro反序列化消息键和值的示例。</p><h3 id="Custom-deserializers"><a href="#Custom-deserializers" class="headerlink" title="Custom deserializers"></a>Custom deserializers</h3><p>让我们采用第3章中序列化的相同自定义对象，并为其编写反序列化器：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Customer</span> </span>&#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">int</span> customerID;</span><br><span class="line">            <span class="keyword">private</span> String customerName;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="title">Customer</span><span class="params">(<span class="keyword">int</span> ID, String name)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">this</span>.customerID = ID;</span><br><span class="line">                    <span class="keyword">this</span>.customerName = name;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getID</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> customerID;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> customerName;</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure></p><p>自定义反序列化器将如下所示：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.errors.SerializationException;</span><br><span class="line">    <span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line">    <span class="keyword">import</span> java.util.Map;</span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerDeserializer</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">      <span class="title">Deserializer</span>&lt;<span class="title">Customer</span>&gt; </span>&#123; <span class="comment">//[1]</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// nothing to configure</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> Customer <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> id;</span><br><span class="line">        <span class="keyword">int</span> nameSize;</span><br><span class="line">        String name;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">          <span class="keyword">if</span> (data.length &lt; <span class="number">8</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Size of data received by</span></span><br><span class="line"><span class="string">              IntegerDeserializer is shorter than expected"</span>);</span><br><span class="line">          ByteBuffer buffer = ByteBuffer.wrap(data);</span><br><span class="line">          id = buffer.getInt();</span><br><span class="line">          String nameSize = buffer.getInt();</span><br><span class="line">          <span class="keyword">byte</span>[] nameBytes = <span class="keyword">new</span> Array[Byte](nameSize);</span><br><span class="line">          buffer.get(nameBytes);</span><br><span class="line">          name = <span class="keyword">new</span> String(nameBytes, <span class="string">'UTF-8'</span>);</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">new</span> Customer(id, name); <span class="comment">//[2]</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Error when serializing Customer</span></span><br><span class="line"><span class="string">           to byte[] "</span> + e);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">              <span class="comment">// nothing to close</span></span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure></p><blockquote><p>[1]: 消费者还需要Customer类的实现，并且类和序列化器都需要匹配生产和消费应用程序。 在一个拥有许多消费者和生产者共享数据访问权限的大型组织中，这可能变得具有挑战性。[2]我们只是在这里颠倒串行器的逻辑 - 我们从字节数组中获取客户ID和名称，并使用它们构造我们需要的对象。</p></blockquote><p>使用此序列化程序的使用者代码与此示例类似：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.CustomerDeserializer"</span>);</span><br><span class="line">    KafkaConsumer&lt;String, Customer&gt; consumer =</span><br><span class="line">      <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">    consumer.subscribe(<span class="string">"customerCountries"</span>)</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, Customer&gt; records =</span><br><span class="line">          consumer.poll(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, Customer&gt; record : records)</span><br><span class="line">        &#123;</span><br><span class="line">        System.out.println(<span class="string">"current customer Id: "</span> +</span><br><span class="line">        record.value().getId() + <span class="string">" and</span></span><br><span class="line"><span class="string">           current customer name: "</span> + record.value().getName());</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure></p><p>同样，重要的是要注意不建议实现自定义序列化器和反序列化器。 它紧密地耦合了生产者和消费者，并且易碎且容易出错。 ==更好的解决方案是使用标准消息格式，如JSON，Thrift，Protobuf或Avro==。 我们现在将看到如何将Avro反序列化器与Kafka消费者一起使用。 </p><h3 id="使用Avro反序列化与Kafka消费者"><a href="#使用Avro反序列化与Kafka消费者" class="headerlink" title="使用Avro反序列化与Kafka消费者"></a>使用Avro反序列化与Kafka消费者</h3><p>假设我们正在使用第3章中显示的Avro中Customer类的实现。为了从Kafka中使用这些对象，你希望实现类似于此的消费应用程序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>,</span><br><span class="line">       <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>,</span><br><span class="line">       <span class="string">"io.confluent.kafka.serializers.KafkaAvroDeserializer"</span>); <span class="comment">//[1]</span></span><br><span class="line">    props.put(<span class="string">"schema.registry.url"</span>, schemaUrl); <span class="comment">//[2]</span></span><br><span class="line">    String topic = <span class="string">"customerContacts"</span></span><br><span class="line">    KafkaConsumer consumer = <span class="keyword">new</span></span><br><span class="line">       KafkaConsumer(createConsumerConfig(brokers, groupId, url));</span><br><span class="line">    consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line">    System.out.println(<span class="string">"Reading topic:"</span> + topic);</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, Customer&gt; records =</span><br><span class="line">          consumer.poll(<span class="number">1000</span>); <span class="comment">//[3]</span></span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, Customer&gt; record: records) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Current customer name is: "</span> +</span><br><span class="line">               record.value().getName()); <span class="comment">//[4]</span></span><br><span class="line">&#125;</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>[1] 我们使用KafkaAvroDeserializer来反序列化Avro消息。[2] schema.registry.url是一个新参数。 这只是指向我们存储模式的位置。 这样，消费者可以使用生产者注册的模式来反序列化消息。[3] 我们将生成的类Customer指定为记录值的类型。[4] record.value()是一个Customer实例，我们可以相应地使用它。</p></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html" target="_blank" rel="noopener">Chapter 4. Kafka Consumers: Reading Data from Kafka#Deserializers</a></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> Deserializer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Spark Dataset API中的Typed transformations和Untyped transformations</title>
      <link href="/2019/03/17/%E5%85%B3%E4%BA%8ESpark%20Dataset%20API%E4%B8%AD%E7%9A%84Typed%20transformations%E5%92%8CUntyped%20transformations/"/>
      <url>/2019/03/17/%E5%85%B3%E4%BA%8ESpark%20Dataset%20API%E4%B8%AD%E7%9A%84Typed%20transformations%E5%92%8CUntyped%20transformations/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习Spark源代码的过程中遇到了Typed transformations和Untyped transformations两个概念，整理了以下相关的笔记。对于这两个概念，不知道怎么翻译好，个人理解为强类型转换和弱类型转换，也不知道对不对，欢迎各位大神指正。</p><h2 id="关于Dataset"><a href="#关于Dataset" class="headerlink" title="关于Dataset"></a>关于Dataset</h2><p>Dataset是特定领域对象(domain-specific object)的强类型集合，它可以使用函数或关系运算进行并行转换。 每个Dataset还有一个名为DataFrame的弱类型视图，相当于<code>Dataset[Row]</code>。对于Spark(Scala)，DataFrames只是类型为Row的Dataset。 “Row”类型是Spark中用于计算的，优化过的，in-memory的一种内部表达。</p><p>Dataset上可用的操作分为 <strong>转换(transformation)</strong> 和 <strong>执行(action)</strong> 两种。</p><ul><li>Transformation操作可以产生新的Dataset，如map，filter，select和aggregate（groupBy）等。</li><li>Action操作触发计算和返回结果。 如count，show或写入文件系统等。</li></ul><h2 id="关于Dataset-API"><a href="#关于Dataset-API" class="headerlink" title="关于Dataset API"></a>关于Dataset API</h2><h3 id="Typed-and-Un-typed-APIs"><a href="#Typed-and-Un-typed-APIs" class="headerlink" title="Typed and Un-typed APIs"></a>Typed and Un-typed APIs</h3><p>实质上，在Saprk的结构化API中，可以分成两类，“无类型(untyped)”的DataFrame API和“类型化(typed)”的Dataset API。 确切的说Dataframe并不是”无类型”的, 它们有类型，只是类型检查没有那么严格，只检查这些类型是否在 ==运行时(run-time)== 与schema中指定的类型对齐。 而Dataset在 ==编译时(compile-time)== 就会检查类型是否符合规范。 </p><p>Dataset API仅适用于 ==基于JVM的语言(Scala和Java)==。我们可以使用Scala 中的case class或Java bean来进行类型指定。</p><p>关于不同语言中的可用API可参考下表。</p><table><thead><tr><th>Language</th><th>Main Abstraction</th></tr></thead><tbody><tr><td>Scala</td><td>Dataset[T] &amp; DataFrame (alias for Dataset[Row])</td></tr><tr><td>Java</td><td>Dataset[T]</td></tr><tr><td>Python<em></em></td><td>DataFrame</td></tr><tr><td>R</td><td>DataFrame</td></tr></tbody></table><blockquote><p>由于Python和R没有<code>compile-time type-safety</code>，因此只有 Untyped API，即DataFrames。</p></blockquote><h2 id="关于Transformations"><a href="#关于Transformations" class="headerlink" title="关于Transformations"></a>关于Transformations</h2><p>转换(transformation)可以被分为:</p><ul><li><strong>强类型转换(Typed transformations)</strong></li><li><strong>弱类型转换(Untyped transformations)</strong><h3 id="Typed-transformations-vs-Untyped-transformations"><a href="#Typed-transformations-vs-Untyped-transformations" class="headerlink" title="Typed transformations vs Untyped transformations"></a>Typed transformations vs Untyped transformations</h3>简单来说，如果转换是弱类型的，它将返回一个Dataframe(==确切的说弱类型转换的返回类型还有 <strong><em>Column</em></strong>,  <strong><em>RelationalGroupedDataset</em></strong>, <strong><em>DataFrameNaFunctions</em></strong>  和 <strong><em>DataFrameStatFunctions</em></strong>  等==)，而强类型转换返回的是一个Dataset。 在源代码中，我们可以看到弱类型转换API的返回类型是Dataframe而不是Dataset，且带有<code>@group untypedrel</code>的注释。 因此，我们可以通过检查该方法的签名来确定它是否是弱类型的(untyped)。<blockquote><p>强类型转换API带有<code>@group typedrel</code>的注释</p></blockquote></li></ul><p>例如Dataset.scala类中的<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L864-L876" target="_blank" rel="noopener">join方法</a>就属于弱类型转换(untyped transformations).<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Join with another `DataFrame`.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Behaves as an INNER JOIN and requires a subsequent join predicate.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> right Right side of the join operation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@group</span> untypedrel</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> 2.0.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">def <span class="title">join</span><span class="params">(right: Dataset[_])</span>: DataFrame </span>= withPlan &#123;</span><br><span class="line">  Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通常，任何更改Dataset列类型或添加新列的的转换是弱类型。 当我们需要修改Dataset的schema时，我们就需要退回到Dataframe进行操作。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/ch04.html" target="_blank" rel="noopener">Structured API Overview</a><a href="http://apache-spark-user-list.1001560.n3.nabble.com/Difference-between-Typed-and-untyped-transformation-in-dataset-API-td34650.html" target="_blank" rel="noopener">Difference-between-Typed-and-untyped-transformation-in-dataset-API</a><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">RDDs vs DataFrames and Datasets</a><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-dataset-operators.html" target="_blank" rel="noopener">spark-sql-dataset-operators</a><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">org.apache.spark.sql.Dataset</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> dataset </tag>
            
            <tag> transformations api </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ETL工具Talend最佳实践</title>
      <link href="/2019/03/01/ETL%E5%B7%A5%E5%85%B7Talend%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
      <url>/2019/03/01/ETL%E5%B7%A5%E5%85%B7Talend%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>和Talend这款软件打交道有一段时间了，主要用它来做一些ETL相关的作业开发，以下总结了一些自己配置与开发过程中的最佳实践。</p><h4 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h4><ol><li><p>可以通过修改Talend Studio 的 <strong>.ini</strong> 配置文件来给其分配更多的内存，例如，以下是我在64位8GB内存的电脑配置的参数</p><table><tr><td align="left">-vmargs <br>-Xms2014m <br>-Xmx4096m <br>-XX:MaxPermSize=512m <br>-Dfile.encoding=UTF-8 <br></td></tr></table></li><li><p>在开发过程中一定要注意对Null值得处理</p></li><li>可以创建Repository Metadata用于数据库连接</li><li>可以使用 ==t\<db>== 的数据连接组件定义数据库连接，并重复使用。</db></li><li>记得使用 ==t\<db>== 组件来关闭数据库连接</db></li><li>避免在Talend的组件中在使用硬编码值(hard coding)，使用Talend context 变量代替</li><li>尽可能使用变量代替硬编码</li><li>对于频繁的变换，可以通过创建routines或者functions来减少工作量</li><li>每次关机前记得保存并关闭Talend Studio！！！</li><li>尽可能早的使用tFilterColumns组件过滤去不需要的字段/列</li><li>尽可能早的使用tFilterRows组件过滤去不需要的数据</li><li>使用Select列表达式从数据库获取数据，尽量避免获取不需要的字段</li><li>当作业出现OOM错误时，调整JVM的参数，例如修改Xms和Xmx来分配更多的内存</li><li>通过使用并行化选项来提高作业性能，减少整体的运行时间，如并行化从数据读写数据等</li><li>给Main job起一个有意义的名字</li><li>在定义Sub job时，务必第一时间记录子作业的标题、描述和目的。</li><li>在设计作业尽可能将复杂的作业切割成一个个小作业</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> ETL </tag>
            
            <tag> Talend </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中RDD、DataFrame和DataSet的区别</title>
      <link href="/2019/03/01/Spark%E4%B8%ADRDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/03/01/Spark%E4%B8%ADRDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>最近同事开始学习使用Spark，问我RDD、DataFrame和DataSet之间有什么区别，以及生产环境中的spark1.6将在不久后被移除，全部使用spark2+。于是今天我就借机整理了以下它们三者之间的异同。</p><h4 id="RDD、DataFrame和DataSet的定义"><a href="#RDD、DataFrame和DataSet的定义" class="headerlink" title="RDD、DataFrame和DataSet的定义"></a>RDD、DataFrame和DataSet的定义</h4><p>在开始Spark RDD与DataFrame与Dataset之间的比较之前，先让我们看一下Spark中的RDD，DataFrame和Datasets的定义：</p><ul><li><strong>Spark RDD</strong> RDD代表弹性分布式数据集。它是记录的只读分区集合。 RDD是Spark的基本数据结构。它允许程序员以容错方式在大型集群上执行内存计算。</li><li><strong>Spark Dataframe</strong> 与RDD不同，数据组以列的形式组织起来，类似于关系数据库中的表。它是一个不可变的分布式数据集合。 Spark中的DataFrame允许开发人员将数据结构(类型)加到分布式数据集合上，从而实现更高级别的抽象。<ul><li><strong>Spark Dataset</strong> Apache Spark中的Dataset是DataFrame API的扩展，它提供了类型安全(type-safe)，面向对象(object-oriented)的编程接口。 Dataset利用Catalyst optimizer可以让用户通过类似于sql的表达式对数据进行查询。</li></ul></li></ul><h4 id="RDD、DataFrame和DataSet的比较"><a href="#RDD、DataFrame和DataSet的比较" class="headerlink" title="RDD、DataFrame和DataSet的比较"></a>RDD、DataFrame和DataSet的比较</h4><h5 id="Spark版本"><a href="#Spark版本" class="headerlink" title="Spark版本"></a>Spark版本</h5><ul><li>RDD – 自Spark 1.0起</li><li>DataFrames – 自Spark 1.3起</li><li>DataSet – 自Spark 1.6起</li></ul><h5 id="数据表示形式"><a href="#数据表示形式" class="headerlink" title="数据表示形式"></a>数据表示形式</h5><ul><li>RDD RDD是分布在集群中许多机器上的数据元素的分布式集合。 RDD是一组表示数据的Java或Scala对象。</li><li>DataFrameDataFrame是命名列构成的分布式数据集合。 它在概念上类似于关系数据库中的表。</li><li>Dataset它是DataFrame API的扩展，提供RDD API的类型安全，面向对象的编程接口以及Catalyst查询优化器的性能优势和DataFrame API的堆外存储机制的功能。</li></ul><h5 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h5><ul><li>RDD 它可以轻松有效地处理结构化和非结构化的数据。 和Dataframe和DataSet一样，RDD不会推断出所获取的数据的结构类型，需要用户来指定它。</li><li>DataFrame 仅适用于结构化和半结构化数据。 它的数据以命名列的形式组织起来。 </li><li>DataSet 它也可以有效地处理结构化和非结构化数据。 它表示行(row)的JVM对象或行对象集合形式的数据。 它通过编码器以表格形式(tabular forms)表示。</li></ul><h5 id="编译时类型安全"><a href="#编译时类型安全" class="headerlink" title="编译时类型安全"></a>编译时类型安全</h5><ul><li>RDDRDD提供了一种熟悉的面向对象编程风格，具有编译时类型安全性。</li><li>DataFrame如果您尝试访问表中不存在的列，则持编译错误。 它仅在运行时检测属性错误。</li><li>DataSet DataSet可以在编译时检查类型, 它提供编译时类型安全性。[TO-DO 什么是编译时的类型安全]<h5 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h5></li><li>RDD每当Spark需要在集群内分发数据或将数据写入磁盘时，它就会使用Java序列化。序列化单个Java和Scala对象的开销很昂贵，并且需要在节点之间发送数据和结构。</li><li><p>DataFrameSpark DataFrame可以将数据序列化为二进制格式的堆外存储（在内存中），然后直接在此堆内存上执行许多转换。无需使用java序列化来编码数据。它提供了一个Tungsten物理执行后端，来管理内存并动态生成字节码以进行表达式评估。</p></li><li><p>DataSet 在序列化数据时，Spark中的数据集API具有编码器的概念，该编码器处理JVM对象与表格表示之间的转换。它使用spark内部Tungsten二进制格式存储表格表示。数据集允许对序列化数据执行操作并改善内存使用。它允许按需访问单个属性，而不会消灭整个对象。</p></li></ul><h5 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h5><ul><li>RDD创建和销毁单个对象会导致垃圾回收。<ul><li>DataFrame避免在为数据集中的每一行构造单个对象时引起的垃圾回收。</li></ul></li><li>DataSet因为序列化是通过Tungsten进行的，它使用了off heap数据序列化，不需要垃圾回收器来摧毁对象</li></ul><h5 id="效率-内存使用"><a href="#效率-内存使用" class="headerlink" title="效率/内存使用"></a>效率/内存使用</h5><ul><li>RDD在java和scala对象上单独执行序列化时，效率会降低，这需要花费大量时间。</li><li>DataFrame使用off heap内存进行序列化可以减少开销。 它动态生成字节代码，以便可以对该序列化数据执行许多操作。 无需对小型操作进行反序列化。</li><li>DataSet它允许对序列化数据执行操作并改善内存使用。 因此，它可以允许按需访问单个属性，而无需反序列化整个对象。</li></ul><h5 id="编程语言支持"><a href="#编程语言支持" class="headerlink" title="编程语言支持"></a>编程语言支持</h5><ul><li>RDDRDD提供Java，Scala，Python和R语言的API。 因此，此功能为开发人员提供了灵活性。</li><li>DataFrameDataFrame同样也提供Java，Scala，Python和R语言的API</li><li>DataSet Dataset 的一些API目前仅支持Scala和Java，对Python和R语言的API在陆续开发中</li></ul><h5 id="聚合操作-Aggregation"><a href="#聚合操作-Aggregation" class="headerlink" title="聚合操作(Aggregation)"></a>聚合操作(Aggregation)</h5><ul><li>RDDRDD API执行简单的分组和聚合操作的速度较慢。</li><li>DataFrameDataFrame API非常易于使用。 探索性分析更快，在大型数据集上创建汇总统计数据。</li><li>DataSet在Dataset中，对大量数据集执行聚合操作的速度更快。</li></ul><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ul><li>当我们需要对数据集进行底层的转换和操作时， 可以选择使用RDD</li><li>当我们需要高级抽象时，可以使用DataFrame和Dataset API。</li><li>对于非结构化数据，例如媒体流或文本流，同样可以使用DataFrame和Dataset API。</li><li>我们可以使用DataFrame和Dataset 中的高级的方法。 例如，filter, maps, aggregation, sum, SQL queries以及通过列访问数据等如果您不关心在按名称或列处理或访问数据属性时强加架构（例如列式格式）。另外，如果我们想要在编译时更高程度的类型安全性。</li></ul><p>RDD提供更底层功能， DataFrame和Dataset则允许创建一些自定义的结构，拥有高级的特定操作，节省空间并高速执行。 </p><p>为了确保我们的代码能够尽可能的利用Tungsten优化带来的好处，推荐使用Scala的 Dataset API（而不是RDD API）。</p><p>Dataset即拥有DataFrame带来的relational transformation的便捷，也拥有RDD中的functional transformation的优势。 </p><p>参考资料<a href="https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/" target="_blank" rel="noopener">apache-spark-rdd-vs-dataframe-vs-dataset</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark.sql.shuffle.partitions 和 spark.default.parallelism 的区别</title>
      <link href="/2019/02/27/spark.sql.shuffle.partitions%20%E5%92%8C%20spark.default.parallelism%20%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/02/27/spark.sql.shuffle.partitions%20%E5%92%8C%20spark.default.parallelism%20%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p>在关于spark任务并行度的设置中，有两个参数我们会经常遇到，spark.sql.shuffle.partitions 和 spark.default.parallelism, 那么这两个参数到底有什么区别的？</p><p>首先，让我们来看下它们的定义</p><table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody><tr><td>spark.sql.shuffle.partitions</td><td>200</td><td align="left" width="200">Configures the number of partitions to use when shuffling data for <b>joins</b> or <b>aggregations</b>.</td></tr><tr><td>spark.default.parallelism</td><td align="left">For distributed shuffle operations like <strong>reduceByKey</strong> and <strong>join</strong>, the largest number of partitions in a parent RDD. <br><br>For operations like parallelize with no parent RDDs, it depends on the cluster manager: <br> <b>- Local mode:</b> number of cores on the local machine <br> <b>- Mesos fine grained mode</b>: 8 <br><b>- Others:</b> total number of cores on all executor nodes or 2, whichever is larger</td><td align="left">Default number of partitions in RDDs returned by transformations like <b>join</b>, <b>reduceByKey</b>, and parallelize when not set by user.</td></tr></tbody></table><p>看起来它们的定义似乎也很相似，但在实际测试中，</p><ul><li>spark.default.parallelism只有在处理RDD时才会起作用，对Spark SQL的无效。</li><li>spark.sql.shuffle.partitions则是对sparks SQL专用的设置</li></ul><p>我们可以在提交作业的通过 <code>--conf</code> 来修改这两个设置的值，方法如下：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --conf spark.sql.shuffle.partitions=20 --conf spark.default.parallelism=20</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Parallelism </tag>
            
            <tag> Tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么建议在Spark中使用Scala定义UDF</title>
      <link href="/2019/02/18/why-use-scala-udf/"/>
      <url>/2019/02/18/why-use-scala-udf/</url>
      
        <content type="html"><![CDATA[<p>虽然在Pyspark中，驱动程序是一个python进程，但是它创建的SparkSession对象以及其他DataFrames或者RDDs等都是利用Python封装过的 ==JVM对象== 。简单地说，虽然控制程序是Python，但它实际上是python代码告诉集群上的分布式Scala程序该做什么。 数据存储在JVM的内存中，并由Scala代码进行转换。</p><p>将这些对象从JVM内存中取出并将它们转换为Python可以读取的形式（称为序列化和反序列化）的过程开销是很大的。 一般情况下，将计算结果收集回Python驱动程序通常针对低容量样本，并且不经常进行，因此这种开销相对不被注意。 但是，如果程序在集群中的对整个数据集的Python和JVM对象之间来回转换时，性能将会受到显著影响。</p><p><img src="./sparkudf.png" alt="Credit:  https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9"></p><p>在上图中，Python程序的指令（1）被转换为Spark执行计划，并通过SparkSession JVM对象（2）传递给集群中不同机器上的两个执行程序（3）。 执行程序通常会从外部源（如HDFS）加载数据，在内存中执行某些转换，然后将数据写回外部存储。 数据将在程序的生命周期内保留在JVM（3）中。</p><p>而使用Python UDF时，数据必须经过几个额外的步骤。 首先，数据必须从Java（4）序列化，这样运行UDF所在的Python进程才可以将其读入（5）。 然后，Python运算完的结果经过一些列序列化和反序列化然后返回到JVM。</p><p>那么我们该如何优化呢？我们可以直接使用Scala来编写Spark UDF。Scala UDF可以直接在执行程序的JVM中运行，因此数据将跳过两轮序列化和反序列化，处理的效率将会比使用Python UDF高的多。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>启动Python进程的开销不小，但是真正的开销在于将数据序列化到Python中。推荐在Spark中定义UDF时首选Scala或Java，即使UDFs是用Scala/Java编写的，不用担心，我们依然可以在python(pyspark)中使用它们，简单实例如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### pyspark --jars [path/to/jar/x.jar]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pre Spark 2.1,</span></span><br><span class="line">spark._jvm.com.test.spark.udf.MyUpper.registerUDF(spark._jsparkSession)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.1+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">sqlContext.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.3+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">spark.udf.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your UDF</span></span><br><span class="line">spark.sql(<span class="string">"""SELECT my_upper('abeD123okoj')"""</span>).show()</span><br></pre></td></tr></table></figure></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9" target="_blank" rel="noopener">Using Scala UDFs in PySpark</a></p><p><a href="http://shop.oreilly.com/product/0636920034957.do" target="_blank" rel="noopener">[BOOK] Spark - The Definitive Guide</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Scala </tag>
            
            <tag> UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac上解决访问github慢之懒人版</title>
      <link href="/2019/02/18/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E4%B9%8B%E6%87%92%E4%BA%BA%E7%89%88/"/>
      <url>/2019/02/18/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E4%B9%8B%E6%87%92%E4%BA%BA%E7%89%88/</url>
      
        <content type="html"><![CDATA[<p>写了一个简单脚本用来解决Mac上访问github慢的问题，基本思路如下：</p><ol><li>访问 <a href="http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo" target="_blank" rel="noopener">http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo</a> 获取github的IP地址</li><li>在/etc/hosts中加入查询到的IP和域名 （需要root 权限）</li><li>在终端在输以下指令刷新DNS（需要root 权限）</li></ol><p>运行以下shell脚本即可，需要root权限。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>get the fastest github ip</span><br><span class="line">fast_ip=`curl http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo 2&gt;/dev/null| grep -E -o "([0-9]&#123;1,3&#125;[\.])&#123;3&#125;[0-9]&#123;1,3&#125;" |sed -n '2p'`;</span><br><span class="line">echo $fast_ip;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>Add or replace the $fast_ip in /etc/hosts</span><br><span class="line">if grep "www.github.com" /etc/hosts &gt;/dev/null;then</span><br><span class="line">echo "github dns exists, replace it with the latest one $fast_ip";</span><br><span class="line">sed -i -e "s|[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;  *www\.github\.com|$fast_ip www\.github\.com|"  /etc/hosts;</span><br><span class="line">else</span><br><span class="line">echo "github dns does not exist, replace it with the latest one $fast_ip";</span><br><span class="line">echo -e "\n#Github\n$fast_ip www.github.com\n" &gt;&gt; /etc/hosts;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">killall -HUP mDNSResponder;say DNS cache has been flushed;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac共享主机网络给虚拟机</title>
      <link href="/2019/02/17/Mac%E5%85%B1%E4%BA%AB%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%99%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
      <url>/2019/02/17/Mac%E5%85%B1%E4%BA%AB%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%99%E8%99%9A%E6%8B%9F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>因工作需要需要且身边没有windows系统的笔记本，无奈只好在mac上利用虚拟机安装一个win7系统作为临时过渡。我使用的虚拟机软件是Parallels Desktop（以下简称PD）<img src="./2019021611181380.png" width="30%" alt>PD提供三种不同网络模式供用户选择:</p><ul><li>共享网络（推荐）</li><li>桥接网络</li><li>Host-Only网络<img src="./20190216110149123.png" width="60%" alt></li></ul><p>各种网络模式的区别请移步<a href="https://kb.parallels.com/en/4948" target="_blank" rel="noopener">官方文档</a></p><p>一开始我并没有任何设置，直接使用默认的<strong>共享网络</strong>模式，使用过程中出现了有时候连得上有时候连不上的情况。后来经过一番搜索发现即使用共享网络模式，也需要一些简单的设置。具体步骤如下：</p><ol><li><p>在PD的偏好设置中进行网络设置，添加(+)端口转发规则如下：<img src="./20190216112753690.png" width="60%" alt></p></li><li><p>在虚拟机的网络设置中使用<strong>共享网络</strong>模式<img src="./20190216113314497.png" width="60%" alt></p></li><li><p>重启虚拟机即可</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac上解决访问github慢问题</title>
      <link href="/2019/02/16/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E9%97%AE%E9%A2%98/"/>
      <url>/2019/02/16/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<ol><li>访问 <a href="http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo" target="_blank" rel="noopener">http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo</a> 获取github的IP地址</li><li>在/etc/hosts中加入查询到的IP和域名 （需要root 权限）</li><li>在终端在输以下指令刷新DNS（需要root 权限）  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo killall -HUP mDNSResponder;say DNS cache has been flushed</span><br></pre></td></tr></table></figure></li></ol><p> 或者也可以参见<a href="https://blog.csdn.net/yolohohohoho/article/details/87647036" target="_blank" rel="noopener">懒人版代码</a></p><p>其他Mac相关问题：<a href="https://blog.csdn.net/yolohohohoho/article/details/87892412" target="_blank" rel="noopener">brew update慢的解决方法</a><a href="https://blog.csdn.net/yolohohohoho/article/details/87893368" target="_blank" rel="noopener">conda install慢的解决方法</a></p>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TD笔记 | Teradata数据压缩</title>
      <link href="/2019/02/16/%5BTD%E7%AC%94%E8%AE%B0%5DTeradata%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/"/>
      <url>/2019/02/16/%5BTD%E7%AC%94%E8%AE%B0%5DTeradata%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/</url>
      
        <content type="html"><![CDATA[<p>工作上需要研究Teradata CLOB类型，因为去看了官方文档，自己做了点笔记如下：</p><h3 id="Teradata数据压缩"><a href="#Teradata数据压缩" class="headerlink" title="Teradata数据压缩"></a>Teradata数据压缩</h3><h4 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h4><p>本章描述了几种数据压缩选项，它能够帮助你减少磁盘空间的使用，在某种情况下，还可以提高I/O性能。</p><ul><li>多值压缩（MVC)</li><li>算法压缩（ALC）</li><li>行压缩</li><li>行标题压缩</li><li>自动压缩</li><li>哈希索引和连接索引行压缩</li><li>块级压缩（BLC) </li></ul><p>压缩的目标是利用最少的位数(bits)来准确的表示信息。压缩方法可分为物理方法和逻辑方法。物理方法独立于数据本身意义对其进行重新编码， 而逻辑方法则通过一个更紧凑的集合来替换。</p><p>压缩通过在单位物理容量中存储更多的逻辑数据来降低存储成本。压缩产生更小的行，因此每个可以数据块存储更多行以减少数据块数量。</p><p>压缩还可以提高系统性能，因为每个查询返回更少的物理数据，同时压缩过的数据在内存中保持压缩状态，因此FSG[1]缓存可容纳更多行，从而减少磁盘I/O的大小。</p><p>[1]FSG cache: File Segment cache, a Teradata caching approach.</p><p>算法压缩可以是有损或者是无损的，这取决于所选用的的算法。</p><p>TD的压缩一个很小的初始成本，但是即使对于小表的查询，主要选择的压缩方法能过减小表的大小，这就是一个净赢。</p><h4 id="块级压缩"><a href="#块级压缩" class="headerlink" title="块级压缩"></a>块级压缩</h4><p>数据块是I/O基本物理单位，用于定义TD如何处理数据。当你指定了块级压缩选项，TD将以压缩格式存储数据来减少存储空间。</p><p>BLC可以应用到这几种类型的表：</p><ul><li>主要数据，回退，甚至是无法重新启动的表</li></ul><p>BLC还可以应用于这几种类型的子表：</p><ul><li>BLOB, CLOB, XML, JOIN INDEX, HASH INDEX和Reference index.</li></ul><p>BLC独立应用于其他任何应用于相同数据的压缩类型。BLC可以使用更多的CPU来动态压缩和解压数据，所以查询性能是否随BLC而增强取决于性能是否受I/O带宽或CPU使用率的限制。</p>]]></content>
      
      
      <categories>
          
          <category> DWH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Teradata </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
