<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>关于Spark Dataset API中的Typed transformations和Untyped transformations</title>
      <link href="/2019/03/17/%E5%85%B3%E4%BA%8ESpark%20Dataset%20API%E4%B8%AD%E7%9A%84Typed%20transformations%E5%92%8CUntyped%20transformations/"/>
      <url>/2019/03/17/%E5%85%B3%E4%BA%8ESpark%20Dataset%20API%E4%B8%AD%E7%9A%84Typed%20transformations%E5%92%8CUntyped%20transformations/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习Spark源代码的过程中遇到了Typed transformations和Untyped transformations两个概念，整理了以下相关的笔记。对于这两个概念，不知道怎么翻译好，个人理解为强类型转换和弱类型转换，也不知道对不对，欢迎各位大神指正。</p><h2 id="关于Dataset"><a href="#关于Dataset" class="headerlink" title="关于Dataset"></a>关于Dataset</h2><p>Dataset是特定领域对象(domain-specific object)的强类型集合，它可以使用函数或关系运算进行并行转换。 每个Dataset还有一个名为DataFrame的弱类型视图，相当于<code>Dataset[Row]</code>。对于Spark(Scala)，DataFrames只是类型为Row的Dataset。 “Row”类型是Spark中用于计算的，优化过的，in-memory的一种内部表达。</p><p>Dataset上可用的操作分为 <strong>转换(transformation)</strong> 和 <strong>执行(action)</strong> 两种。</p><ul><li>Transformation操作可以产生新的Dataset，如map，filter，select和aggregate（groupBy）等。</li><li>Action操作触发计算和返回结果。 如count，show或写入文件系统等。</li></ul><h2 id="关于Dataset-API"><a href="#关于Dataset-API" class="headerlink" title="关于Dataset API"></a>关于Dataset API</h2><h3 id="Typed-and-Un-typed-APIs"><a href="#Typed-and-Un-typed-APIs" class="headerlink" title="Typed and Un-typed APIs"></a>Typed and Un-typed APIs</h3><p>实质上，在Saprk的结构化API中，可以分成两类，“无类型(untyped)”的DataFrame API和“类型化(typed)”的Dataset API。 确切的说Dataframe并不是”无类型”的, 它们有类型，只是类型检查没有那么严格，只检查这些类型是否在 ==运行时(run-time)== 与schema中指定的类型对齐。 而Dataset在 ==编译时(compile-time)== 就会检查类型是否符合规范。 </p><p>Dataset API仅适用于 ==基于JVM的语言(Scala和Java)==。我们可以使用Scala 中的case class或Java bean来进行类型指定。</p><p>关于不同语言中的可用API可参考下表。</p><table><thead><tr><th>Language</th><th>Main Abstraction</th></tr></thead><tbody><tr><td>Scala</td><td>Dataset[T] &amp; DataFrame (alias for Dataset[Row])</td></tr><tr><td>Java</td><td>Dataset[T]</td></tr><tr><td>Python<em></em></td><td>DataFrame</td></tr><tr><td>R</td><td>DataFrame</td></tr></tbody></table><blockquote><p>由于Python和R没有<code>compile-time type-safety</code>，因此只有 Untyped API，即DataFrames。</p></blockquote><h2 id="关于Transformations"><a href="#关于Transformations" class="headerlink" title="关于Transformations"></a>关于Transformations</h2><p>转换(transformation)可以被分为:</p><ul><li><strong>强类型转换(Typed transformations)</strong></li><li><strong>弱类型转换(Untyped transformations)</strong><h3 id="Typed-transformations-vs-Untyped-transformations"><a href="#Typed-transformations-vs-Untyped-transformations" class="headerlink" title="Typed transformations vs Untyped transformations"></a>Typed transformations vs Untyped transformations</h3>简单来说，如果转换是弱类型的，它将返回一个Dataframe(==确切的说弱类型转换的返回类型还有 <strong><em>Column</em></strong>,  <strong><em>RelationalGroupedDataset</em></strong>, <strong><em>DataFrameNaFunctions</em></strong>  和 <strong><em>DataFrameStatFunctions</em></strong>  等==)，而强类型转换返回的是一个Dataset。 在源代码中，我们可以看到弱类型转换API的返回类型是Dataframe而不是Dataset，且带有<code>@group untypedrel</code>的注释。 因此，我们可以通过检查该方法的签名来确定它是否是弱类型的(untyped)。<blockquote><p>强类型转换API带有<code>@group typedrel</code>的注释</p></blockquote></li></ul><p>例如Dataset.scala类中的<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L864-L876" target="_blank" rel="noopener">join方法</a>就属于弱类型转换(untyped transformations).<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Join with another `DataFrame`.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Behaves as an INNER JOIN and requires a subsequent join predicate.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> right Right side of the join operation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@group</span> untypedrel</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> 2.0.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">def <span class="title">join</span><span class="params">(right: Dataset[_])</span>: DataFrame </span>= withPlan &#123;</span><br><span class="line">  Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通常，任何更改Dataset列类型或添加新列的的转换是弱类型。 当我们需要修改Dataset的schema时，我们就需要退回到Dataframe进行操作。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/ch04.html" target="_blank" rel="noopener">Structured API Overview</a><a href="http://apache-spark-user-list.1001560.n3.nabble.com/Difference-between-Typed-and-untyped-transformation-in-dataset-API-td34650.html" target="_blank" rel="noopener">Difference-between-Typed-and-untyped-transformation-in-dataset-API</a><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">RDDs vs DataFrames and Datasets</a><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-dataset-operators.html" target="_blank" rel="noopener">spark-sql-dataset-operators</a><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">org.apache.spark.sql.Dataset</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> dataset </tag>
            
            <tag> transformations api </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么建议在Spark中使用Scala定义UDF</title>
      <link href="/2019/02/18/why-use-scala-udf/"/>
      <url>/2019/02/18/why-use-scala-udf/</url>
      
        <content type="html"><![CDATA[<p>虽然在Pyspark中，驱动程序是一个python进程，但是它创建的SparkSession对象以及其他DataFrames或者RDDs等都是利用Python封装过的 ==JVM对象== 。简单地说，虽然控制程序是Python，但它实际上是python代码告诉集群上的分布式Scala程序该做什么。 数据存储在JVM的内存中，并由Scala代码进行转换。</p><p>将这些对象从JVM内存中取出并将它们转换为Python可以读取的形式（称为序列化和反序列化）的过程开销是很大的。 一般情况下，将计算结果收集回Python驱动程序通常针对低容量样本，并且不经常进行，因此这种开销相对不被注意。 但是，如果程序在集群中的对整个数据集的Python和JVM对象之间来回转换时，性能将会受到显著影响。</p><p><img src="./sparkudf.png" alt="Credit:  https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9"></p><p>在上图中，Python程序的指令（1）被转换为Spark执行计划，并通过SparkSession JVM对象（2）传递给集群中不同机器上的两个执行程序（3）。 执行程序通常会从外部源（如HDFS）加载数据，在内存中执行某些转换，然后将数据写回外部存储。 数据将在程序的生命周期内保留在JVM（3）中。</p><p>而使用Python UDF时，数据必须经过几个额外的步骤。 首先，数据必须从Java（4）序列化，这样运行UDF所在的Python进程才可以将其读入（5）。 然后，Python运算完的结果经过一些列序列化和反序列化然后返回到JVM。</p><p>那么我们该如何优化呢？我们可以直接使用Scala来编写Spark UDF。Scala UDF可以直接在执行程序的JVM中运行，因此数据将跳过两轮序列化和反序列化，处理的效率将会比使用Python UDF高的多。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>启动Python进程的开销不小，但是真正的开销在于将数据序列化到Python中。推荐在Spark中定义UDF时首选Scala或Java，即使UDFs是用Scala/Java编写的，不用担心，我们依然可以在python(pyspark)中使用它们，简单实例如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### pyspark --jars [path/to/jar/x.jar]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pre Spark 2.1,</span></span><br><span class="line">spark._jvm.com.test.spark.udf.MyUpper.registerUDF(spark._jsparkSession)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.1+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">sqlContext.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark 2.3+</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line">spark.udf.registerJavaFunction(<span class="string">"my_upper"</span>, <span class="string">"com.test.spark.udf.MyUpper"</span>, StringType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use your UDF</span></span><br><span class="line">spark.sql(<span class="string">"""SELECT my_upper('abeD123okoj')"""</span>).show()</span><br></pre></td></tr></table></figure></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9" target="_blank" rel="noopener">Using Scala UDFs in PySpark</a></p><p><a href="http://shop.oreilly.com/product/0636920034957.do" target="_blank" rel="noopener">[BOOK] Spark - The Definitive Guide</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Scala </tag>
            
            <tag> UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac共享主机网络给虚拟机</title>
      <link href="/2019/02/17/Mac%E5%85%B1%E4%BA%AB%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%99%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
      <url>/2019/02/17/Mac%E5%85%B1%E4%BA%AB%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%99%E8%99%9A%E6%8B%9F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>因工作需要需要且身边没有windows系统的笔记本，无奈只好在mac上利用虚拟机安装一个win7系统作为临时过渡。我使用的虚拟机软件是Parallels Desktop（以下简称PD）<img src="./2019021611181380.png" width="30%" alt>PD提供三种不同网络模式供用户选择:</p><ul><li>共享网络（推荐）</li><li>桥接网络</li><li>Host-Only网络<img src="./20190216110149123.png" width="60%" alt></li></ul><p>各种网络模式的区别请移步<a href="https://kb.parallels.com/en/4948" target="_blank" rel="noopener">官方文档</a></p><p>一开始我并没有任何设置，直接使用默认的<strong>共享网络</strong>模式，使用过程中出现了有时候连得上有时候连不上的情况。后来经过一番搜索发现即使用共享网络模式，也需要一些简单的设置。具体步骤如下：</p><ol><li><p>在PD的偏好设置中进行网络设置，添加(+)端口转发规则如下：<img src="./20190216112753690.png" width="60%" alt></p></li><li><p>在虚拟机的网络设置中使用<strong>共享网络</strong>模式<img src="./20190216113314497.png" width="60%" alt></p></li><li><p>重启虚拟机即可</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac上解决访问github慢问题</title>
      <link href="/2019/02/16/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E9%97%AE%E9%A2%98/"/>
      <url>/2019/02/16/Mac%E4%B8%8A%E8%A7%A3%E5%86%B3%E8%AE%BF%E9%97%AEgithub%E6%85%A2%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<ol><li>访问 <a href="http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo" target="_blank" rel="noopener">http://github.global.ssl.fastly.net.ipaddress.com/#ipinfo</a> 获取github的IP地址</li><li>在/etc/hosts中加入查询到的IP和域名 （需要root 权限）</li><li>在终端在输以下指令刷新DNS（需要root 权限）  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo killall -HUP mDNSResponder;say DNS cache has been flushed</span><br></pre></td></tr></table></figure></li></ol><p> 或者也可以参见<a href="https://blog.csdn.net/yolohohohoho/article/details/87647036" target="_blank" rel="noopener">懒人版代码</a></p><p>其他Mac相关问题：<a href="https://blog.csdn.net/yolohohohoho/article/details/87892412" target="_blank" rel="noopener">brew update慢的解决方法</a><a href="https://blog.csdn.net/yolohohohoho/article/details/87893368" target="_blank" rel="noopener">conda install慢的解决方法</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Mac </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
